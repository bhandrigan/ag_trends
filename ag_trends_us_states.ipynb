{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNx70m/CbGHNH92F/2yBL+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhandrigan/ag_trends/blob/main/ag_trends_us_states.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends google-ads scrapingbee  shapely>=2.0.0 -q"
      ],
      "metadata": {
        "id": "tiIKBDK609ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LoX0e81uFmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load libraries and functions\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import numpy as np\n",
        "import requests\n",
        "import gzip\n",
        "from io import BytesIO\n",
        "from pytrends.request import TrendReq\n",
        "from google.ads.googleads.client import GoogleAdsClient\n",
        "from google.api_core.exceptions import ResourceExhausted\n",
        "from requests.exceptions import RequestException\n",
        "from requests.exceptions import ReadTimeout\n",
        "import time\n",
        "import os\n",
        "import zipfile\n",
        "from urllib.parse import urlparse\n",
        "import tempfile\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import zipfile\n",
        "import re\n",
        "import random\n",
        "from scrapingbee import ScrapingBeeClient\n",
        "import shapely\n",
        "from shapely.geometry import shape\n",
        "import multiprocessing as mp\n",
        "import unicodedata\n",
        "import matplotlib.ticker as ticker\n",
        "from stats_can import StatsCan\n",
        "from census import Census\n",
        "import us\n",
        "import gc\n",
        "\n",
        "# Initialize the Census API\n",
        "CENSUS_API_KEY = userdata.get('CENSUS_API')\n",
        "c = Census(CENSUS_API_KEY, year=2022)  # Update year to latest ACS available if needed\n",
        "sc = StatsCan()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scrapingbee_api_key = userdata.get('SCRAPING_BEE')\n",
        "gads_sa = json.loads(userdata.get('N90_GADS_SA_ACCOUNT_JSON'))\n",
        "gads_api_key = userdata.get('N90_GADS_API_KEY')\n",
        "sa_account = tempfile.NamedTemporaryFile(delete=False)\n",
        "sa_account.write(json.dumps(gads_sa).encode())\n",
        "sa_account.close()\n",
        "sa_account_path = sa_account.name\n",
        "search_api_io_key = userdata.get('SEARCH_API_IO_KEY')\n",
        "\n",
        "gads_account = \"8417741864\"\n",
        "use_proto_plus = True\n",
        "impersonated_email = \"api@n90.co\"\n",
        "\n",
        "\n",
        "# create google-ads.yaml file text\n",
        "yaml_content = f\"\"\"\n",
        "developer_token: {gads_api_key}\n",
        "use_proto_plus: {use_proto_plus}\n",
        "json_key_file_path: {sa_account_path}\n",
        "impersonated_email: {impersonated_email}\n",
        "login_customer_id: {gads_account}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the client with the dictionary configuration (hypothetical)\n",
        "client = GoogleAdsClient.load_from_string(yaml_content)\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    return ''.join(c for c in text if not unicodedata.combining(c)).upper()\n",
        "\n",
        "def fetch_unzip_load_shapefile_flexible(url, output_dir):\n",
        "    \"\"\"\n",
        "    Fetches a zip file containing shapefiles from a URL,\n",
        "    unzips it if needed, and loads the first .shp file found\n",
        "    in the output directory into a GeoDataFrame.\n",
        "\n",
        "    Removes query string variables from the local filename.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the zip file containing the shapefiles.\n",
        "        output_dir (str): The directory where the zip file will be downloaded\n",
        "                          and unzipped.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: The loaded shapefile as a GeoDataFrame, or None if\n",
        "                          download, unzipping, or loading fails, or if no\n",
        "                          .shp files are found.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Parse the URL to get the path component\n",
        "    parsed_url = urlparse(url)\n",
        "    # Get the base filename from the URL path and remove query string\n",
        "    zip_filename = os.path.basename(parsed_url.path)\n",
        "    zip_file_path = os.path.join(output_dir, zip_filename)\n",
        "\n",
        "    # Check if the zip file already exists (indicates previous download/unzip)\n",
        "    if os.path.exists(zip_file_path):\n",
        "        print(f\"Zip file '{zip_filename}' already exists in '{output_dir}'. Skipping download.\")\n",
        "    else:\n",
        "        # If the zip file doesn't exist, attempt to download\n",
        "        print(f\"Zip file '{zip_filename}' not found. Attempting to download from {url}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True, verify=False)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "            # Download the zip file\n",
        "            with open(zip_file_path, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"Zip file downloaded to '{zip_file_path}'.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Check if .shp files already exist in the output directory\n",
        "    shp_files = [f for f in os.listdir(output_dir) if f.endswith('.shp')]\n",
        "    if shp_files:\n",
        "        print(f\"Shapefile(s) found in '{output_dir}'. Loading the first one...\")\n",
        "        shapefile_name_to_load = shp_files[0]  # Load the first .shp file found\n",
        "        shape_file_path = os.path.join(output_dir, shapefile_name_to_load)\n",
        "        try:\n",
        "            gdf = gpd.read_file(shape_file_path)\n",
        "            print(f\"Shapefile '{shapefile_name_to_load}' loaded successfully.\")\n",
        "            return gdf\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading shapefile '{shapefile_name_to_load}': {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # If no .shp files are found, attempt to unzip\n",
        "        print(f\"No .shp files found in '{output_dir}'. Attempting to unzip '{zip_filename}'...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(output_dir)\n",
        "            print(f\"Zip file extracted to '{output_dir}'.\")\n",
        "\n",
        "            # After unzipping, look for .shp files again\n",
        "            shp_files_after_unzip = [f for f in os.listdir(output_dir) if f.endswith('.shp')]\n",
        "            if shp_files_after_unzip:\n",
        "                print(f\"Shapefile(s) found after unzipping. Loading the first one...\")\n",
        "                shapefile_name_to_load = shp_files_after_unzip[0]\n",
        "                shape_file_path = os.path.join(output_dir, shapefile_name_to_load)\n",
        "                try:\n",
        "                    gdf = gpd.read_file(shape_file_path)\n",
        "                    print(f\"Shapefile '{shapefile_name_to_load}' loaded successfully.\")\n",
        "                    return gdf\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading shapefile '{shapefile_name_to_load}': {e}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(f\"No .shp files found in '{output_dir}' after unzipping.\")\n",
        "                return None\n",
        "\n",
        "        except zipfile.BadZipFile as e:\n",
        "            print(f\"Error unzipping file (Bad zip file): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during unzipping or loading: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "# Build request to fetch keyword plan metrics by location\n",
        "def get_keyword_estimates(client, keywords, geo_ids, batch_size=20):\n",
        "    \"\"\"\n",
        "    Builds a keyword\n",
        "    \"\"\"\n",
        "    keyword_plan_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for state, geo_id in geo_ids.items():\n",
        "        for i in range(0, len(keywords), batch_size):\n",
        "            batch_keywords = keywords[i:i + batch_size]\n",
        "\n",
        "            request = {\n",
        "                \"customer_id\": client.login_customer_id,\n",
        "                \"language\": \"languageConstants/1000\",  # English\n",
        "                \"geo_target_constants\": [f\"geoTargetConstants/{geo_id}\"],\n",
        "                \"keyword_plan_network\": client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH_AND_PARTNERS,\n",
        "                \"keyword_seed\": None,\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = keyword_plan_service.generate_keyword_ideas(request=request)\n",
        "\n",
        "                for result in response:\n",
        "                    results.append({\n",
        "                        'state': state,\n",
        "                        'keyword': result.text,\n",
        "                        'avg_monthly_searches': result.keyword_idea_metrics.avg_monthly_searches,\n",
        "                        'competition': result.keyword_idea_metrics.competition.name,\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error in state {state} with geo ID {geo_id}, batch starting with '{batch_keywords[0]}': {e}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.api_core.exceptions import ResourceExhausted\n",
        "\n",
        "def get_exact_keyword_volumes(client, keywords, geo_ids, batch_size=20, max_retries=5):\n",
        "    keyword_plan_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "    results = []\n",
        "\n",
        "    for state, geo_id in geo_ids.items():\n",
        "        for i in range(0, len(keywords), batch_size):\n",
        "            batch_keywords = keywords[i:i + batch_size]\n",
        "\n",
        "            request = {\n",
        "                \"customer_id\": client.login_customer_id,\n",
        "                \"language\": \"languageConstants/1000\",\n",
        "                \"geo_target_constants\": [f\"geoTargetConstants/{geo_id}\"],\n",
        "                \"keyword_plan_network\": client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH_AND_PARTNERS,\n",
        "                \"keywords\": batch_keywords,\n",
        "                \"historical_metrics_options\": {\n",
        "                    \"include_average_cpc\": False\n",
        "                },\n",
        "            }\n",
        "\n",
        "            retries = 0\n",
        "            while retries < max_retries:\n",
        "                try:\n",
        "                    response = keyword_plan_service.generate_keyword_historical_metrics(request=request)\n",
        "                    for result in response.results:\n",
        "                        results.append({\n",
        "                            'state': state,\n",
        "                            'keyword': result.text,\n",
        "                            'avg_monthly_searches': result.keyword_metrics.avg_monthly_searches,\n",
        "                            'competition': result.keyword_metrics.competition.name,\n",
        "                        })\n",
        "                    break  # Exit loop after success\n",
        "                except ResourceExhausted as e:\n",
        "                    retry_delay = 2 ** retries\n",
        "                    print(f\"[{state}] Quota exceeded. Retry in {retry_delay} seconds. Attempt {retries+1}/{max_retries}.\")\n",
        "                    time.sleep(retry_delay)\n",
        "                    retries += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"[{state}] Error for batch '{batch_keywords[0]}': {e}\")\n",
        "                    break  # Exit loop if non-quota error encountered\n",
        "\n",
        "            if retries == max_retries:\n",
        "                print(f\"[{state}] Max retries exceeded for batch '{batch_keywords[0]}'. Moving to next batch.\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "def get_repair_keyword_ideas(client, seed_keywords, geo_id='2840', language_id='1000', suggestions_per_keyword=10, retries=5):\n",
        "    \"\"\"\n",
        "    Fetch keyword ideas per seed keyword, salvaging results if API limit reached.\n",
        "\n",
        "    Args:\n",
        "        client: GoogleAdsClient instance.\n",
        "        seed_keywords (list): Initial seed keywords.\n",
        "        geo_id (str): Geo target ID ('2840' for US).\n",
        "        language_id (str): Language ID ('1000' for English).\n",
        "        suggestions_per_keyword (int): Top suggestions per seed keyword.\n",
        "        retries (int): Retries on API rate-limit errors.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Collected keyword suggestions and metrics.\n",
        "    \"\"\"\n",
        "    keyword_plan_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "    results = []\n",
        "\n",
        "    for seed_keyword in seed_keywords:\n",
        "        request = {\n",
        "            \"customer_id\": client.login_customer_id,\n",
        "            \"language\": f\"languageConstants/{language_id}\",\n",
        "            \"geo_target_constants\": [f\"geoTargetConstants/{geo_id}\"],\n",
        "            \"keyword_plan_network\": client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH_AND_PARTNERS,\n",
        "            \"keyword_seed\": {\"keywords\": [seed_keyword]},\n",
        "            \"page_size\": suggestions_per_keyword\n",
        "        }\n",
        "\n",
        "        attempt = 0\n",
        "        wait_time = 5  # Initial backoff in seconds\n",
        "\n",
        "        while attempt <= retries:\n",
        "            try:\n",
        "                response = keyword_plan_service.generate_keyword_ideas(request=request)\n",
        "\n",
        "                for result in response:\n",
        "                    results.append({\n",
        "                        'seed_keyword': seed_keyword,\n",
        "                        'suggested_keyword': result.text,\n",
        "                        'avg_monthly_searches': result.keyword_idea_metrics.avg_monthly_searches,\n",
        "                        'competition': result.keyword_idea_metrics.competition.name,\n",
        "                    })\n",
        "                # Successfully retrieved results; break retry loop\n",
        "                break\n",
        "\n",
        "            except ResourceExhausted:\n",
        "                print(f\"Rate limit hit fetching '{seed_keyword}'. Attempt {attempt + 1}/{retries}. Retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                wait_time *= 2  # Exponential backoff\n",
        "                attempt += 1\n",
        "\n",
        "        if attempt > retries:\n",
        "            print(f\"Exceeded max retries for '{seed_keyword}'. Moving to next keyword.\")\n",
        "\n",
        "    # Return all successfully collected results even if some requests failed\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def prep_paid_search_data(df, all_regions, geo_col='state', neutral_factor=0.75):\n",
        "    df = df.groupby(geo_col)['avg_monthly_searches'].sum().reset_index()\n",
        "    df[geo_col] = df[geo_col].str.upper()\n",
        "\n",
        "    total_searches = df['avg_monthly_searches'].sum()\n",
        "    if total_searches == 0:\n",
        "        df['paid_search_composite_factor_100'] = neutral_factor\n",
        "    else:\n",
        "        df['paid_search_composite_factor_100'] = (df['avg_monthly_searches'] / total_searches) * 100\n",
        "\n",
        "    df.loc[df['avg_monthly_searches'] == 0, 'paid_search_composite_factor_100'] = neutral_factor\n",
        "\n",
        "    # Add missing regions explicitly with neutral factor\n",
        "    missing_regions = set(all_regions) - set(df[geo_col])\n",
        "    if missing_regions:\n",
        "        missing_df = pd.DataFrame({\n",
        "            geo_col: list(missing_regions),\n",
        "            'avg_monthly_searches': 0,\n",
        "            'paid_search_composite_factor_100': neutral_factor\n",
        "        })\n",
        "        df = pd.concat([df, missing_df], ignore_index=True)\n",
        "\n",
        "    return df[[geo_col, 'avg_monthly_searches', 'paid_search_composite_factor_100']].reset_index(drop=True)\n",
        "\n",
        "def get_trends_via_scrapingbee(\n",
        "        keyword_list, scrapingbee_api_key, language='en-US', tz=360, geo='US',\n",
        "        resolution='REGION', timeframe='today 12-m', inc_low_vol=True,\n",
        "        max_retries=4, initial_wait=10, timeout=(10, 30)):  # increased timeouts clearly\n",
        "\n",
        "    proxy_url = f\"http://{scrapingbee_api_key}:@proxy.scrapingbee.com:8886\"\n",
        "    proxies = [proxy_url]\n",
        "    dfs = []\n",
        "\n",
        "    for kw in keyword_list:\n",
        "        retries = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retries <= max_retries:\n",
        "            try:\n",
        "                pytrends = TrendReq(\n",
        "                    hl=language,\n",
        "                    tz=tz,\n",
        "                    proxies=proxies,\n",
        "                    timeout=timeout,\n",
        "                    requests_args={'verify': False}\n",
        "                )\n",
        "\n",
        "                pytrends.build_payload([kw], geo=geo, timeframe=timeframe)\n",
        "                df_kw = pytrends.interest_by_region(resolution=resolution, inc_low_vol=inc_low_vol)\n",
        "                df_kw.rename(columns={kw: kw.replace(' ', '_')}, inplace=True)\n",
        "                dfs.append(df_kw)\n",
        "                print(f\"[Success] '{kw}' retrieved successfully.\")\n",
        "                break\n",
        "\n",
        "            except (RequestException, ReadTimeout) as e:\n",
        "                error_str = str(e).lower()\n",
        "                if '429' in error_str or 'too many requests' in error_str or 'timeout' in error_str:\n",
        "                    print(f\"[Retryable Error] '{kw}' - Retrying in {wait_time}s (attempt {retries+1}/{max_retries}): {e}\")\n",
        "                    jitter = random.uniform(2, 5)\n",
        "                    time.sleep(wait_time + jitter)\n",
        "                    wait_time *= 2\n",
        "                    retries += 1\n",
        "                else:\n",
        "                    print(f\"[Critical Error] '{kw}' - Non-retryable error: {e}\")\n",
        "                    raise\n",
        "\n",
        "        if retries > max_retries:\n",
        "            print(f\"[Skipped] Exceeded maximum retries for keyword '{kw}'.\")\n",
        "\n",
        "        sleep_between_keywords = random.uniform(5, 10)\n",
        "        print(f\"Waiting {sleep_between_keywords:.1f}s before next keyword...\")\n",
        "        time.sleep(sleep_between_keywords)\n",
        "\n",
        "    if dfs:\n",
        "        df_combo = pd.concat(dfs, axis=1)\n",
        "        print(\"Data successfully retrieved for some/all keywords.\")\n",
        "    else:\n",
        "        df_combo = pd.DataFrame()\n",
        "        print(\"No data retrieved for keywords after retries.\")\n",
        "\n",
        "    return df_combo\n",
        "\n",
        "import time\n",
        "import random\n",
        "from pytrends.request import TrendReq\n",
        "from requests.exceptions import RequestException\n",
        "import pandas as pd\n",
        "\n",
        "from pytrends.exceptions import TooManyRequestsError\n",
        "\n",
        "def get_trends(keyword_list, language='en-US', tz=360, geo='US', resolution='REGION',\n",
        "               timeframe='today 12-m', inc_low_vol=True, max_retries=5, initial_wait=10):\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for kw in keyword_list:\n",
        "        retries = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retries <= max_retries:\n",
        "            try:\n",
        "                pytrends = TrendReq(hl=language, tz=tz)\n",
        "                pytrends.build_payload([kw], geo=geo, timeframe=timeframe)\n",
        "                df_kw = pytrends.interest_by_region(resolution=resolution, inc_low_vol=inc_low_vol)\n",
        "                df_kw.rename(columns={kw: kw.replace(' ', '_')}, inplace=True)\n",
        "                dfs.append(df_kw)\n",
        "                print(f\"Successfully retrieved data for keyword '{kw}'\")\n",
        "                break\n",
        "\n",
        "            except TooManyRequestsError as e:\n",
        "                jitter = random.uniform(1, 5)\n",
        "                print(f\"429 TooManyRequestsError for '{kw}', retrying in {wait_time+jitter:.1f}s \"\n",
        "                      f\"(attempt {retries+1}/{max_retries})...\")\n",
        "                time.sleep(wait_time + jitter)\n",
        "                wait_time *= 2\n",
        "                retries += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Non-retryable error: {e}\")\n",
        "                raise\n",
        "\n",
        "        if retries > max_retries:\n",
        "            print(f\"Exceeded maximum retries for keyword '{kw}'. Skipping keyword.\")\n",
        "\n",
        "        sleep_duration = random.uniform(5, 10)\n",
        "        print(f\"Pausing for {sleep_duration:.1f}s before next keyword...\")\n",
        "        time.sleep(sleep_duration)\n",
        "\n",
        "    if dfs:\n",
        "        df_combo = pd.concat(dfs, axis=1)\n",
        "    else:\n",
        "        df_combo = pd.DataFrame()\n",
        "        print(\"Warning: No keyword data successfully retrieved.\")\n",
        "\n",
        "    return df_combo\n",
        "\n",
        "def prep_trends_data(df, all_regions, region_col='region', neutral_factor=0.75):\n",
        "    df[region_col] = df[region_col].str.upper().apply(normalize_text)\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include='number').columns\n",
        "    df['st_composite_sum'] = df[numeric_cols].sum(axis=1)\n",
        "\n",
        "    total_sum = df['st_composite_sum'].sum()\n",
        "    if total_sum == 0:\n",
        "        df['st_composite_factor_100'] = neutral_factor\n",
        "    else:\n",
        "        df['st_composite_factor_100'] = (df['st_composite_sum'] / total_sum) * 100\n",
        "\n",
        "    df['st_composite_factor_100'] = df['st_composite_factor_100'].clip(upper=5)\n",
        "    df.loc[df['st_composite_sum'] == 0, 'st_composite_factor_100'] = neutral_factor\n",
        "\n",
        "    # Add missing regions explicitly with neutral factor\n",
        "    missing_regions = set(all_regions) - set(df[region_col])\n",
        "    if missing_regions:\n",
        "        missing_df = pd.DataFrame({\n",
        "            region_col: list(missing_regions),\n",
        "            'st_composite_sum': 0,\n",
        "            'st_composite_factor_100': neutral_factor\n",
        "        })\n",
        "        df = pd.concat([df[[region_col, 'st_composite_sum', 'st_composite_factor_100']], missing_df], ignore_index=True)\n",
        "\n",
        "    return df[['region', 'st_composite_sum', 'st_composite_factor_100']].reset_index(drop=True)\n",
        "\n",
        "def add_us_stats_and_geos(gdf, df_trends, df_stats, paid_df, gdf_geo_col='NAME', trends_geo_col='region', stats_geo_col='STATE_NAME', paid_geo_col='state', counts_col=None):\n",
        "    if counts_col is None:\n",
        "        return \"Error: counts_col must be provided.\"\n",
        "    gdf[gdf_geo_col] = gdf[gdf_geo_col].apply(normalize_text)\n",
        "    df_trends[trends_geo_col] = df_trends[trends_geo_col].apply(normalize_text)\n",
        "    df_stats[stats_geo_col] = df_stats[stats_geo_col].apply(normalize_text)\n",
        "    paid_df[paid_geo_col] = paid_df[paid_geo_col].apply(normalize_text)\n",
        "    merged_df = gdf.merge(df_trends, left_on=gdf_geo_col, right_on=trends_geo_col, how='left')\n",
        "    merged_df = pd.merge(merged_df, df_stats, left_on=gdf_geo_col, right_on=stats_geo_col, how='left', suffixes=('', '_extra'))\n",
        "    for col in merged_df.columns:\n",
        "        if col.endswith('_extra'):\n",
        "            merged_df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    merged_df['Ops_below_250k'] = merged_df['Ops_below_250k'].fillna(0).round(0).astype(int)\n",
        "    merged_df['Ops_250k_or_more'] = merged_df['Ops_250k_or_more'].fillna(0).round(0).astype(int)\n",
        "    merged_df['Total_Ops'] = merged_df['Total_Ops'].fillna(0).round(0).astype(int)\n",
        "    comp_col_label = f\"{counts_col}_composite_factor_100\"\n",
        "    merged_df[comp_col_label] = (merged_df[counts_col] / merged_df[counts_col].sum()) * 100\n",
        "    # merged_df['Ops_below_250k_composite_factor_100'] = (merged_df['Ops_below_250k'] / merged_df['Ops_250k_or_more'].sum()) * 100\n",
        "    merged_df2 = merged_df.merge(paid_df, left_on=gdf_geo_col, right_on=paid_geo_col, how='left').copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "    # merged_df2 = merged_df2.loc[~merged_df2[paid_geo_col].isna()].copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    mean_trends = merged_df2['st_composite_factor_100'].fillna(0).mean()\n",
        "    mean_volume = merged_df2['paid_search_composite_factor_100'].fillna(0).mean()\n",
        "\n",
        "    # Clearly calculate relative positions\n",
        "    merged_df2['trends_relative'] = merged_df2['st_composite_factor_100'] / mean_trends\n",
        "    merged_df2['volume_relative'] = merged_df2['paid_search_composite_factor_100'] / mean_volume\n",
        "\n",
        "    # Clearly combine both into single adjustment factor\n",
        "    merged_df2['combined_relative_factor'] = (merged_df2['trends_relative'] + merged_df2['volume_relative']) / 2\n",
        "\n",
        "    # Adjusted audience clearly calculated\n",
        "    audience_label = f\"adjusted_audience_{counts_col}\"\n",
        "    merged_df2[audience_label] = (merged_df2[counts_col] * merged_df2['combined_relative_factor']).fillna(0).round(0).astype(int)\n",
        "    return merged_df2\n",
        "\n",
        "\n",
        "def reposition_alaska_hawaii(gdf):\n",
        "    # Separate states\n",
        "    contiguous_us = gdf[~gdf['NAME'].isin(['ALASKA', 'HAWAII'])]\n",
        "    alaska = gdf[gdf['NAME'] == 'ALASKA'].copy()\n",
        "    hawaii = gdf[gdf['NAME'] == 'HAWAII'].copy()\n",
        "    # Adjusted scale and translate for Alaska and Hawaii\n",
        "    alaska.geometry = alaska.scale(xfact=0.4, yfact=0.4, origin='center').translate(xoff=1500000, yoff=-4400000)\n",
        "    hawaii.geometry = hawaii.scale(xfact=0.7, yfact=0.7, origin='center').translate(xoff=5200000, yoff=-1700000)\n",
        "    # Combine adjusted geometries\n",
        "    repositioned_us = pd.concat([contiguous_us, alaska, hawaii])\n",
        "    # and remove all other islands like puerto rico\n",
        "    repositioned_us = repositioned_us[~(repositioned_us['STATEFP'] >= '60')].sort_values('NAME').reset_index(drop=True)\n",
        "    return repositioned_us\n",
        "\n",
        "def plot_us_map(gdf, _density_cmap, _column, _legend_kwds, _title, _footer, _vmax=50000, projection=None, filename='map_output.png', dpi=300):\n",
        "    if projection is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5), subplot_kw={'projection': projection})\n",
        "\n",
        "    gdf.plot(ax=ax, column=_column, cmap=_density_cmap, legend=True, vmin=0, vmax=_vmax,\n",
        "             legend_kwds=_legend_kwds)\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_title(_title, fontsize=14, pad=20, horizontalalignment='center')\n",
        "\n",
        "    colorbar = ax.get_figure().axes[-1]\n",
        "    colorbar.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x)))\n",
        "\n",
        "    fig.subplots_adjust(bottom=0.1)\n",
        "    fig.text(0.5, 0.01, _footer, ha='center', fontsize=10)\n",
        "\n",
        "    # Save figure at higher resolution\n",
        "    plt.savefig(filename, dpi=dpi, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def fetch_google_ads_geo_targets(client, country_codes=['US', 'CA']):\n",
        "    ga_service = client.get_service(\"GoogleAdsService\")\n",
        "\n",
        "    country_codes_formatted = ', '.join(f\"'{code}'\" for code in country_codes)\n",
        "\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            geo_target_constant.resource_name,\n",
        "            geo_target_constant.id,\n",
        "            geo_target_constant.name,\n",
        "            geo_target_constant.country_code,\n",
        "            geo_target_constant.target_type\n",
        "        FROM geo_target_constant\n",
        "        WHERE geo_target_constant.country_code IN ({country_codes_formatted})\n",
        "        AND geo_target_constant.target_type IN ('State', 'Province', 'Country')\n",
        "    \"\"\"\n",
        "\n",
        "    response = ga_service.search_stream(customer_id=client.login_customer_id, query=query)\n",
        "\n",
        "    geo_targets = []\n",
        "    for batch in response:\n",
        "        for row in batch.results:\n",
        "            geo_targets.append({\n",
        "                'resource_name': row.geo_target_constant.resource_name,\n",
        "                'google_id': row.geo_target_constant.id,\n",
        "                'name': row.geo_target_constant.name,\n",
        "                'country_code': row.geo_target_constant.country_code,\n",
        "                'target_type': row.geo_target_constant.target_type,\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(geo_targets)\n",
        "\n",
        "def load_or_download_csv(local_filename, data_url):\n",
        "    \"\"\"\n",
        "    Loads a CSV file locally or downloads it if not available locally.\n",
        "    Handles CSV files compressed in ZIP, GZIP, or uncompressed formats.\n",
        "\n",
        "    Args:\n",
        "        local_filename (str): Path to the local CSV file.\n",
        "        data_url (str): URL to download the CSV from if not available locally.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataframe from CSV.\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_filename):\n",
        "        try:\n",
        "            df = pd.read_csv(local_filename)\n",
        "            print(f\"Loaded data successfully from {local_filename}.\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading local file, will attempt download: {e}\")\n",
        "\n",
        "    print(f\"Downloading data from {data_url}...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    content_type = response.headers.get('Content-Type', '').lower()\n",
        "\n",
        "    # Handle ZIP compressed files\n",
        "    if 'zip' in content_type or data_url.lower().endswith('.zip'):\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            csv_files = [name for name in z.namelist() if name.endswith('.csv')]\n",
        "            if not csv_files:\n",
        "                raise ValueError(\"No CSV file found in ZIP archive.\")\n",
        "            with z.open(csv_files[0]) as f:\n",
        "                df = pd.read_csv(f)\n",
        "\n",
        "    # Handle GZIP compressed files\n",
        "    elif 'gzip' in content_type or data_url.lower().endswith('.gz'):\n",
        "        with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "    # Handle regular CSV files\n",
        "    else:\n",
        "        df = pd.read_csv(BytesIO(response.content))\n",
        "\n",
        "    # Save locally\n",
        "    df.to_csv(local_filename, index=False)\n",
        "    print(f\"Data downloaded and saved locally to {local_filename}.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Clearly define Canadian sales categories explicitly\n",
        "below_250k_categories_canada = [\n",
        "    '$0',\n",
        "    '$1 to $9,999',\n",
        "    '$10,000 to $24,999',\n",
        "    '$25,000 to $49,999',\n",
        "    '$50,000 to $99,999',\n",
        "    '$100,000 to $249,999'\n",
        "]\n",
        "\n",
        "above_250k_categories_canada = [\n",
        "    '$250,000 to $499,999',\n",
        "    '$500,000 to $999,999',\n",
        "    '$1,000,000 to $1,999,999',\n",
        "    '$2,000,000 and over'\n",
        "]\n",
        "\n",
        "# Function to categorize revenues explicitly\n",
        "def categorize_revenues(df, geo_level, geo_col, country_geo_val, above_250k_cats, below_250k_cats, country, pivot_col='Total farm revenues distribution'):\n",
        "    if country == 'CA':\n",
        "        df_filtered = df.loc[df[geo_col] != country_geo_val] if geo_level != 'national' else df.loc[df[geo_col] == country_geo_val]\n",
        "        df_filtered = df[(df[geo_col] != country_geo_val) & ~(df[geo_col].str.contains(','))] if geo_level == 'provincial' else df_filtered\n",
        "        # Example assuming your dataframe is df:\n",
        "        summary_df = df_filtered.pivot_table(\n",
        "            index=geo_col,\n",
        "            columns=pivot_col,\n",
        "            values='VALUE',\n",
        "            aggfunc='sum',\n",
        "            fill_value=0\n",
        "        ).reset_index()\n",
        "\n",
        "        # Explicit aggregation for below and above $250k\n",
        "        summary_df['Ops_below_250k'] = summary_df[below_250k_cats].sum(axis=1)\n",
        "        summary_df['Ops_250k_or_more'] = summary_df[above_250k_cats].sum(axis=1)\n",
        "        summary_df['geo_code'] = summary_df[geo_col].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "        summary_df['geo_name'] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "        summary_df['Total_Ops'] = summary_df['Ops_below_250k'] + summary_df['Ops_250k_or_more']\n",
        "        summary_df['PRUID'] = (\n",
        "            summary_df['geo_code']\n",
        "            .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "            .str[:2]                             # clearly take the first two characters\n",
        "            .astype(int)                         # clearly convert to integer\n",
        "        )\n",
        "        final_columns = ['PRUID','geo_name', 'geo_code', 'Ops_below_250k', 'Ops_250k_or_more', 'Total_Ops']\n",
        "        summary_df = summary_df[final_columns]\n",
        "        summary_df.columns = final_columns\n",
        "        summary_df = summary_df.copy().sort_values('PRUID').reset_index(drop=True)\n",
        "        return summary_df[final_columns].reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        print('not configured for US yet')\n",
        "\n",
        "def extract_2_vectorized_stats(df, geo_level, geo_col, country_geo_val, pivot1, pivot2, pivot1_label, pivot2_label, pivot_total_label, country, pivot_col, values_col, geo_code_col, geo_name_col, fips_col, agg_function='sum'):\n",
        "    if country == 'CA':\n",
        "        df_filtered = df.loc[df[geo_col] != country_geo_val] if geo_level != 'national' else df.loc[df[geo_col] == country_geo_val]\n",
        "        df_filtered = df[(df[geo_col] != country_geo_val) & ~(df[geo_col].str.contains(','))] if geo_level == 'provincial' else df_filtered\n",
        "        # Example assuming your dataframe is df:\n",
        "        summary_df = df_filtered.pivot_table(\n",
        "            index=geo_col,\n",
        "            columns=pivot_col,\n",
        "            values=values_col,\n",
        "            aggfunc=agg_function,\n",
        "            fill_value=0\n",
        "        ).reset_index()\n",
        "\n",
        "        # Explicit aggregation for below and above $250k\n",
        "        summary_df[pivot1_label] = summary_df[pivot1].sum(axis=1)\n",
        "        summary_df[pivot2_label] = summary_df[pivot2].sum(axis=1)\n",
        "        summary_df[geo_code_col] = summary_df[geo_col].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "        summary_df[geo_name_col] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "        summary_df[pivot_total_label] = summary_df[pivot1_label] + summary_df[pivot2_label]\n",
        "        summary_df[fips_col] = (\n",
        "            summary_df[geo_code_col]\n",
        "            .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "            .str[:2]                             # clearly take the first two characters\n",
        "            .astype(int)                         # clearly convert to integer\n",
        "        )\n",
        "        final_columns = [fips_col, geo_code_col, geo_name_col, pivot1_label, pivot2_label, pivot_total_label]\n",
        "        summary_df = summary_df[final_columns]\n",
        "        summary_df.columns = final_columns\n",
        "        summary_df = summary_df.copy().sort_values(fips_col).reset_index(drop=True)\n",
        "        return summary_df[final_columns].reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        print('not configured for US yet')\n",
        "\n",
        "def load_or_download_csv(local_filename, data_url):\n",
        "    \"\"\"\n",
        "    Loads a CSV file locally or downloads it if not available locally.\n",
        "    Handles CSV files compressed in ZIP, GZIP, or uncompressed formats.\n",
        "\n",
        "    Args:\n",
        "        local_filename (str): Path to the local CSV file.\n",
        "        data_url (str): URL to download the CSV from if not available locally.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataframe from CSV.\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_filename):\n",
        "        try:\n",
        "            df = pd.read_csv(local_filename)\n",
        "            print(f\"Loaded data successfully from {local_filename}.\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading local file, will attempt download: {e}\")\n",
        "\n",
        "    print(f\"Downloading data from {data_url}...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    content_type = response.headers.get('Content-Type', '').lower()\n",
        "\n",
        "    # Handle ZIP compressed files\n",
        "    if 'zip' in content_type or data_url.lower().endswith('.zip'):\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            csv_files = [name for name in z.namelist() if name.endswith('.csv')]\n",
        "            if not csv_files:\n",
        "                raise ValueError(\"No CSV file found in ZIP archive.\")\n",
        "            with z.open(csv_files[0]) as f:\n",
        "                df = pd.read_csv(f)\n",
        "\n",
        "    # Handle GZIP compressed files\n",
        "    elif 'gzip' in content_type or data_url.lower().endswith('.gz'):\n",
        "        with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "    # Handle regular CSV files\n",
        "    else:\n",
        "        df = pd.read_csv(BytesIO(response.content))\n",
        "\n",
        "    # Save locally\n",
        "    df.to_csv(local_filename, index=False)\n",
        "    print(f\"Data downloaded and saved locally to {local_filename}.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def simplify_geom(geom, tolerance=0.1):\n",
        "    return geom.simplify(tolerance, preserve_topology=True)\n",
        "\n",
        "def safe_extract(x, item='value'):\n",
        "    try:\n",
        "        val = x[0][item]\n",
        "        return val.strip() if isinstance(val, str) else val\n",
        "    except (IndexError, KeyError, TypeError, AttributeError):\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_trends_from_search_api(kw_list, api_key, geo='US'):\n",
        "    dfs = []\n",
        "    kw_list = kw_list if isinstance(kw_list, list) else [kw_list]\n",
        "    url = f'https://www.searchapi.io/api/v1/search?api_key={api_key}'\n",
        "    for kw in kw_list:\n",
        "        params = {\n",
        "            \"engine\": \"google_trends\",\n",
        "            \"q\": kw,\n",
        "            \"data_type\": \"GEO_MAP\",\n",
        "            \"tz\": 360,\n",
        "            \"geo\": geo,\n",
        "            \"resolution\": \"REGION\",\n",
        "            \"timeframe\": \"today 12-m\",\n",
        "            \"inc_low_vol\": True,\n",
        "            \"region\": \"REGION\",\n",
        "            \"api_key\": api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        interest_raw = response.json().get('interest_by_region', [])\n",
        "\n",
        "        df = pd.DataFrame(interest_raw)\n",
        "        if df.empty:\n",
        "            print(f\"No data found for keyword: {kw}\")\n",
        "            continue\n",
        "        df['region'] = df['name'].str.strip().str.upper()\n",
        "\n",
        "        val_col_name = f\"{kw.replace(' ', '_').lower()}\"\n",
        "        df[val_col_name] = df['values'].apply(safe_extract, item='extracted_value')\n",
        "        df[val_col_name] = df[val_col_name].fillna(0).astype(int)\n",
        "\n",
        "        dfs.append(df[['region', val_col_name]])\n",
        "\n",
        "    # Concatenate and explicitly aggregate by region\n",
        "    combo_df = pd.concat(dfs)\n",
        "    combo_df = combo_df.groupby('region').sum()\n",
        "\n",
        "    return combo_df\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def safe_extract(value, item='extracted_value'):\n",
        "    if isinstance(value, dict):\n",
        "        return value.get(item, 0)\n",
        "    return 0\n",
        "\n",
        "def fetch_trend_for_kw(kw, api_key, geo='US'):\n",
        "    url = f'https://www.searchapi.io/api/v1/search'\n",
        "    params = {\n",
        "        \"engine\": \"google_trends\",\n",
        "        \"q\": kw,\n",
        "        \"data_type\": \"GEO_MAP\",\n",
        "        \"tz\": 360,\n",
        "        \"geo\": geo,\n",
        "        \"resolution\": \"REGION\",\n",
        "        \"timeframe\": \"today 12-m\",\n",
        "        \"inc_low_vol\": True,\n",
        "        \"region\": \"REGION\",\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    interest_raw = response.json().get('interest_by_region', [])\n",
        "\n",
        "    df = pd.DataFrame(interest_raw)\n",
        "    if df.empty:\n",
        "        print(f\"No data found for keyword: {kw}\")\n",
        "        return None\n",
        "\n",
        "    df['region'] = df['name'].str.strip().str.upper()\n",
        "\n",
        "    val_col_name = f\"{kw.replace(' ', '_').lower()}\"\n",
        "    df[val_col_name] = df['values'].apply(safe_extract, item='extracted_value')\n",
        "    df[val_col_name] = df[val_col_name].fillna(0).astype(int)\n",
        "\n",
        "    return df[['region', val_col_name]]\n",
        "\n",
        "def load_trends_from_search_api_parallel(kw_list, api_key, geo='US', max_threads=10):\n",
        "    kw_list = kw_list if isinstance(kw_list, list) else [kw_list]\n",
        "\n",
        "    dfs = []\n",
        "    with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
        "        future_to_kw = {\n",
        "            executor.submit(fetch_trend_for_kw, kw, api_key, geo): kw for kw in kw_list\n",
        "        }\n",
        "\n",
        "        for future in as_completed(future_to_kw):\n",
        "            kw = future_to_kw[future]\n",
        "            try:\n",
        "                df = future.result()\n",
        "                if df is not None:\n",
        "                    dfs.append(df)\n",
        "            except Exception as e:\n",
        "                print(f\"Exception for keyword {kw}: {e}\")\n",
        "\n",
        "    # Concatenate and aggregate\n",
        "    if dfs:\n",
        "        combo_df = pd.concat(dfs)\n",
        "        combo_df = combo_df.groupby('region').sum()\n",
        "        return combo_df\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def estimate_audience_distribution(search_df, population_df, total_audience=38_000_000):\n",
        "    # Sum total searches per state\n",
        "    state_searches = search_df.groupby(['geo_name'])['avg_monthly_searches'].sum().reset_index()\n",
        "\n",
        "    # Merge with population data\n",
        "    merged_df = state_searches.merge(population_df, on='geo_name', how='inner')\n",
        "\n",
        "    # Calculate Search-to-Population Index (SPI)\n",
        "    merged_df['SPI'] = merged_df['avg_monthly_searches'] / merged_df['total_population']\n",
        "\n",
        "    # Normalize SPI\n",
        "    merged_df['normalized_SPI'] = merged_df['SPI'] / merged_df['SPI'].sum()\n",
        "\n",
        "    # Estimate audience per state\n",
        "    merged_df['estimated_audience'] = (merged_df['normalized_SPI'] * total_audience).astype(int)\n",
        "\n",
        "    return merged_df[['geo_code', 'geo_name', 'total_population', 'avg_monthly_searches', 'estimated_audience']]\n",
        "\n",
        "\n",
        "def estimate_search_population_indexes(search_df, population_df, population_col='total_population', baseline_audince_factor=None, interest_multiplier=None):\n",
        "    # Sum total searches per state\n",
        "    state_searches = search_df.groupby(['geo_name'])['avg_monthly_searches'].sum().reset_index()\n",
        "\n",
        "    # Merge with population data\n",
        "    merged_df = state_searches.merge(population_df, on='geo_name', how='inner')\n",
        "\n",
        "\n",
        "    # Calculate Search-to-Population Index (SPI)\n",
        "    merged_df['SPI'] = merged_df['avg_monthly_searches'] / merged_df[population_col]\n",
        "\n",
        "    # Normalize SPI\n",
        "    merged_df['normalized_SPI'] = merged_df['SPI'] / merged_df['SPI'].sum()\n",
        "    median_normalized_SPI = merged_df['normalized_SPI'].median()\n",
        "    merged_df['median_normalized_SPI'] = median_normalized_SPI\n",
        "    merged_df['interest_scaling_factor'] = merged_df['normalized_SPI'] - merged_df['median_normalized_SPI']\n",
        "    if baseline_audince_factor is not None and interest_multiplier is not None:\n",
        "        merged_df['estimated_audience_factor'] = (baseline_audince_factor + (merged_df['interest_scaling_factor']))\n",
        "    else:\n",
        "        merged_df['estimated_audience_factor'] = np.nan\n",
        "    return merged_df[['geo_code', 'geo_name', population_col, 'avg_monthly_searches', 'SPI', 'normalized_SPI', 'median_normalized_SPI', 'interest_scaling_factor', 'estimated_audience_factor']]\n",
        "\n",
        "def estimate_regional_search_interest(search_df, population_df):\n",
        "    # Sum total searches per state\n",
        "    state_searches = search_df.groupby(['geo_name'])['avg_monthly_searches'].sum().reset_index()\n",
        "    total_searches = state_searches['avg_monthly_searches'].sum()\n",
        "    state_searches['avg_monthly_interest_percent'] = state_searches['avg_monthly_searches'] / total_searches\n",
        "\n",
        "    # Merge with population data\n",
        "    merged_df = state_searches.merge(population_df, on='geo_name', how='inner')\n",
        "\n",
        "    # Calculate Search-to-Population Index (SPI)\n",
        "    merged_df['SPI'] = merged_df['avg_monthly_searches'] / merged_df['total_population']\n",
        "\n",
        "    # Normalize SPI\n",
        "    merged_df['normalized_SPI'] = merged_df['SPI'] / merged_df['SPI'].sum()\n",
        "\n",
        "    return merged_df[['geo_code', 'geo_name', 'total_population', 'avg_monthly_searches', 'SPI', 'normalized_SPI']]\n",
        "\n",
        "\n",
        "def convert_state_abbrev_to_full(df, abbrev_col='state'):\n",
        "    df['state_full'] = df[abbrev_col].apply(lambda x: us.states.lookup(x).name if us.states.lookup(x) else x).str.upper()\n",
        "    df['geo_name'] = df['state_full'].str.upper()\n",
        "    df['geo_code'] = df[abbrev_col]\n",
        "    return df"
      ],
      "metadata": {
        "id": "vA6dmsLE7kif",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load US and Canada State / Province shapefiles\n",
        "\n",
        "# Remove GDAL's GeoJSON size limit explicitly\n",
        "os.environ['OGR_GEOJSON_MAX_OBJ_SIZE'] = '0'\n",
        "\n",
        "try:\n",
        "    gdf_us_states = gpd.read_file('simplified_us_state_geos.gpkg')\n",
        "except Exception as e:\n",
        "    print(\"File not found. Attempting to fetch and load shapefile.\")\n",
        "\n",
        "    geo_address = 'https://www2.census.gov/geo/tiger/TIGER2023/STATE/tl_2023_us_state.zip'\n",
        "\n",
        "    gdf_us_states = (fetch_unzip_load_shapefile_flexible(geo_address, 'state_tiger').to_crs('EPSG:5070'))\n",
        "# gdf_us_states = gdf_us_states.simplify(\n",
        "#         tolerance=0.05,  # Adjust this if needed; higher = more simplified\n",
        "#         preserve_topology=True\n",
        "#     )\n",
        "# gdf_us_states = gdf_us_states.simplify()\n",
        "\n",
        "# https://www2.census.gov/geo/tiger/TIGER2024/ESTATE/tl_2024_78_estate.zip\n",
        "\n",
        "# Load geographic shapes for US/Canada\n",
        "# gdf_us_states = gpd.read_file('/content/us_state/tl_2024_us_state.shp').to_crs('EPSG:5070')\n",
        "    gdf_us_states_ref = gdf_us_states[['STATEFP', 'NAME']].drop_duplicates().sort_values('STATEFP').reset_index(drop=True)\n",
        "    gdf_us_states_ref['NAME'] = gdf_us_states_ref['NAME'].str.upper()\n",
        "    gdf_us_states['NAME'] = gdf_us_states['NAME'].str.upper()\n",
        "\n",
        "# save this ref to a gcs bucket\n",
        "    gdf_us_states_ref.to_csv('us_state_ref.csv', index=False)\n",
        "    with mp.Pool(mp.cpu_count()) as pool:\n",
        "        simplified_geometries = pool.map(simplify_geom, gdf_us_states.geometry)\n",
        "    gdf_us_states['geometry'] = simplified_geometries\n",
        "    gdf_us_states.to_file('simplified_us_state_geos.gpkg', driver='GPKG')\n",
        "\n",
        "\n",
        "\n",
        "repositioned_us = reposition_alaska_hawaii(gdf_us_states).sort_values('STATEFP').reset_index(drop=True)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "\n",
        "try:\n",
        "    canada_provinces_gdf = gpd.read_file('simplified_canada_province_geos.gpkg')\n",
        "except Exception as e:\n",
        "    print(\"File not found. Attempting to fetch and load shapefile.\")\n",
        "    url = \"https://www12.statcan.gc.ca/census-recensement/2021/geo/sip-pis/boundary-limites/files-fichiers/lpr_000b21a_e.zip?st=wk4IrLBG\"\n",
        "    output_directory = '/content/province-shapes/'\n",
        "    canada_provinces_gdf = fetch_unzip_load_shapefile_flexible(url, output_directory)\n",
        "# canada_provinces_gdf = canada_provinces_gdf.simplify(\n",
        "#         tolerance=0.05,  # Adjust this if needed; higher = more simplified\n",
        "#         preserve_topology=True\n",
        "#     )\n",
        "\n",
        "    if canada_provinces_gdf is not None:\n",
        "        canada_provinces_gdf['PRENAME'] = canada_provinces_gdf['PRENAME'].str.upper()\n",
        "        canada_provinces_gdf = canada_provinces_gdf.to_crs('EPSG:5070')\n",
        "        print(\"\\nGeoDataFrame loaded:\")\n",
        "        print(canada_provinces_gdf.head())\n",
        "        # # Adjust the tolerance to balance simplification and detail retention\n",
        "        # simplification_tolerance = 0.05  # Example tolerance (increase to simplify more aggressively)\n",
        "        # # Simplify geometries explicitly\n",
        "        # canada_provinces_gdf['geometry'] = canada_provinces_gdf['geometry'].simplify(\n",
        "        #     tolerance=simplification_tolerance, preserve_topology=True\n",
        "        # )\n",
        "        # # Ensure geometries remain valid\n",
        "        # canada_provinces_gdf = canada_provinces_gdf[canada_provinces_gdf.is_valid]\n",
        "        with mp.Pool(mp.cpu_count()) as pool:\n",
        "            simplified_geometries = pool.map(simplify_geom, canada_provinces_gdf.geometry)\n",
        "\n",
        "        canada_provinces_gdf['geometry'] = simplified_geometries\n",
        "        canada_provinces_gdf = canada_provinces_gdf.copy().sort_values('PRUID').reset_index(drop=True)\n",
        "        canada_provinces_gdf.to_file('simplified_canada_province_geos.gpkg', driver='GPKG')\n",
        "    else:\n",
        "        print(\"No valid GeoDataFrame loaded.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J1ahh-Ctt2UG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  @title farm operator repair keywords\n",
        "us_high_revenue_final_keywords = [\n",
        "    \"tractor hydraulic repair\",\n",
        "    \"diesel tractor troubleshooting\",\n",
        "    \"tractor transmission repair\",\n",
        "    \"combine maintenance\",\n",
        "    \"farm equipment diagnostics\",\n",
        "    \"precision agriculture calibration\",\n",
        "    \"tractor emissions repair\",\n",
        "    \"planter upgrades\",\n",
        "    \"tractor GPS installation\",\n",
        "    \"tractor AC repair\",\n",
        "    \"OEM tractor parts\",\n",
        "    \"tractor performance tuning\",\n",
        "    \"tractor PTO troubleshooting\",\n",
        "    \"tractor repair manuals\",\n",
        "    \"tractor ECU issues\",\n",
        "    \"yield monitor setup\",\n",
        "    \"hydraulic retrofitting\",\n",
        "    \"tractor service training\",\n",
        "    \"preventive tractor maintenance\",\n",
        "    \"engine rebuild for tractors\",\n",
        "    \"John Deere equipment maintenance\",\n",
        "    \"John Deere diagnostic tools\",\n",
        "    \"john deere part\",\n",
        "    \"tractor winter preparation\",\n",
        "    \"tractor block heater install\",\n",
        "    \"tractor engine overhaul\",\n",
        "    \"tractor electrical diagnostics\",\n",
        "    \"tractor hydraulic systems\",\n",
        "    \"tractor engine upgrades\",\n",
        "    \"tractor troubleshooting guide\",\n",
        "    \"tractor attachment repair\",\n",
        "]\n",
        "\n",
        "ca_high_revenue_final_keywords = [\n",
        "    \"tractor repair Canada\",\n",
        "    \"diesel tractor service\",\n",
        "    \"combine maintenance\",\n",
        "    \"tractor diagnostics tools\",\n",
        "    \"precision agriculture Canada\",\n",
        "    \"tractor GPS systems\",\n",
        "    \"tractor AC service\",\n",
        "    \"tractor emission repair\",\n",
        "    \"large tractor tires\",\n",
        "    \"farm planter equipment\",\n",
        "    \"OEM tractor parts Canada\",\n",
        "    \"tractor upgrades Canada\",\n",
        "    \"tractor PTO repair\",\n",
        "    \"tractor repair manuals\",\n",
        "    \"tractor preventive maintenance\",\n",
        "    \"tractor training courses\",\n",
        "    \"hydraulic systems repair\",\n",
        "    \"tractor engine rebuild\",\n",
        "    \"tractor winterization Canada\",\n",
        "    \"tractor block heater\",\n",
        "    \"farm equipment repair\",\n",
        "    \"agricultural machinery repair\",\n",
        "    \"tractor fuel system service\",\n",
        "    \"farm machinery maintenance\",\n",
        "    \"equipment hydraulic repair\",\n",
        "    \"agricultural machinery parts\",\n",
        "    \"heavy equipment repair\",\n",
        "    \"farm equipment parts\",\n",
        "    \"tractor servicing\",\n",
        "    \"tractor electronic diagnostics\",\n",
        "    # French-language keywords\n",
        "    \"rparation tracteur agricole\",\n",
        "    \"entretien diesel tracteur\",\n",
        "    \"diagnostic tracteur\",\n",
        "    \"manuel entretien tracteur\",\n",
        "    \"chauffe-bloc tracteur\",\n",
        "    \"hivernisation tracteur\",\n",
        "    \"pices quipement agricole\",\n",
        "    \"rparation quipement agricole\",\n",
        "]\n",
        "\n",
        "us_low_revenue_final_keywords = [\n",
        "    \"tractor oil change\",\n",
        "    \"lawn mower repair\",\n",
        "    \"compact tractor issues\",\n",
        "    \"small tractor maintenance\",\n",
        "    \"tractor belt replacement\",\n",
        "    \"tractor battery issues\",\n",
        "    \"tractor tire replacement\",\n",
        "    \"tractor decal restoration\",\n",
        "    \"antique tractor refurbishing\",\n",
        "    \"tractor painting guide\",\n",
        "    \"tractor seat repair\",\n",
        "    \"tractor hydraulic leak fix\",\n",
        "    \"tractor rust removal\",\n",
        "    \"tractor basic maintenance\",\n",
        "    \"john deere part\",\n",
        "    \"tractor DIY videos\",\n",
        "    \"tractor aftermarket parts\",\n",
        "    \"tractor carburetor repair\",\n",
        "    \"garden tractor upkeep\",\n",
        "    \"tractor electrical issues\",\n",
        "    \"John Deere lawn tractor repair\",\n",
        "    \"compact tractor attachments\",\n",
        "    \"tractor winterizing\",\n",
        "    \"tractor snowblower upkeep\",\n",
        "    \"cold weather tractor battery\",\n",
        "    \"tractor engine tune-up\",\n",
        "    \"tractor starter repair\",\n",
        "    \"tractor maintenance schedule\",\n",
        "    \"tractor troubleshooting\",\n",
        "    \"tractor attachments DIY\",\n",
        "    \"tractor parts online\",\n",
        "]\n",
        "\n",
        "ca_low_revenue_final_keywords = [\n",
        "    \"tractor oil change Canada\",\n",
        "    \"compact tractor problems\",\n",
        "    \"small tractor repair\",\n",
        "    \"tractor belt replacement\",\n",
        "    \"tractor battery replacement\",\n",
        "    \"compact tractor tires\",\n",
        "    \"tractor restoration\",\n",
        "    \"tractor upholstery\",\n",
        "    \"tractor hydraulic leaks\",\n",
        "    \"tractor rust removal\",\n",
        "    \"tractor carburetor repair\",\n",
        "    \"tractor maintenance Canada\",\n",
        "    \"tractor parts Canada\",\n",
        "    \"aftermarket tractor parts\",\n",
        "    \"tractor electrical repair\",\n",
        "    \"tractor DIY\",\n",
        "    \"tractor snowblower maintenance\",\n",
        "    \"tractor block heater\",\n",
        "    \"tractor winter preparation\",\n",
        "    \"garden tractor repair\",\n",
        "    \"compact tractor care\",\n",
        "    \"farm tractor repair\",\n",
        "    \"lawn tractor maintenance\",\n",
        "    \"small engine tractor repair\",\n",
        "    \"agricultural equipment maintenance\",\n",
        "    \"tractor troubleshooting\",\n",
        "    \"equipment parts Canada\",\n",
        "    \"tractor attachment repair\",\n",
        "    # French-language keywords\n",
        "    \"vidange huile tracteur\",\n",
        "    \"entretien tracteur\",\n",
        "    \"rparation tondeuse\",\n",
        "    \"chauffe-bloc tracteur\",\n",
        "    \"hivernisation tracteur\",\n",
        "    \"entretien souffleuse\",\n",
        "    \"rparation rouille tracteur\",\n",
        "    \"pices tracteur Canada\",\n",
        "    \"manuel rparation tracteur\",\n",
        "]\n",
        "standard_diy_indicator_keywords = [\n",
        "    \"John Deere parts\",\n",
        "    \"John Deere tractor parts\",\n",
        "    \"John Deere repair\",\n",
        "    \"John Deere service manual\",\n",
        "    \"John Deere tractor maintenance\",\n",
        "    \"JD tractor troubleshooting\",\n",
        "    \"JD tractor parts online\",\n",
        "    \"John Deere oil change\",\n",
        "    \"John Deere tractor accessories\",\n",
        "    \"John Deere tractor filters\",\n",
        "    \"JD parts catalog\",\n",
        "    \"John Deere DIY repair\",\n",
        "    \"John Deere aftermarket parts\",\n",
        "    \"John Deere mower repair\",\n",
        "    \"JD tractor belts\",\n",
        "    \"John Deere hydraulic repair\",\n",
        "    \"John Deere battery replacement\",\n",
        "    \"John Deere tractor tires\",\n",
        "    \"John Deere tractor attachments\",\n",
        "    \"John Deere tractor electrical repair\",\n",
        "    \"John Deere DIY parts\",\n",
        "    \"John Deere tractor fluid changes\",\n",
        "    \"John Deere engine maintenance\",\n",
        "    \"John Deere tractor troubleshooting\",\n",
        "    \"John Deere tractor DIY videos\",\n",
        "    \"John Deere replacement blades\",\n",
        "    \"JD tractor lighting kits\",\n",
        "    \"John Deere maintenance schedule\",\n",
        "    \"JD parts near me\",\n",
        "    \"John Deere tractor restoration\",\n",
        "]"
      ],
      "metadata": {
        "id": "yiZ-cGQ6IBKu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title prep state and provinces with population and google location ids for states and provinces\n",
        "\n",
        "# load us census data\n",
        "\n",
        "# Convert necessary columns to integers explicitly\n",
        "age_vars = ['B01001_001E', 'B01001_003E', 'B01001_004E', 'B01001_005E', 'B01001_006E',\n",
        "            'B01001_027E', 'B01001_028E', 'B01001_029E', 'B01001_030E',\n",
        "            'B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E', 'B01001_024E', 'B01001_025E',\n",
        "            'B01001_044E', 'B01001_045E', 'B01001_046E', 'B01001_047E', 'B01001_048E', 'B01001_049E']\n",
        "\n",
        "acs_variables = {\n",
        "    \"B01003_001E\": \"total_population\",\n",
        "    \"B25001_001E\": \"total_housing_units\",\n",
        "    \"B25003_001E\": \"total_owner_occupied_units\",\n",
        "    \"B01001_003E\": \"male_under_5\",\n",
        "    \"B01001_004E\": \"male_5_to_9\",\n",
        "    \"B01001_005E\": \"male_10_to_14\",\n",
        "    \"B01001_006E\": \"male_15_to_17\",\n",
        "    \"B01001_027E\": \"female_under_5\",\n",
        "    \"B01001_028E\": \"female_5_to_9\",\n",
        "    \"B01001_029E\": \"female_10_to_14\",\n",
        "    \"B01001_030E\": \"female_15_to_17\",\n",
        "}\n",
        "\n",
        "df_acs_pop = c.acs5.get(age_vars, {'for': 'county:*'})\n",
        "df_acs_pop = pd.DataFrame(df_acs_pop)\n",
        "\n",
        "\n",
        "\n",
        "for col in age_vars:\n",
        "    df_acs_pop[col] = df_acs_pop[col].astype(int)\n",
        "\n",
        "# Under 18 calculation\n",
        "df_acs_pop['under_18'] = (\n",
        "    df_acs_pop[['B01001_003E', 'B01001_004E', 'B01001_005E', 'B01001_006E',\n",
        "                  'B01001_027E', 'B01001_028E', 'B01001_029E', 'B01001_030E']].sum(axis=1)\n",
        ")\n",
        "\n",
        "# 65 and older calculation\n",
        "df_acs_pop['age_65_plus'] = (\n",
        "    df_acs_pop[['B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E', 'B01001_024E', 'B01001_025E',\n",
        "                  'B01001_044E', 'B01001_045E', 'B01001_046E', 'B01001_047E', 'B01001_048E', 'B01001_049E']].sum(axis=1)\n",
        ")\n",
        "\n",
        "# Adults 1864 clearly calculated:\n",
        "df_acs_pop['adults_18_to_64'] = df_acs_pop['B01001_001E'] - (df_acs_pop['under_18'] + df_acs_pop['age_65_plus'])\n",
        "\n",
        "\n",
        "# for col in acs_variables.values():\n",
        "#     df_acs_pop[col] = pd.to_numeric(df_acs_pop[col]).astype(int)\n",
        "\n",
        "# df_acs_pop['under_18'] = (\n",
        "#     df_acs_pop['male_under_5'].astype(int) + df_acs_pop['female_under_5'].astype(int) +\n",
        "#     df_acs_pop['male_5_to_9'].astype(int) + df_acs_pop['female_5_to_9'].astype(int) +\n",
        "#     df_acs_pop['male_10_to_14'].astype(int) + df_acs_pop['female_10_to_14'].astype(int) +\n",
        "#     df_acs_pop['male_15_to_17'].astype(int) + df_acs_pop['female_15_to_17'].astype(int)\n",
        "# )\n",
        "\n",
        "# df_acs_pop['18_and_over'] = df_acs_pop['total_population'] - df_acs_pop['under_18']\n",
        "# df_acs_pop['total_population_all'] = df_acs_pop['18_and_over']\n",
        "# df_acs_pop = df_acs_pop.rename(columns={'18_and_over': 'total_population'})\n",
        "\n",
        "# Convert numeric columns\n",
        "\n",
        "# Create full FIPS code\n",
        "df_acs_pop['FIPS'] = df_acs_pop['state'] + df_acs_pop['county']\n",
        "\n",
        "df_us_state_pop = df_acs_pop[['state', 'adults_18_to_64']].groupby('state').sum().sort_values(by='state').reset_index()\n",
        "\n",
        "\n",
        "df_us_state_pop_clean = convert_state_abbrev_to_full(df_us_state_pop)\n",
        "df_us_state_pop_clean['geo_country'] = 'us'\n",
        "df_us_state_pop_clean['geo_code_na'] = df_us_state_pop_clean['geo_country'] + '-' + df_us_state_pop_clean['geo_code']\n",
        "df_us_state_pop = df_us_state_pop_clean.loc[((df_us_state_pop_clean['state'] != '11') & (df_us_state_pop_clean['state'].astype(int) < 58)),  ['geo_code','geo_code_na', 'geo_name', 'geo_country', 'adults_18_to_64']].copy().sort_values(by='geo_code').reset_index(drop=True)\n",
        "del df_us_state_pop_clean\n",
        "gc.collect()\n",
        "df_us_state_pop['adults_18_to_64'].sum()\n",
        "# 202,684,160 adults 18 - 64\n",
        "# load ca census population data 202,684,160\n",
        "\n",
        "# ca_census_vars = {\n",
        "#     \"Population and dwelling counts (13): Population, 2021 [1]\": \"total_population\",\n",
        "#     \"Population and dwelling counts (13): Private dwellings occupied by usual residents, 2021 [7]\": \"total_private_dwellings\"\n",
        "# }\n",
        "# ca_age_exclude = '0 to 17 years'\n",
        "ca_pop_fields = {\"18 to 64 years\": \"adults_18_to_64\"}\n",
        "ca_pop_df = sc.table_to_df(\"17-10-0005-01\") # canadian population\n",
        "recent_ca_pop = ca_pop_df[ca_pop_df['REF_DATE'] == ca_pop_df['REF_DATE'].max()]\n",
        "# recent_ca_pop.loc[recent_ca_pop['DGUID'].str.len() == 11]\n",
        "\n",
        "\n",
        "recent_ca_pop['VALUE'] = recent_ca_pop['VALUE'].fillna(0.0).astype(int)\n",
        "# ca total pop 21,962,032\n",
        "# ca 18 to 64 12,429,081\n",
        "recent_ca_pop_18_64 = recent_ca_pop.loc[(recent_ca_pop['Age group'].isin(list(ca_pop_fields.keys()))) & (recent_ca_pop['Gender'] == 'Total - gender')].copy()\n",
        "\n",
        "# for col in ca_census_vars.keys():\n",
        "#     ca_pop_df[col] = pd.to_numeric(ca_pop_df[col]).fillna(0.0).astype(int)\n",
        "# ca_pop_df = ca_pop_df.rename(columns=ca_census_vars)\n",
        "\n",
        "\n",
        "ca_province_pop_df = recent_ca_pop_18_64[recent_ca_pop_18_64['DGUID'].str.len() == 11].copy().reset_index(drop=True)\n",
        "ca_province_pop_df['adults_18_to_64'] = ca_province_pop_df['VALUE'].fillna(0.0).astype(int)\n",
        "ca_province_pop_df['geo_code'] = ca_province_pop_df['DGUID'].str[-2:]\n",
        "ca_province_pop_df['geo_country'] = 'ca'\n",
        "ca_province_pop_df['geo_code_na'] = ca_province_pop_df['geo_country'] + '-' +  ca_province_pop_df['DGUID'].str[-2:]\n",
        "ca_province_pop_df['geo_name'] = ca_province_pop_df['GEO'].str.upper()\n",
        "ca_province_pop_df = ca_province_pop_df[['geo_code','geo_code_na', 'geo_name', 'geo_country', 'adults_18_to_64']].sort_values(by='geo_code').copy().reset_index(drop=True)\n",
        "ca_province_pop_df['adults_18_to_64'].sum()\n",
        "# 25,844,882 adults 18 to 64\n",
        "\n",
        "# load google geos for states and provinces\n",
        "google_geos = fetch_google_ads_geo_targets(client)\n",
        "\n",
        "\n",
        "google_us_states = google_geos[((google_geos['country_code'] == 'US') & (google_geos['target_type'] == 'State'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_us_states = google_us_states[['name', 'google_id']].sort_values('name').reset_index(drop=True)\n",
        "google_us_states['name'] = google_us_states['name'].str.upper()\n",
        "df_us_state_pop = pd.merge(df_us_state_pop, google_us_states, left_on='geo_name', right_on='name', how='left')\n",
        "\n",
        "# google_us_states.rename(columns={'Name': 'NAME', 'Criteria ID': 'geo_id'}, inplace=True)\n",
        "# us_states_google = google_us_states.set_index('name')['id'].to_dict()\n",
        "google_ca_provinces = google_geos[((google_geos['country_code'] == 'CA') & (google_geos['target_type'] == 'Province'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_ca_provinces['name'] = google_ca_provinces['name'].str.upper()\n",
        "google_ca_provinces = google_ca_provinces[['name', 'google_id']].sort_values('name').reset_index(drop=True)\n",
        "ca_province_pop_df = pd.merge(ca_province_pop_df, google_ca_provinces, left_on='geo_name', right_on='name', how='left')\n",
        "ca_province_pop_df['google_id'] = ca_province_pop_df['google_id'].fillna(0.0).astype(int)\n",
        "# ca_province_pop_df.rename(columns={'Name': 'NAME', 'Criteria ID': 'geo_id'}, inplace=True\n",
        "# ca_provinces = google_ca_provinces.set_index('name')['id'].to_dict()\n",
        "merge_cols = ['geo_country', 'geo_code','geo_code_na', 'geo_name', 'google_id',  'adults_18_to_64']\n",
        "na_population_by_region = pd.concat([df_us_state_pop[merge_cols], ca_province_pop_df[merge_cols]], ignore_index=True).copy()\n",
        "\n",
        "us_states_google = df_us_state_pop.set_index('geo_name')['google_id'].to_dict()\n",
        "ca_provinces_google = ca_province_pop_df.loc[ca_province_pop_df['google_id'] != 0].set_index('geo_name')['google_id'].to_dict()\n",
        "na_regions_google = {**us_states_google, **ca_provinces_google}\n"
      ],
      "metadata": {
        "id": "uQ9W4gxK-Y6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAbsKHLuuJuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# kws = us_high_revenue_keywords\n",
        "kws = us_high_revenue_final_keywords\n",
        "\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, us_states_google)\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].astype(int)\n",
        "paid_trends['avg_monthly_searches'].sum()\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "us_over250_paid_trends = paid_trends\n",
        "\n",
        "kws = us_low_revenue_final_keywords\n",
        "\n",
        "\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, us_states_google)\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].astype(int)\n",
        "paid_trends['avg_monthly_searches'].sum()\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "us_under_250_paid_trends = paid_trends\n",
        "\n",
        "kws = ca_high_revenue_final_keywords\n",
        "ca_paid_trends = get_exact_keyword_volumes(client, kws, ca_provinces_google)\n",
        "ca_paid_trends['avg_monthly_searches'] = ca_paid_trends['avg_monthly_searches'].astype(int)\n",
        "ca_paid_trends['avg_monthly_searches'].sum()\n",
        "ca_paid_trends['geo_name'] = ca_paid_trends['state']\n",
        "ca_over250_paid_trends = ca_paid_trends\n",
        "ca_over250_paid_trends['avg_monthly_searches'].sum()\n",
        "\n",
        "kws = ca_low_revenue_final_keywords\n",
        "ca_paid_trends = get_exact_keyword_volumes(client, kws, ca_provinces_google)\n",
        "ca_paid_trends['avg_monthly_searches'] = ca_paid_trends['avg_monthly_searches'].astype(int)\n",
        "ca_paid_trends['avg_monthly_searches'].sum()\n",
        "ca_paid_trends['geo_name'] = ca_paid_trends['state']\n",
        "ca_under250_paid_trends = ca_paid_trends\n",
        "\n",
        "# trt interest\n",
        "\n",
        "# 96300\n",
        "# deep_lg_op_diy_results = paid_trends\n",
        "# deep_lg_op_diy_results['avg_monthly_searches'].sum()\n",
        "# 1420\n",
        "# estimate_search_population_indexes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0h7xsJusa8WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load USDA and Canada Agriculture census data\n",
        "# URL to USDA Census of Agriculture 2022 data (corrected link)\n",
        "data_url = 'https://www.nass.usda.gov/datasets/qs.census2022.txt.gz'\n",
        "\n",
        "# Download the GZIP file\n",
        "local_filename = 'us_agriculture.csv'\n",
        "try:\n",
        "    df = pd.read_csv(local_filename)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading data: - downloading\", e)\n",
        "    response = requests.get(data_url)\n",
        "    response.raise_for_status()\n",
        "    # Decompress and load the file into a pandas DataFrame\n",
        "    with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "        df = pd.read_csv(f, delimiter='\\t')  # adjust delimiter if necessary\n",
        "        df.to_csv(local_filename, index=False)\n",
        "df_us = df.copy()\n",
        "\n",
        "# data_url = base_url = \"https://www150.statcan.gc.ca/n1/en/tbl/csv/32100239-eng.zip?st=wk4IrLBG\"\n",
        "# local_filename = 'ca_agriculture.csv'\n",
        "# df_canada = load_or_download_csv(local_filename, data_url)\n",
        "df_canada = sc.table_to_df(\"32100239\")\n",
        "\n",
        "# df_canada = df_canada.rename(columns={'Total farm revenues distribution': 'revenue_distribution'})\n",
        "# df_canada['Total farm revenues distribution'].unique()\n",
        "\n",
        "\n",
        "geo_level='provincial'\n",
        "provincial_summary_canada = categorize_revenues(df_canada, geo_level=geo_level, geo_col='GEO', country_geo_val='Canada [000000000]', above_250k_cats=above_250k_categories_canada, below_250k_cats=below_250k_categories_canada, country='CA')\n",
        "\n",
        "print('/nn')\n",
        "print(provincial_summary_canada.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "aIfNLctg_JXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# workig on maturity model"
      ],
      "metadata": {
        "id": "XgsIuR6jmI2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['SOURCE_DESC',\n",
        " 'SECTOR_DESC',\n",
        " 'GROUP_DESC',\n",
        " 'COMMODITY_DESC',\n",
        " 'CLASS_DESC',\n",
        " 'PRODN_PRACTICE_DESC',\n",
        " 'UTIL_PRACTICE_DESC',\n",
        " 'STATISTICCAT_DESC',\n",
        " 'UNIT_DESC',\n",
        " 'SHORT_DESC',\n",
        " 'DOMAIN_DESC',\n",
        " 'DOMAINCAT_DESC',\n",
        " 'AGG_LEVEL_DESC',\n",
        " 'STATE_ANSI',\n",
        " 'STATE_FIPS_CODE',\n",
        " 'STATE_ALPHA',\n",
        " 'STATE_NAME',\n",
        " 'ASD_CODE',\n",
        " 'ASD_DESC',\n",
        " 'COUNTY_ANSI',\n",
        " 'COUNTY_CODE',\n",
        " 'COUNTY_NAME',\n",
        " 'REGION_DESC']\n",
        "\n",
        "df_us.columns.to_list()\n",
        "\n",
        "df_us['SECTOR_DESC'].drop_duplicates()\n",
        "# sector DEMOGRAPHICS\n",
        "#  grou_desc PRODUCERS, INCOME, FRUIT & TREE NUTS, CROP TOTALS\n",
        "age_groups = ['AGE GE 75',\n",
        " 'AGE 25 TO 34',\n",
        " 'AGE 55 TO 64',\n",
        " 'AGE 45 TO 54',\n",
        " 'AGE LT 25',\n",
        " 'AGE 35 TO 44',\n",
        " 'AGE 65 TO 74']\n",
        "df_us.loc[df_us['SECTOR_DESC'] == 'DEMOGRAPHICS', 'GROUP_DESC'].drop_duplicates()\n",
        "df_us_age = df_us.loc[(df_us['SECTOR_DESC'] == 'DEMOGRAPHICS') & (df_us['GROUP_DESC'] == 'PRODUCERS') & (df_us['UNIT_DESC'] == 'PRODUCERS') & (df_us['SHORT_DESC'].str.contains('AGE') )& (df_us['CLASS_DESC'].isin(age_groups) & (df_us['AGG_LEVEL_DESC'] == 'STATE'))].drop_duplicates().copy().reset_index(drop=True)\n",
        "df_us_age['VALUE_NUMERIC'] = pd.to_numeric(df_us_age['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "# df_us_age['COUNTY_CODE'] =  df_us_age['COUNTY_CODE'].astype(int)\n",
        "# df_us_age['fips'] = (df_us_age['STATE_FIPS_CODE'].astype(str).astype(str).str.zfill(2) + df_us_age['COUNTY_CODE'].astype(str).astype(str).str.zfill(3)).replace('.0','')\n",
        "df_us_age['fips'] = df_us_age['STATE_FIPS_CODE'].astype(str).astype(str).str.zfill(2)\n",
        "df_us_age_clean = df_us_age.sort_values(['fips','CLASS_DESC']).copy().reset_index(drop=True)\n",
        "fips_age_summary = df_us_age_clean.groupby(['fips', 'CLASS_DESC'], as_index=False)['VALUE_NUMERIC'].sum()\n",
        "# Compute total operations by FIPS explicitly\n",
        "fips_totals = fips_age_summary.groupby('fips')['VALUE_NUMERIC'].transform('sum')\n",
        "\n",
        "# Add percentage explicitly\n",
        "fips_age_summary['percent_of_ops'] = fips_age_summary['VALUE_NUMERIC'] / fips_totals * 100\n",
        "\n",
        "# GFR $5M+\n",
        "# Age 35-50\n",
        "\n",
        "age_pivot = fips_age_summary.pivot(index='fips', columns='CLASS_DESC', values=['VALUE_NUMERIC', 'percent_of_ops'])\n",
        "\n",
        "# Calculate explicitly for 35-50\n",
        "age_pivot['VALUE_NUMERIC', 'AGE 35 TO 50'] = (\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 35 TO 44'] +\n",
        "    0.6 * age_pivot['VALUE_NUMERIC', 'AGE 45 TO 54']\n",
        ")\n",
        "\n",
        "age_pivot['percent_of_ops', 'AGE 35 TO 50'] = (\n",
        "    age_pivot['percent_of_ops', 'AGE 35 TO 44'] +\n",
        "    0.6 * age_pivot['percent_of_ops', 'AGE 45 TO 54']\n",
        ")\n",
        "\n",
        "# Reset and flatten columns for convenience\n",
        "age_pivot.columns = ['_'.join(col).strip() for col in age_pivot.columns.values]\n",
        "age_pivot = age_pivot.reset_index()\n",
        "\n",
        "# View clearly\n",
        "print(age_pivot[['fips', 'VALUE_NUMERIC_AGE 35 TO 50', 'percent_of_ops_AGE 35 TO 50']])\n",
        "\n",
        "fips_age_summary = pd.merge(fips_age_summary, age_pivot[['fips', 'VALUE_NUMERIC_AGE 35 TO 50', 'percent_of_ops_AGE 35 TO 50']], on='fips', how='left')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Pivot clearly for calculation ease\n",
        "age_pivot = fips_age_summary.pivot(index='fips', columns='CLASS_DESC', values=['VALUE_NUMERIC', 'percent_of_ops'])\n",
        "\n",
        "# Explicit calculation of 4060 range\n",
        "age_pivot['VALUE_NUMERIC', 'AGE 40 TO 60'] = (\n",
        "    0.5 * age_pivot['VALUE_NUMERIC', 'AGE 35 TO 44'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 45 TO 54'] +\n",
        "    0.6 * age_pivot['VALUE_NUMERIC', 'AGE 55 TO 64']\n",
        ")\n",
        "\n",
        "age_pivot['percent_of_ops', 'AGE 40 TO 60'] = (\n",
        "    0.5 * age_pivot['percent_of_ops', 'AGE 35 TO 44'] +\n",
        "    age_pivot['percent_of_ops', 'AGE 45 TO 54'] +\n",
        "    0.6 * age_pivot['percent_of_ops', 'AGE 55 TO 64']\n",
        ")\n",
        "\n",
        "# Flatten columns explicitly\n",
        "age_pivot.columns = ['_'.join(col).strip() for col in age_pivot.columns.values]\n",
        "age_pivot.reset_index(inplace=True)\n",
        "\n",
        "# Clear view of results\n",
        "print(age_pivot[['fips', 'VALUE_NUMERIC_AGE 40 TO 60', 'percent_of_ops_AGE 40 TO 60']])\n",
        "\n",
        "fips_age_summary = pd.merge(fips_age_summary, age_pivot[['fips', 'VALUE_NUMERIC_AGE 40 TO 60', 'percent_of_ops_AGE 40 TO 60']], on='fips', how='left')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Pivot your data for clarity and calculation\n",
        "age_pivot = fips_age_summary.pivot(index='fips', columns='CLASS_DESC', values=['VALUE_NUMERIC', 'percent_of_ops'])\n",
        "\n",
        "# Calculate explicitly for 4565\n",
        "age_pivot['VALUE_NUMERIC', 'AGE 45 TO 65'] = (\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 45 TO 54'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 55 TO 64'] +\n",
        "    0.1 * age_pivot['VALUE_NUMERIC', 'AGE 65 TO 74']\n",
        ")\n",
        "\n",
        "age_pivot['percent_of_ops', 'AGE 45 TO 65'] = (\n",
        "    age_pivot['percent_of_ops', 'AGE 45 TO 54'] +\n",
        "    age_pivot['percent_of_ops', 'AGE 55 TO 64'] +\n",
        "    0.1 * age_pivot['percent_of_ops', 'AGE 65 TO 74']\n",
        ")\n",
        "\n",
        "# Flatten columns for easier usage\n",
        "age_pivot.columns = ['_'.join(col).strip() for col in age_pivot.columns.values]\n",
        "age_pivot.reset_index(inplace=True)\n",
        "\n",
        "# View results explicitly\n",
        "print(age_pivot[['fips', 'VALUE_NUMERIC_AGE 45 TO 65', 'percent_of_ops_AGE 45 TO 65']])\n",
        "\n",
        "fips_age_summary = pd.merge(fips_age_summary, age_pivot[['fips', 'VALUE_NUMERIC_AGE 45 TO 65', 'percent_of_ops_AGE 45 TO 65']], on='fips', how='left')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Pivot data for clarity and easy calculation\n",
        "age_pivot = fips_age_summary.pivot(index='fips', columns='CLASS_DESC', values=['VALUE_NUMERIC', 'percent_of_ops'])\n",
        "\n",
        "# Calculate explicitly for 60+\n",
        "age_pivot['VALUE_NUMERIC', 'AGE 55+'] = (\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 55 TO 64'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 65 TO 74'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE GE 75']\n",
        ")\n",
        "\n",
        "age_pivot['percent_of_ops', 'AGE 55+'] = (\n",
        "    age_pivot['percent_of_ops', 'AGE 55 TO 64'] +\n",
        "    age_pivot['percent_of_ops', 'AGE 65 TO 74'] +\n",
        "    age_pivot['percent_of_ops', 'AGE GE 75']\n",
        ")\n",
        "\n",
        "# Flatten columns for convenience\n",
        "age_pivot.columns = ['_'.join(col).strip() for col in age_pivot.columns.values]\n",
        "age_pivot.reset_index(inplace=True)\n",
        "\n",
        "# View results clearly\n",
        "print(age_pivot[['fips', 'VALUE_NUMERIC_AGE 55+', 'percent_of_ops_AGE 55+']])\n",
        "\n",
        "fips_age_summary = pd.merge(fips_age_summary, age_pivot[['fips', 'VALUE_NUMERIC_AGE 55+', 'percent_of_ops_AGE 55+']], on='fips', how='left')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Pivot data for clarity and easy calculation\n",
        "age_pivot = fips_age_summary.pivot(index='fips', columns='CLASS_DESC', values=['VALUE_NUMERIC', 'percent_of_ops'])\n",
        "\n",
        "# Calculate explicitly for 60+\n",
        "age_pivot['VALUE_NUMERIC', 'AGE 60+'] = (\n",
        "    0.5 * age_pivot['VALUE_NUMERIC', 'AGE 55 TO 64'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE 65 TO 74'] +\n",
        "    age_pivot['VALUE_NUMERIC', 'AGE GE 75']\n",
        ")\n",
        "\n",
        "# Calculate explicitly for 60+\n",
        "age_pivot['percent_of_ops', 'AGE 60+'] = (\n",
        "    0.5 * age_pivot['percent_of_ops', 'AGE 55 TO 64'] +\n",
        "    age_pivot['percent_of_ops', 'AGE 65 TO 74'] +\n",
        "    age_pivot['percent_of_ops', 'AGE GE 75']\n",
        ")\n",
        "\n",
        "# Flatten columns for convenience\n",
        "age_pivot.columns = ['_'.join(col).strip() for col in age_pivot.columns.values]\n",
        "age_pivot.reset_index(inplace=True)\n",
        "\n",
        "# View results clearly\n",
        "print(age_pivot[['fips', 'VALUE_NUMERIC_AGE 60+', 'percent_of_ops_AGE 60+']])\n",
        "\n",
        "fips_age_summary = pd.merge(fips_age_summary, age_pivot[['fips', 'VALUE_NUMERIC_AGE 60+', 'percent_of_ops_AGE 60+']], on='fips', how='left')\n",
        "\n",
        "fips_age_summary_all = (\n",
        "    df_us_age_clean\n",
        "    .groupby('fips', as_index=False)\n",
        "    .agg({'VALUE_NUMERIC': 'sum'})\n",
        "    .rename(columns={'VALUE_NUMERIC': 'total_ops_all_ages'})\n",
        ")\n",
        "fips_age_summary_all['total_ops_all_ages'].sum()\n",
        "\n",
        "fips_maturity_model_age_distibutions = fips_age_summary[['fips', 'VALUE_NUMERIC_AGE 35 TO 50', 'percent_of_ops_AGE 35 TO 50', 'VALUE_NUMERIC_AGE 40 TO 60', 'percent_of_ops_AGE 40 TO 60', 'VALUE_NUMERIC_AGE 45 TO 65', 'percent_of_ops_AGE 45 TO 65', 'VALUE_NUMERIC_AGE 55+', 'percent_of_ops_AGE 55+', 'VALUE_NUMERIC_AGE 60+', 'percent_of_ops_AGE 60+']].drop_duplicates().copy().reset_index(drop=True)\n",
        "fips_maturity_model_age_distibutions.to_csv('fips_maturity_model_age_distibutions.csv', index=False)\n",
        "fips_maturity_model_age_distibutions['VALUE_NUMERIC_AGE 35 TO 50'].sum()\n",
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "QtaBdjY6mNAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['SOURCE_DESC',\n",
        " 'SECTOR_DESC',\n",
        " 'GROUP_DESC',\n",
        " 'COMMODITY_DESC',\n",
        " 'CLASS_DESC',\n",
        " 'PRODN_PRACTICE_DESC',\n",
        " 'UTIL_PRACTICE_DESC',\n",
        " 'STATISTICCAT_DESC',\n",
        " 'UNIT_DESC',\n",
        " 'SHORT_DESC',\n",
        " 'DOMAIN_DESC',\n",
        " 'DOMAINCAT_DESC',\n",
        " 'AGG_LEVEL_DESC',\n",
        " 'STATE_ANSI',\n",
        " 'STATE_FIPS_CODE',\n",
        " 'STATE_ALPHA',\n",
        " 'STATE_NAME',\n",
        " 'ASD_CODE',\n",
        " 'ASD_DESC',\n",
        " 'COUNTY_ANSI',\n",
        " 'COUNTY_CODE',\n",
        " 'COUNTY_NAME',\n",
        " 'REGION_DESC']\n",
        ""
      ],
      "metadata": {
        "id": "McC0d695xN0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operator_field_to_use = 'COMMODITY TOTALS - OPERATIONS WITH SALES'\n",
        "['NOT SPECIFIED',\n",
        " 'FARM SALES: (10,000 TO 19,999 $)',\n",
        " 'FARM SALES: (1,000 TO 2,499 $)',\n",
        " 'FARM SALES: (40,000 TO 49,999 $)',\n",
        " 'FARM SALES: (LESS THAN 1,000 $)',\n",
        " 'FARM SALES: (10,000 TO 24,999 $)',\n",
        " 'FARM SALES: (100,000 OR MORE $)',\n",
        " 'FARM SALES: (100,000 TO 249,999 $)',\n",
        " 'FARM SALES: (25,000 TO 39,999 $)',\n",
        " 'FARM SALES: (50,000 TO 99,999 $)',\n",
        " 'FARM SALES: (LESS THAN 2,500 $)',\n",
        " 'FARM SALES: (500,000 OR MORE $)',\n",
        " 'FARM SALES: (2,500 TO 4,999 $)',\n",
        " 'FARM SALES: (250,000 TO 499,999 $)',\n",
        " 'FARM SALES: (20,000 TO 24,999 $)',\n",
        " 'FARM SALES: (25,000 TO 49,999 $)',\n",
        " 'FARM SALES: (5,000 TO 9,999 $)']\n",
        "below_250k_categories = [\n",
        "    'FARM SALES: (LESS THAN 1,000 $)',\n",
        "    'FARM SALES: (1,000 TO 2,499 $)',\n",
        "    'FARM SALES: (2,500 TO 4,999 $)',\n",
        "    'FARM SALES: (5,000 TO 9,999 $)',\n",
        "    'FARM SALES: (10,000 TO 19,999 $)',\n",
        "    'FARM SALES: (20,000 TO 24,999 $)',\n",
        "    'FARM SALES: (10,000 TO 24,999 $)', # careful of potential overlap\n",
        "    'FARM SALES: (25,000 TO 39,999 $)',\n",
        "    'FARM SALES: (40,000 TO 49,999 $)',\n",
        "    'FARM SALES: (25,000 TO 49,999 $)', # careful of potential overlap\n",
        "    'FARM SALES: (50,000 TO 99,999 $)',\n",
        "    'FARM SALES: (100,000 TO 249,999 $)',\n",
        "    'FARM SALES: (LESS THAN 2,500 $)' # overlaps with smaller categories, consider carefully\n",
        "]\n",
        "above_250k_categories = [\n",
        "    'FARM SALES: (250,000 TO 499,999 $)',\n",
        "    'FARM SALES: (500,000 TO 999,999 $)',\n",
        "    'FARM SALES: (500,000 OR MORE $)',\n",
        "    'FARM SALES: (1,000,000 OR MORE $)'\n",
        "]\n",
        "\n",
        "rev_desc_to_keep = ['FARM SALES: (LESS THAN 1,000 $)', 'FARM SALES: (1,000 TO 2,499 $)', 'FARM SALES: (2,500 TO 4,999 $)', 'FARM SALES: (5,000 TO 9,999 $)', 'FARM SALES: (10,000 TO 19,999 $)', 'FARM SALES: (20,000 TO 24,999 $)', 'FARM SALES: (25,000 TO 39,999 $)', 'FARM SALES: (40,000 TO 49,999 $)', 'FARM SALES: (50,000 TO 99,999 $)', 'FARM SALES: (100,000 TO 249,999 $)', 'FARM SALES: (250,000 TO 499,999 $)', 'FARM SALES: (500,000 TO 999,999 $)', 'FARM SALES: (1,000,000 TO 2,499,999 $)', 'FARM SALES: (2,500,000 TO 4,999,999 $)', 'FARM SALES: (5,000,000 OR MORE $)']\n",
        "\n",
        "\n",
        "df_us['SECTOR_DESC'].drop_duplicates()\n",
        "df_us.loc[df_us['SECTOR_DESC'] == 'ECONOMICS', 'GROUP_DESC'].drop_duplicates()\n",
        "# fips_sales = df_us.loc[((df_us['SHORT_DESC'] == operator_field_to_use) & ((df_us['DOMAINCAT_DESC'].isin(below_250k_categories) | (df_us['DOMAINCAT_DESC'].isin(above_250k_categories)))) & (df_us['AGG_LEVEL_DESC'] == 'COUNTY'))].drop_duplicates().sort_values(['STATE_FIPS_CODE', 'COUNTY_CODE']).copy().reset_index(drop=True)\n",
        "fips_sales = df_us.loc[((df_us['SHORT_DESC'] == operator_field_to_use)  & (df_us['AGG_LEVEL_DESC'] == 'STATE'))].drop_duplicates().sort_values(['STATE_FIPS_CODE', 'COUNTY_CODE']).copy().reset_index(drop=True)\n",
        "fips_sales['VALUE_NUMERIC']  = pd.to_numeric(fips_sales['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "fips_sales\n",
        "\n",
        "fips_sales.loc[((fips_sales['SECTOR_DESC'] == 'ECONOMICS') & (fips_sales['SHORT_DESC'] == operator_field_to_use)& (fips_sales['DOMAIN_DESC'] == 'FARM SALES')),['STATE_FIPS_CODE','SECTOR_DESC', 'GROUP_DESC', 'SHORT_DESC' ,'VALUE', 'DOMAIN_DESC', 'DOMAINCAT_DESC']].drop_duplicates().sort_values('STATE_FIPS_CODE')\n",
        "\n",
        "fips_sales.loc[((fips_sales['SECTOR_DESC'] == 'ECONOMICS') & (fips_sales['SHORT_DESC'] == operator_field_to_use)& (fips_sales['DOMAIN_DESC'] == 'FARM SALES') & (fips_sales['DOMAINCAT_DESC'].isin(rev_desc_to_keep))),['VALUE_NUMERIC']].sum()\n",
        "fips_sales_clean = fips_sales.loc[((fips_sales['SECTOR_DESC'] == 'ECONOMICS') & (fips_sales['SHORT_DESC'] == operator_field_to_use)& (fips_sales['DOMAIN_DESC'] == 'FARM SALES') & (fips_sales['DOMAINCAT_DESC'].isin(rev_desc_to_keep)))].copy()\n",
        "fips_sales_clean['VALUE_NUMERIC'].sum()\n",
        "# 3,813,560\n",
        "\n",
        "\n",
        "# fips_sales.loc[((fips_sales['SECTOR_DESC'] == 'DEMOGRAPHICS') & fips_sales['SECTOR_DESC'] == 'DEMOGRAPHICS')),['VALUE_NUMERIC']].astype(int).sum()\n",
        "# 11,967,386\n",
        "fips_sales = fips_sales_clean.copy().reset_index(drop=True)\n",
        "fips_sales\n",
        "# fips_sales['COUNTY_CODE'] =  fips_sales['COUNTY_CODE'].astype(int)\n",
        "fips_sales['fips'] = fips_sales['STATE_FIPS_CODE'].astype(str).astype(str).str.zfill(2)\n",
        "\n",
        "\n",
        "# fips_sales['fips'] = (fips_sales['STATE_FIPS_CODE'].astype(str).astype(str).str.zfill(2) + fips_sales['COUNTY_CODE'].astype(str).astype(str).str.zfill(3)).replace('.0','')\n",
        "\n",
        "# fips_sales['DOMAINCAT_DESC'].drop_duplicates()\n",
        "# fips_sales.loc[fips_sales['DOMAINCAT_DESC'].str.contains('FARM SALES'), 'SHORT_DESC'].drop_duplicates()\n",
        "# fips_sales.loc[fips_sales['SHORT_DESC'].str.contains('COMMODITY TOTALS - OPERATIONS WITH SALES'), 'DOMAINCAT_DESC'].drop_duplicates().to_list()\n",
        "\n",
        "fips_sales_pivot = fips_sales.pivot_table(index='fips', columns='DOMAINCAT_DESC', values='VALUE_NUMERIC', aggfunc='sum').reset_index()\n",
        "fips_sales_pivot\n",
        "\n",
        "\n",
        "fips_sales_pivot['FARM_SALES_5M_PLUS'] = fips_sales_pivot['FARM SALES: (5,000,000 OR MORE $)']\n",
        "fips_sales_pivot['FARM_SALES_2_TO_5_MILLION'] = (fips_sales_pivot['FARM SALES: (2,500,000 TO 4,999,999 $)'] + fips_sales_pivot['FARM SALES: (1,000,000 TO 2,499,999 $)'] * (1/3)).round(0).astype(int)\n",
        "fips_sales_pivot['FARM_SALES_1_TO_2_MILLION'] = (\n",
        "    # Two-thirds explicitly represent 1M2M\n",
        "    fips_sales_pivot['FARM SALES: (1,000,000 TO 2,499,999 $)'] * (2/3)\n",
        ").round(0).astype(int)\n",
        "fips_sales_pivot['FARM_SALES_500K_TO_1MILLION'] = fips_sales_pivot['FARM SALES: (500,000 TO 999,999 $)'].round(0).astype(int)\n",
        "\n",
        "fips_sales_pivot['FARM_SALES_UNDER_500K'] = fips_sales_pivot['FARM SALES: (1,000 TO 2,499 $)'] + fips_sales_pivot['FARM SALES: (2,500 TO 4,999 $)'] + fips_sales_pivot['FARM SALES: (5,000 TO 9,999 $)'] + fips_sales_pivot['FARM SALES: (10,000 TO 19,999 $)'] + fips_sales_pivot['FARM SALES: (20,000 TO 24,999 $)'] + fips_sales_pivot['FARM SALES: (25,000 TO 39,999 $)'] + fips_sales_pivot['FARM SALES: (40,000 TO 49,999 $)'] + fips_sales_pivot['FARM SALES: (50,000 TO 99,999 $)'] + fips_sales_pivot['FARM SALES: (100,000 TO 249,999 $)'] + fips_sales_pivot['FARM SALES: (250,000 TO 499,999 $)']\n",
        "fips_sales_pivot['FARM_SALES_UNDER_250K'] = fips_sales_pivot['FARM SALES: (1,000 TO 2,499 $)'] + fips_sales_pivot['FARM SALES: (2,500 TO 4,999 $)'] + fips_sales_pivot['FARM SALES: (5,000 TO 9,999 $)'] + fips_sales_pivot['FARM SALES: (10,000 TO 19,999 $)'] + fips_sales_pivot['FARM SALES: (20,000 TO 24,999 $)'] + fips_sales_pivot['FARM SALES: (25,000 TO 39,999 $)'] + fips_sales_pivot['FARM SALES: (40,000 TO 49,999 $)'] + fips_sales_pivot['FARM SALES: (50,000 TO 99,999 $)'] + fips_sales_pivot['FARM SALES: (100,000 TO 249,999 $)']\n",
        "fips_sales_pivot['FARM_SALES_OVER_250K'] = fips_sales_pivot['FARM SALES: (250,000 TO 499,999 $)'] + fips_sales_pivot['FARM SALES: (500,000 TO 999,999 $)'] + fips_sales_pivot['FARM SALES: (1,000,000 TO 2,499,999 $)'] + fips_sales_pivot['FARM SALES: (2,500,000 TO 4,999,999 $)'] + fips_sales_pivot['FARM SALES: (5,000,000 OR MORE $)']\n",
        "fips_sales_pivot\n",
        "fips_sales_state_summary = fips_sales_pivot[['fips', 'FARM_SALES_UNDER_250K', 'FARM_SALES_OVER_250K', 'FARM_SALES_UNDER_500K', 'FARM_SALES_500K_TO_1MILLION', 'FARM_SALES_1_TO_2_MILLION', 'FARM_SALES_2_TO_5_MILLION', 'FARM_SALES_5M_PLUS']]\n",
        "\n",
        "\n",
        "fips_sales_pivot['FARM_SALES_OVER_250K'].sum()\n",
        "\n",
        "fips_sales_state_summary.to_csv('fips_sales_breaks.csv', index=False)\n",
        "# 1,900,487\n"
      ],
      "metadata": {
        "id": "rkGf195ROs_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(fips_sales_pivot['FARM_SALES_5M_PLUS']).sum()"
      ],
      "metadata": {
        "id": "mE9AKJu13YP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fips_sales_with_age = pd.merge(fips_sales_state_summary, fips_maturity_model_age_distibutions[['fips', 'percent_of_ops_AGE 35 TO 50', 'percent_of_ops_AGE 40 TO 60', 'percent_of_ops_AGE 45 TO 65', 'percent_of_ops_AGE 55+','percent_of_ops_AGE 60+']], on='fips', how='left').copy().reset_index(drop=True)\n",
        "fips_sales_with_age.to_csv('fips_sales_with_age.csv', index=False)\n",
        "\n",
        "fips_sales_with_age['MM_INNOVATORS'] = (fips_sales_with_age['FARM_SALES_5M_PLUS'] * (fips_sales_with_age['percent_of_ops_AGE 35 TO 50']/100)).round(0).astype(int)\n",
        "fips_sales_with_age['MM_EARLY_ADOPTERS'] = (fips_sales_with_age['FARM_SALES_2_TO_5_MILLION'] * (fips_sales_with_age['percent_of_ops_AGE 40 TO 60']/100)).round(0).astype(int)\n",
        "fips_sales_with_age['MM_EARLY_MAJORITY'] = (fips_sales_with_age['FARM_SALES_1_TO_2_MILLION'] * (fips_sales_with_age['percent_of_ops_AGE 45 TO 65']/100)).round(0).astype(int)\n",
        "fips_sales_with_age['MM_LATE_MAJORITY'] = (fips_sales_with_age['FARM_SALES_500K_TO_1MILLION'] * (fips_sales_with_age['percent_of_ops_AGE 55+']/100)).round(0).astype(int)\n",
        "\n",
        "fips_sales_with_age['MM_LAGGARDS'] = (fips_sales_with_age['FARM_SALES_UNDER_500K'] * (fips_sales_with_age['percent_of_ops_AGE 60+']/100)).round(0).astype(int)\n",
        "fips_sales_with_age['MM_LAGGARDS'].sum()\n",
        "\n",
        "fips_sales_with_age.to_csv('fips_sales_with_age_maturity_model.csv', index=False)\n",
        "#\n",
        "# 19,713,906\n",
        "# 617,209\n"
      ],
      "metadata": {
        "id": "_XMs1H-36n9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maturity model for canada"
      ],
      "metadata": {
        "id": "ynjRKQHqVM9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_canada.head()\n",
        "\n",
        "df_prov_sales = df_canada.loc[~df_canada['GEO'].str.contains('Canada') & ~df_canada['GEO'].str.contains(',')]\n",
        "df_prov_sales['VALUE_NUMERIC']  = pd.to_numeric(df_prov_sales['VALUE'].fillna(0.0).round(0).astype(int), errors='coerce')\n",
        "df_prov_sales['geo_code'] = df_prov_sales['DGUID'].str[-2:]\n",
        "df_prov_sales_pivot = df_prov_sales.pivot_table(index='geo_code', columns='Total farm revenues distribution', values='VALUE_NUMERIC', aggfunc='sum').reset_index()\n",
        "df_prov_sales_pivot\n",
        "\n",
        "\n",
        "df_prov_sales_pivot['FARM_SALES_5M_PLUS'] = (df_prov_sales_pivot['$2,000,000 and over'] * .24).round(0).astype(int)\n",
        "df_prov_sales_pivot['FARM_SALES_2_TO_5_MILLION'] = (df_prov_sales_pivot['$2,000,000 and over'] * .76).round(0).astype(int)\n",
        "df_prov_sales_pivot['FARM_SALES_1_TO_2_MILLION'] = df_prov_sales_pivot['$1,000,000 to $1,999,999']\n",
        "df_prov_sales_pivot['FARM_SALES_500K_TO_1MILLION'] = df_prov_sales_pivot['$500,000 to $999,999'].round(0).astype(int)\n",
        "\n",
        "df_prov_sales_pivot['FARM_SALES_UNDER_500K'] = df_prov_sales_pivot['$0'] + df_prov_sales_pivot['$1 to $9,999'] + df_prov_sales_pivot['$10,000 to $24,999'] + df_prov_sales_pivot['$25,000 to $49,999'] + df_prov_sales_pivot['$50,000 to $99,999'] + df_prov_sales_pivot['$100,000 to $249,999'] + df_prov_sales_pivot['$250,000 to $499,999']\n",
        "df_prov_sales_pivot['FARM_SALES_UNDER_250K'] = df_prov_sales_pivot['$0'] + df_prov_sales_pivot['$1 to $9,999'] + df_prov_sales_pivot['$10,000 to $24,999'] + df_prov_sales_pivot['$25,000 to $49,999'] + df_prov_sales_pivot['$50,000 to $99,999'] + df_prov_sales_pivot['$100,000 to $249,999']\n",
        "\n",
        "df_prov_sales_pivot['FARM_SALES_OVER_250K'] = df_prov_sales_pivot['$250,000 to $499,999'] + df_prov_sales_pivot['$500,000 to $999,999'] + df_prov_sales_pivot['$1,000,000 to $1,999,999'] + df_prov_sales_pivot['$2,000,000 and over']\n",
        "df_prov_sales_pivot\n",
        "df_prov_sales_summary = df_prov_sales_pivot[['geo_code', 'FARM_SALES_UNDER_250K', 'FARM_SALES_OVER_250K', 'FARM_SALES_UNDER_500K', 'FARM_SALES_500K_TO_1MILLION', 'FARM_SALES_1_TO_2_MILLION', 'FARM_SALES_2_TO_5_MILLION', 'FARM_SALES_5M_PLUS']]\n",
        "\n",
        "df_prov_sales_pivot.sum()\n",
        "\n",
        "df_prov_sales_pivot['FARM_SALES_OVER_250K'].sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "Upqb_NH8VEM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df_prov_sales_pivot['FARM_SALES_5M_PLUS'] ).sum()"
      ],
      "metadata": {
        "id": "VcTECiE13uZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca_age_bands = ['Age - 35 to 54 years',\n",
        " 'Age - 55 years and over',\n",
        " 'Age - under 35 years',\n",
        "                'Total number of farms'\n",
        "]\n",
        "['VALUE_NUMERIC_AGE 35 TO 50', 'percent_of_ops_AGE 35 TO 50', 'VALUE_NUMERIC_AGE 40 TO 60', 'percent_of_ops_AGE 40 TO 60', 'VALUE_NUMERIC_AGE 45 TO 65', 'percent_of_ops_AGE 45 TO 65', 'VALUE_NUMERIC_AGE 55+', 'percent_of_ops_AGE 55+', 'VALUE_NUMERIC_AGE 60+', 'percent_of_ops_AGE 60+']\n",
        "df_cana_farmer_age = sc.table_to_df('32-10-0381-01')\n",
        "\n",
        "df_cana_farmer_age = df_cana_farmer_age.loc[~df_cana_farmer_age['GEO'].str.contains('Canada') & ~df_cana_farmer_age['GEO'].str.contains(',') & df_cana_farmer_age['Farms according to the number of operators reported'].str.contains('All') & df_cana_farmer_age['Characteristics'].isin(ca_age_bands)]\n",
        "df_cana_farmer_age['VALUE_NUMERIC']  = pd.to_numeric(df_cana_farmer_age['VALUE'].fillna(0.0).round(0).astype(int), errors='coerce')\n",
        "df_cana_farmer_age['geo_code'] = df_cana_farmer_age['DGUID'].str[-2:]\n",
        "# df_cana_farmer_age['VALUE_NUMERIC']  = pd.to_numeric(df_cana_farmer_age['VALUE'].fillna(0.0).round(0).astype(int), errors='coerce')\n",
        "df_cana_farmer_age['Characteristics'].drop_duplicates().sort_values().to_list()\n",
        "df_cana_farmer_age_pivot = df_cana_farmer_age.pivot_table(index='geo_code', columns='Characteristics', values='VALUE_NUMERIC', aggfunc='sum').reset_index()\n",
        "# df_cana_farmer_age_pivot\n",
        "\n",
        "# 3550\n",
        "factor_35_50 = 0.69\n",
        "df_cana_farmer_age_pivot['VALUE 35-50'] = (df_cana_farmer_age_pivot['Age - 35 to 54 years'] * factor_35_50).fillna(0.0).round().astype(int)\n",
        "df_cana_farmer_age_pivot['PCT 35-50'] = (df_cana_farmer_age_pivot['VALUE 35-50'] / df_cana_farmer_age_pivot['Total number of farms'] * 100).fillna(0.0).astype(float)\n",
        "\n",
        "# 4060\n",
        "factor_40_60 = 0.646\n",
        "df_cana_farmer_age_pivot['VALUE 40-60'] = (df_cana_farmer_age_pivot['Age - 35 to 54 years'] * factor_40_60 * (39.5/100) + df_cana_farmer_age_pivot['Age - 55 years and over'] * factor_40_60 * (14.1/100)).fillna(0.0).round().astype(int)\n",
        "df_cana_farmer_age_pivot['PCT 40-60'] = (df_cana_farmer_age_pivot['VALUE 40-60'] / df_cana_farmer_age_pivot['Total number of farms'] * 100).fillna(0.0).astype(float)\n",
        "\n",
        "# 4565\n",
        "factor_45_65 = 0.487\n",
        "df_cana_farmer_age_pivot['VALUE 45-65'] = (df_cana_farmer_age_pivot['Age - 35 to 54 years'] * factor_45_65 * (39.5/100) + df_cana_farmer_age_pivot['Age - 55 years and over'] * factor_45_65 * ((14.1+5.6)/100)).fillna(0.0).round().astype(int)\n",
        "df_cana_farmer_age_pivot['PCT 45-65'] = (df_cana_farmer_age_pivot['VALUE 45-65'] / df_cana_farmer_age_pivot['Total number of farms'] * 100).fillna(0.0).astype(float)\n",
        "\n",
        "# 55+\n",
        "# Already in dataset\n",
        "df_cana_farmer_age_pivot['VALUE 55+'] = (df_cana_farmer_age_pivot['Age - 55 years and over'] ).fillna(0.0).round().astype(int)\n",
        "df_cana_farmer_age_pivot['PCT 55+'] = (df_cana_farmer_age_pivot['VALUE 55+'] / df_cana_farmer_age_pivot['Total number of farms'] * 100).fillna(0.0).astype(float)\n",
        "\n",
        "\n",
        "# 60+\n",
        "factor_60_plus = 0.75\n",
        "df_cana_farmer_age_pivot['VALUE 60+'] = (df_cana_farmer_age_pivot['Age - 55 years and over'] * factor_60_plus).fillna(0.0).round().astype(int)\n",
        "df_cana_farmer_age_pivot['PCT 60+'] = (df_cana_farmer_age_pivot['VALUE 60+'] / df_cana_farmer_age_pivot['Total number of farms'] * 100).fillna(0.0).astype(float)\n",
        "\n",
        "df_cana_farmer_age_pivot.to_csv('ca_farmer_age_pivot')"
      ],
      "metadata": {
        "id": "rt_VndwjZ4FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ca_maturity_model = pd.merge(df_prov_sales_summary, df_cana_farmer_age_pivot, on='geo_code', how='left', suffixes=('', '_age')).copy().reset_index(drop=True)\n",
        "ca_maturity_model\n",
        "\n",
        "ca_maturity_model['MM_INNOVATORS'] = (ca_maturity_model['FARM_SALES_5M_PLUS'] * (ca_maturity_model['PCT 35-50']/100)).round(0).astype(int)\n",
        "ca_maturity_model['MM_EARLY_ADOPTERS'] = (ca_maturity_model['FARM_SALES_2_TO_5_MILLION'] * (ca_maturity_model['PCT 40-60']/100)).round(0).astype(int)\n",
        "ca_maturity_model['MM_EARLY_MAJORITY'] = (ca_maturity_model['FARM_SALES_1_TO_2_MILLION'] * (ca_maturity_model['PCT 45-65']/100)).round(0).astype(int)\n",
        "ca_maturity_model['MM_LATE_MAJORITY'] = (ca_maturity_model['FARM_SALES_500K_TO_1MILLION'] * (ca_maturity_model['PCT 55+']/100)).round(0).astype(int)\n",
        "\n",
        "ca_maturity_model['MM_LAGGARDS'] = (ca_maturity_model['FARM_SALES_UNDER_500K'] * (ca_maturity_model['PCT 60+']/100)).round(0).astype(int)\n",
        "ca_maturity_model['MM_LAGGARDS'].sum()\n",
        "\n",
        "ca_maturity_model.to_csv('ca_sales_with_age_maturity_model.csv', index=False)\n"
      ],
      "metadata": {
        "id": "B_Fal73yfF6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your service account data loaded into a dictionary\n",
        "# ca_tractor_census = 'https://www150.statcan.gc.ca/t1/tbl1/en/dtl!downloadDbLoadingData-nonTraduit.action?pid=3210022901&latestN=5&startDate=&endDate=&csvLocale=en&selectedMembers=%5B%5B1%5D%2C%5B%5D%5D&checkedLevels=1D1'\n",
        "['SOURCE_DESC',\n",
        " 'SECTOR_DESC',\n",
        " 'GROUP_DESC',\n",
        " 'COMMODITY_DESC',\n",
        " 'CLASS_DESC',\n",
        " 'PRODN_PRACTICE_DESC',\n",
        " 'UTIL_PRACTICE_DESC',\n",
        " 'STATISTICCAT_DESC',\n",
        " 'UNIT_DESC',\n",
        " 'SHORT_DESC',\n",
        " 'DOMAIN_DESC',\n",
        " 'DOMAINCAT_DESC',\n",
        " 'AGG_LEVEL_DESC',\n",
        " 'STATE_ANSI',\n",
        " 'STATE_FIPS_CODE',\n",
        " 'STATE_ALPHA',\n",
        " 'STATE_NAME',\n",
        " 'ASD_CODE',\n",
        " 'ASD_DESC',\n",
        " 'COUNTY_ANSI',\n",
        " 'COUNTY_CODE',\n",
        " 'COUNTY_NAME',\n",
        " 'REGION_DESC']\n",
        "\n"
      ],
      "metadata": {
        "id": "93xqZZstAaOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "over_10_acreage_categories = [\n",
        "    'AREA OPERATED: (70.0 TO 99.9 ACRES)',\n",
        "       'AREA OPERATED: (100 TO 139 ACRES)',\n",
        "       'AREA OPERATED: (140 TO 179 ACRES)',\n",
        "       'AREA OPERATED: (10.0 TO 49.9 ACRES)',\n",
        "       'AREA OPERATED: (50.0 TO 69.9 ACRES)',\n",
        "       'AREA OPERATED: (2,000 OR MORE ACRES)',\n",
        "       'AREA OPERATED: (180 TO 219 ACRES)',\n",
        "       'AREA OPERATED: (500 TO 999 ACRES)',\n",
        "       'AREA OPERATED: (220 TO 259 ACRES)',\n",
        "       'AREA OPERATED: (260 TO 499 ACRES)',\n",
        "       'AREA OPERATED: (1,000 TO 1,999 ACRES)'\n",
        "]"
      ],
      "metadata": {
        "id": "T_3EGKVEpwST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_us['SECTOR_DESC'].drop_duplicates()\n",
        "equip_settings = ['TRACTORS', 'SELF PROPELLED', 'MACHINERY, OTHER']\n",
        "df_us.loc[df_us['SECTOR_DESC'] == 'DEMOGRAPHICS', 'GROUP_DESC'].drop_duplicates().sort_values()\n",
        "df_us.loc[((df_us['SECTOR_DESC'] == 'DEMOGRAPHICS') & (df_us['GROUP_DESC'] == 'FARMS & LAND & ASSETS')), ['CLASS_DESC']].drop_duplicates().sort_values('CLASS_DESC')\n",
        "\n",
        "df_us.loc[df_us['CLASS_DESC'].str.contains('A'), 'SHORT_DESC'].drop_duplicates()\n",
        "#\n",
        "'YEARS'\n"
      ],
      "metadata": {
        "id": "tlmUeR-EFYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bBNfv9hbKu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKLarufPrgS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYK8CgwU0wme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# county_df = df.dropna(subset=['COUNTY_CODE', 'STATE_FIPS_CODE']).copy().reset_index(drop=True)\n",
        "# county_df['COUNTY_FIPS'] = county_df['STATE_FIPS_CODE'].astype(str).str.zfill(2) + \\\n",
        "#                            county_df['COUNTY_CODE'].astype(int).astype(str).str.zfill(3)\n",
        "operator_field_to_use = 'COMMODITY TOTALS - OPERATIONS WITH SALES'"
      ],
      "metadata": {
        "id": "7oar2YqXr5qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title us over and under 250k with diy\n",
        "# summarize usda farm operations data\n",
        "state_df = None\n",
        "county_df = None\n",
        "national_df = None\n",
        "state_summary = None\n",
        "county_summary = None\n",
        "national_summary = None\n",
        "\n",
        "below_250k_categories = [\n",
        "    'FARM SALES: (LESS THAN 1,000 $)',\n",
        "    'FARM SALES: (1,000 TO 2,499 $)',\n",
        "    'FARM SALES: (2,500 TO 4,999 $)',\n",
        "    'FARM SALES: (5,000 TO 9,999 $)',\n",
        "    'FARM SALES: (10,000 TO 24,999 $)',\n",
        "    'FARM SALES: (25,000 TO 49,999 $)',\n",
        "    'FARM SALES: (50,000 TO 99,999 $)',\n",
        "    'FARM SALES: (100,000 TO 249,999 $)'\n",
        "]\n",
        "\n",
        "above_250k_categories = [\n",
        "    'FARM SALES: (250,000 TO 499,999 $)',\n",
        "    'FARM SALES: (500,000 TO 999,999 $)',\n",
        "    'FARM SALES: (1,000,000 OR MORE $)'\n",
        "]\n",
        "\n",
        "\n",
        "state_df = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'STATE') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "\n",
        "state_df['VALUE_NUMERIC'] = pd.to_numeric(state_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "state_df.columns.to_list()\n",
        "# Aggregate clearly by state\n",
        "state_df['STATEFP'] = state_df['STATE_FIPS_CODE'].astype(int).astype(str).str.zfill(2)\n",
        "state_summary = state_df.groupby(['STATEFP','STATE_NAME']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "\n",
        "# Add total clearly\n",
        "state_summary['Total_Ops'] = state_summary['Ops_below_250k'] + state_summary['Ops_250k_or_more']\n",
        "state_summary['total_population'] = state_summary['Total_Ops']\n",
        "\n",
        "state_summary = state_summary.rename(columns={'STATEFP': 'geo_code', 'STATE_NAME': 'geo_name'})\n",
        "\n",
        "us_over250k_farm_diy_spi_index = estimate_search_population_indexes(us_over250_paid_trends, state_summary, baseline_audince_factor=0.75, interest_multiplier=5)\n",
        "us_over250k_farm_diy_spi_index['Ops_over_250k_diy_factor'] = us_over250k_farm_diy_spi_index['estimated_audience_factor']\n",
        "\n",
        "\n",
        "us_under250k_farm_diy_spi_index = estimate_search_population_indexes(us_under_250_paid_trends, state_summary, baseline_audince_factor=0.6, interest_multiplier=5)\n",
        "us_under250k_farm_diy_spi_index['Ops_below_250k_diy_factor'] = us_under250k_farm_diy_spi_index['estimated_audience_factor']\n",
        "\n",
        "\n",
        "us_farm_ops_by_state_w_diy = pd.merge(state_summary, us_over250k_farm_diy_spi_index[['geo_code', 'Ops_over_250k_diy_factor']], on='geo_code', how='left', suffixes=('', '_spi'))\n",
        "us_farm_ops_by_state_w_diy = pd.merge(us_farm_ops_by_state_w_diy, us_under250k_farm_diy_spi_index[['geo_code', 'Ops_below_250k_diy_factor']], on='geo_code', how='left', suffixes=('', '_uspi'))\n",
        "# state_farm_ops_by_state_w_diy['Ops_below_250k_diy'] = (state_farm_ops_by_state_w_diy['Ops_below_250k'] * state_farm_ops_by_state_w_diy['SPI']).round(0).astype(int)\n",
        "us_farm_ops_by_state_w_diy['Ops_below_250k_diy'] = (us_farm_ops_by_state_w_diy['Ops_below_250k'] * us_farm_ops_by_state_w_diy['Ops_below_250k_diy_factor']).round(0).astype(int)\n",
        "us_farm_ops_by_state_w_diy['Ops_250k_or_more_diy'] = (us_farm_ops_by_state_w_diy['Ops_250k_or_more'] * us_farm_ops_by_state_w_diy['Ops_over_250k_diy_factor']).round(0).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "us_farm_ops_by_state_w_diy.to_csv('us_farm_ops_by_state_w_diy.csv', index=False)\n",
        "\n",
        "\n",
        "us_over250k_farm_diy_spi_index.to_csv('us_over250k_diy_spi_index.csv', index=False)\n",
        "us_under250k_farm_diy_spi_index.to_csv('us_under250k_diy_spi_index.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "county_df = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'COUNTY') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "county_df['VALUE_NUMERIC'] = pd.to_numeric(county_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "# Construct County FIPS explicitly\n",
        "try:\n",
        "    county_df['FIPS'] = county_df['STATE_FIPS_CODE'].astype(str).str.zfill(2) + \\\n",
        "                        county_df['COUNTY_CODE'].astype(int).astype(str).str.zfill(3)\n",
        "except ValueError:\n",
        "    print(ValueError)\n",
        "\n",
        "county_summary = county_df.groupby(['STATE_NAME', 'COUNTY_NAME', 'FIPS']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "\n",
        "# Calculate total explicitly\n",
        "county_summary['Total_Ops'] = county_summary['Ops_below_250k'] + county_summary['Ops_250k_or_more']\n",
        "\n",
        "print(county_summary.head(10))\n",
        "national_summary = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'NATIONAL') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "national_summary['VALUE_NUMERIC'] = pd.to_numeric(national_summary['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "national_below_250k = national_summary[\n",
        "    national_summary['DOMAINCAT_DESC'].isin(below_250k_categories)\n",
        "]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "national_above_250k = national_summary[\n",
        "    national_summary['DOMAINCAT_DESC'].isin(above_250k_categories)\n",
        "]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "total_national_ops = national_below_250k + national_above_250k\n",
        "\n",
        "print(f\"National Operations < $250k: {int(national_below_250k):,}\")\n",
        "print(f\"National Operations  $250k: {int(national_above_250k):,}\")\n",
        "print(f\"National Total Operations: {int(total_national_ops):,}\")"
      ],
      "metadata": {
        "id": "ueO_ZKLIsWQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "canada large - small"
      ],
      "metadata": {
        "id": "aRFzjLhgXHiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "provincial_summary_canada\n",
        "# Add total clearly\n",
        "\n",
        "provincial_summary_canada['total_population'] = provincial_summary_canada['Total_Ops']\n",
        "\n",
        "# state_summary = state_summary.rename(columns={'STATEFP': 'geo_code', 'STATE_NAME': 'geo_name'})\n",
        "ca_over250k_farm_diy_spi_index = estimate_search_population_indexes(ca_over250_paid_trends, provincial_summary_canada, baseline_audince_factor=0.75, interest_multiplier=5)\n",
        "ca_over250k_farm_diy_spi_index['Ops_over_250k_diy_factor'] = ca_over250k_farm_diy_spi_index['estimated_audience_factor']\n",
        "\n",
        "\n",
        "ca_under250k_farm_diy_spi_index = estimate_search_population_indexes(ca_under250_paid_trends, provincial_summary_canada, baseline_audince_factor=0.6, interest_multiplier=5)\n",
        "ca_under250k_farm_diy_spi_index['Ops_below_250k_diy_factor'] = ca_under250k_farm_diy_spi_index['estimated_audience_factor']\n",
        "missing_ca_geos = set(provincial_summary_canada['geo_code'])- set(ca_over250k_farm_diy_spi_index['geo_code'])\n",
        "default_over250k_diy_factor = 0.75\n",
        "missing_ca_data = pd.DataFrame(columns=ca_over250k_farm_diy_spi_index.columns)\n",
        "for geo in missing_ca_geos:\n",
        "    rec_df = pd.DataFrame(columns=ca_over250k_farm_diy_spi_index.columns)\n",
        "    rec = {\n",
        "        'geo_code': geo,\n",
        "        'Ops_over_250k_diy_factor': default_over250k_diy_factor,\n",
        "    }\n",
        "    missing_ca_data = pd.concat([missing_ca_data, pd.DataFrame([rec])], ignore_index=True)\n",
        "\n",
        "ca_over250k_farm_diy_spi_index = pd.concat([ca_over250k_farm_diy_spi_index, missing_ca_data], ignore_index=True)\n",
        "\n",
        "default_over250k_diy_factor = 0.6\n",
        "missing_ca_data = pd.DataFrame(columns=ca_under250k_farm_diy_spi_index.columns)\n",
        "for geo in missing_ca_geos:\n",
        "    rec_df = pd.DataFrame(columns=ca_under250k_farm_diy_spi_index.columns)\n",
        "    rec = {\n",
        "        'geo_code': geo,\n",
        "        'Ops_below_250k_diy_factor': default_over250k_diy_factor,\n",
        "    }\n",
        "    missing_ca_data = pd.concat([missing_ca_data, pd.DataFrame([rec])], ignore_index=True)\n",
        "\n",
        "ca_under250k_farm_diy_spi_index = pd.concat([ca_under250k_farm_diy_spi_index, missing_ca_data], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# provincial_summary_canada['geo_code'] = provincial_summary_canada['geo_code'].str.replace('PR', '')\n",
        "all_ca_geo_names = provincial_summary_canada['geo_name'].to_list()\n",
        "\n",
        "ca_under250k_farm_diy_spi_index\n",
        "\n",
        "ca_farm_ops_by_province_w_diy = pd.merge(provincial_summary_canada, ca_over250k_farm_diy_spi_index[['geo_code', 'Ops_over_250k_diy_factor']], on='geo_code', how='left', suffixes=('', '_spi'))\n",
        "ca_farm_ops_by_province_w_diy = pd.merge(ca_farm_ops_by_province_w_diy, ca_under250k_farm_diy_spi_index[['geo_code', 'Ops_below_250k_diy_factor']], on='geo_code', how='left', suffixes=('', '_uspi'))\n",
        "# state_farm_ops_by_state_w_diy['Ops_below_250k_diy'] = (state_farm_ops_by_state_w_diy['Ops_below_250k'] * state_farm_ops_by_state_w_diy['SPI']).round(0).astype(int)\n",
        "ca_farm_ops_by_province_w_diy['Ops_below_250k_diy'] = (ca_farm_ops_by_province_w_diy['Ops_below_250k'] * ca_farm_ops_by_province_w_diy['Ops_below_250k_diy_factor']).round(0).astype(int)\n",
        "ca_farm_ops_by_province_w_diy['Ops_250k_or_more_diy'] = (ca_farm_ops_by_province_w_diy['Ops_250k_or_more'] * ca_farm_ops_by_province_w_diy['Ops_over_250k_diy_factor']).round(0).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "ca_farm_ops_by_province_w_diy.to_csv('ca_farm_ops_by_province_w_diy.csv', index=False)\n",
        "\n",
        "\n",
        "ca_over250k_farm_diy_spi_index.to_csv('ca_over250k_diy_spi_index.csv', index=False)\n",
        "ca_under250k_farm_diy_spi_index.to_csv('ca_under250k_diy_spi_index.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "SOwLd9nIXHTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DU2ZvC1uXHOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Asto3tkyXG_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-8lgKGi6teIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsO22s6-Xqgk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "data_col='adjusted_audience_Ops_250k_or_more'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations > $250k'\n",
        "\n",
        "\n",
        "\n",
        "# # us_large_search_trends = get_trends_via_scrapingbee(us_over_250k_keywords, scrapingbee_api_key)\n",
        "# us_large_search_trends_flat = us_large_search_trends.reset_index()\n",
        "# us_large_trends_prepped = prep_trends_data(us_large_search_trends_flat, us_all_regions, neutral_factor=0.5)\n",
        "\n",
        "\n",
        "# us_large_paid_prepped = prep_paid_search_data(us_large_paid, us_all_regions, neutral_factor=0.5)\n",
        "\n",
        "# counts_col='Ops_250k_or_more'\n",
        "# us_large_audiences = add_us_stats_and_geos(repositioned_us, us_large_trends_prepped, state_summary, us_large_paid_prepped, counts_col=counts_col)\n",
        "# us_large_audiences = us_large_audiences.to_crs(epsg=5070)\n",
        "# vert_max = us_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "# data_col = 'adjusted_audience_Ops_250k_or_more'\n",
        "# file_stub = 'us_diy_over250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: US Farm Operations >= $250k'\n",
        "# footer_txt = 'Data Sources: 2022 USDA Ag Census, Google Ads API, Google Search Trends'\n",
        "\n",
        "# plot_us_map(us_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# us_large_audiences[['NAME', 'STATEFP',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "# # ca_province_list = canada_provinces_gdf['PRENAME'].drop_duplicates().tolist()\n",
        "# # ca_search_trends['geoName'] = ca_search_trends['geoName'].apply(normalize_text)\n",
        "# ca_search_trends_flat = ca_search_trends.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ca_large_trends_prepped = prep_trends_data(ca_search_trends_flat, ca_all_regions, neutral_factor=0.5)\n",
        "\n",
        "# # ca_large_trends_prepped['region'] = ca_large_trends_prepped['region'].apply(normalize_text)\n",
        "# ca_large_paid_prepped = prep_paid_search_data(ca_large_paid, ca_all_regions, neutral_factor=0.5)\n",
        "\n",
        "# ca_large_paid_prepped\n",
        "# counts_col='Ops_250k_or_more'\n",
        "\n",
        "# def add_us_stats_and_geos(gdf, df_trends, df_stats, paid_df, gdf_geo_col='NAME', trends_geo_col='region', stats_geo_col='STATE_NAME', paid_geo_col='state', counts_col=None):\n",
        "#     if counts_col is None:\n",
        "#         return \"Error: counts_col must be provided.\"\n",
        "#     gdf[gdf_geo_col] = gdf[gdf_geo_col].apply(normalize_text)\n",
        "#     df_trends[trends_geo_col] = df_trends[trends_geo_col].apply(normalize_text)\n",
        "#     df_stats[stats_geo_col] = df_stats[stats_geo_col].apply(normalize_text)\n",
        "#     paid_df[paid_geo_col] = paid_df[paid_geo_col].apply(normalize_text)\n",
        "#     merged_df = gdf.merge(df_trends, left_on=gdf_geo_col, right_on=trends_geo_col, how='left')\n",
        "#     merged_df = pd.merge(merged_df, df_stats, left_on=gdf_geo_col, right_on=stats_geo_col, how='left', suffixes=('', '_extra'))\n",
        "#     for col in merged_df.columns:\n",
        "#         if col.endswith('_extra'):\n",
        "#             merged_df.drop(columns=[col], inplace=True)\n",
        "\n",
        "#     merged_df['Ops_below_250k'] = merged_df['Ops_below_250k'].fillna(0).round(0).astype(int)\n",
        "#     merged_df['Ops_250k_or_more'] = merged_df['Ops_250k_or_more'].fillna(0).round(0).astype(int)\n",
        "#     merged_df['Total_Ops'] = merged_df['Total_Ops'].fillna(0).round(0).astype(int)\n",
        "#     comp_col_label = f\"{counts_col}_composite_factor_100\"\n",
        "#     merged_df[comp_col_label] = (merged_df[counts_col] / merged_df[counts_col].sum()) * 100\n",
        "#     # merged_df['Ops_below_250k_composite_factor_100'] = (merged_df['Ops_below_250k'] / merged_df['Ops_250k_or_more'].sum()) * 100\n",
        "#     merged_df2 = merged_df.merge(paid_df, left_on=gdf_geo_col, right_on=paid_geo_col, how='left').copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "#     # merged_df2 = merged_df2.loc[~merged_df2[paid_geo_col].isna()].copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#     mean_trends = merged_df2['st_composite_factor_100'].fillna(0).mean()\n",
        "#     mean_volume = merged_df2['paid_search_composite_factor_100'].fillna(0).mean()\n",
        "\n",
        "#     # Clearly calculate relative positions\n",
        "#     merged_df2['trends_relative'] = merged_df2['st_composite_factor_100'] / mean_trends\n",
        "#     merged_df2['volume_relative'] = merged_df2['paid_search_composite_factor_100'] / mean_volume\n",
        "\n",
        "#     # Clearly combine both into single adjustment factor\n",
        "#     merged_df2['combined_relative_factor'] = (merged_df2['trends_relative'] + merged_df2['volume_relative']) / 2\n",
        "\n",
        "#     # Adjusted audience clearly calculated\n",
        "#     audience_label = f\"adjusted_audience_{counts_col}\"\n",
        "#     merged_df2[audience_label] = (merged_df2[counts_col] * merged_df2['combined_relative_factor']).fillna(0).round(0).astype(int)\n",
        "#     return merged_df2\n",
        "\n",
        "\n",
        "# ca_large_audiences = add_us_stats_and_geos(canada_provinces_gdf, ca_large_trends_prepped, provincial_summary_canada, ca_large_paid_prepped, counts_col=counts_col, gdf_geo_col='PRENAME',\n",
        "#                                            stats_geo_col='geo_name').copy()\n",
        "\n",
        "# ca_large_audiences.columns\n",
        "# ca_large_audiences = ca_large_audiences.to_crs(epsg=5070)\n",
        "# vert_max = ca_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "# data_col = 'adjusted_audience_Ops_250k_or_more'\n",
        "# file_stub = 'ca_diy_over250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: CA Farm Operations >= $250k'\n",
        "# footer_txt = 'Data Sources: 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "\n",
        "# plot_us_map(ca_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# ca_large_audiences[['PRENAME', 'PRUID',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "# na_large_audiences = pd.concat([us_large_audiences[['adjusted_audience_Ops_250k_or_more','Ops_below_250k','Ops_250k_or_more','Total_Ops', 'geometry']], ca_large_audiences[['adjusted_audience_Ops_250k_or_more', 'Ops_below_250k','Ops_250k_or_more','Total_Ops','geometry']]])\n",
        "# vert_max = na_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "# ile_stub = 'na_diy_over250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: NA Farm Operations >= $250k'\n",
        "# footer_txt = 'Data Sources: 2022 USDA Ag Census, 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "# plot_us_map(na_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)"
      ],
      "metadata": {
        "id": "CHtsxsAtX5ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'All US Farm Operations'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census'\n",
        "# vert_max = us_large_audiences[data_col].max()\n",
        "# plot_us_map(us_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'All CA Farm Operations'\n",
        "# footer_txt = 'Data Source: 2021 CA Ag Census'\n",
        "# vert_max = ca_large_audiences[data_col].max()\n",
        "# plot_us_map(ca_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'All North American Farm Operations'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census & 2021 CA Ag Census'\n",
        "# vert_max = na_large_audiences[data_col].max()\n",
        "# plot_us_map(na_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "# data_col='Ops_250k_or_more'\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'US Farm Operations Over $250k'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census'\n",
        "# vert_max = us_large_audiences[data_col].max()\n",
        "# plot_us_map(us_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'CA Farm Operations Over $250k'\n",
        "# footer_txt = 'Data Source: 2021 CA Ag Census'\n",
        "# vert_max = ca_large_audiences[data_col].max()\n",
        "# plot_us_map(ca_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'North American Farm Operations Over $250k'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census & 2021 CA Ag Census'\n",
        "# vert_max = na_large_audiences[data_col].max()\n",
        "# plot_us_map(na_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "# data_col='Ops_below_250k'\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'US Farm Operations Under $250k'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census'\n",
        "# vert_max = us_large_audiences[data_col].max()\n",
        "# plot_us_map(us_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'CA Farm Operations Under $250k'\n",
        "# footer_txt = 'Data Source: 2021 CA Ag Census'\n",
        "# vert_max = ca_large_audiences[data_col].max()\n",
        "# plot_us_map(ca_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "# # @title Over / Under 250k and total maps and data\n",
        "# density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "# density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "\n",
        "# leg_kwds_dict={'label': \"Total Operations\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'North American Farm Operations Under $250k'\n",
        "# footer_txt = 'Data Source: 2022 USDA Ag Census & 2021 CA Ag Census'\n",
        "# vert_max = na_large_audiences[data_col].max()\n",
        "# plot_us_map(na_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n"
      ],
      "metadata": {
        "id": "lYY_SSTEWiSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #  start on small operators\n",
        "# us_under_250k_diy_keywords = [\n",
        "#     'John Deere mower repair',\n",
        "#     'John Deere lawn tractor parts',\n",
        "#     'John Deere mower maintenance',\n",
        "#     'small tractor repair services',\n",
        "#     'small tractor troubleshooting',\n",
        "#     'john deere parts'\n",
        "# ]\n",
        "# ca_under_250k_diy_keywords = [\n",
        "#     'John Deere mower repair',\n",
        "#     'John Deere lawn tractor parts',\n",
        "#     'John Deere mower maintenance',\n",
        "#     'small tractor repair services',\n",
        "#     'small tractor troubleshooting',\n",
        "#     'John Deere parts',\n",
        "#     'John Deere snow blower parts',       # Explicitly relevant due to snow conditions\n",
        "#     'John Deere snow blower repair',      # Frequent seasonal repairs\n",
        "#     'garden tractor snow blade',          # Attachments specifically useful in Canada\n",
        "#     'compact tractor winter maintenance',  # Maintenance specific to colder climates\n",
        "#     'rparation tondeuse John Deere',\n",
        "#     'pices tracteur pelouse John Deere'\n",
        "# ]\n",
        "# us_small_search_trends = load_trends_from_search_api(us_under_250k_diy_keywords, search_api_io_key)\n",
        "# us_small_paid = get_keyword_estimates(client, us_under_250k_diy_keywords, us_states)\n",
        "# ca_small_search_trends = load_trends_from_search_api(ca_under_250k_diy_keywords, search_api_io_key, geo='CA')\n",
        "# ca_small_paid = get_keyword_estimates(client, ca_under_250k_diy_keywords, ca_provinces)"
      ],
      "metadata": {
        "id": "x93z7UENK4Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDYeI5kaL16Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmOnfun6J-7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVu4U-fbYZVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xJuxe-5-fxnF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ti-HmhOh0CNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5QDQbrzZ-LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naYJfD5vkwWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# us_small_search_trends_flat = us_small_search_trends.reset_index()\n",
        "# us_small_op_trends_prepped = prep_trends_data(us_small_search_trends_flat, us_all_regions, neutral_factor=0.5)\n",
        "# us_small_op_trends_prepped\n",
        "# us_small_op_paid_prepped = prep_paid_search_data(us_small_paid, us_all_regions, neutral_factor=0.5)\n",
        "# us_small_op_paid_prepped\n",
        "# counts_col='Ops_below_250k'\n",
        "# us_small_op_audiences = add_us_stats_and_geos(repositioned_us, us_small_op_trends_prepped, state_summary, us_small_op_paid_prepped, counts_col=counts_col)\n",
        "# sus_mall_op_audiences = us_small_op_audiences.to_crs(epsg=5070)\n",
        "# us_small_op_audiences.loc[us_small_op_audiences['NAME'] == 'PUERTO RICO']\n",
        "# vert_max = us_small_op_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "# data_col = 'adjusted_audience_Ops_below_250k'\n",
        "# file_stub = 'us_diy_under250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: US Farm Operations < $250k'\n",
        "# footer_txt = 'Data Sources: USDA, Google Ads API, Google Trends (Data normalized individually then summed)'\n",
        "\n",
        "# plot_us_map(us_small_op_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# us_small_op_audiences[['NAME', 'STATEFP',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "# ca_small_search_trends_flat = ca_small_search_trends.reset_index()\n",
        "# ca_small_op_trends_prepped = prep_trends_data(ca_small_search_trends_flat, ca_all_regions, neutral_factor=0.5)\n",
        "# ca_small_op_trends_prepped\n",
        "# ca_small_op_paid_prepped = prep_paid_search_data(ca_small_paid, ca_all_regions, neutral_factor=0.5)\n",
        "# ca_small_op_paid_prepped\n",
        "# counts_col='Ops_below_250k'\n",
        "# ca_small_op_audiences = add_us_stats_and_geos(canada_provinces_gdf, ca_small_op_trends_prepped, provincial_summary_canada, ca_small_op_paid_prepped, counts_col=counts_col, gdf_geo_col='PRENAME',\n",
        "#                                            stats_geo_col='geo_name')\n",
        "# ca_small_op_audiences = ca_small_op_audiences.to_crs(epsg=5070)\n",
        "# vert_max = ca_small_op_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "# data_col = 'adjusted_audience_Ops_below_250k'\n",
        "# file_stub = 'ca_diy_under250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: US Farm Operations < $250k'\n",
        "# footer_txt = 'Data Sources: 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "\n",
        "# plot_us_map(ca_small_op_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# ca_small_op_audiences[['PRENAME', 'PRUID',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "# na_small_audiences = pd.concat([us_small_op_audiences[['adjusted_audience_Ops_below_250k', 'geometry']], ca_small_op_audiences[['adjusted_audience_Ops_below_250k', 'geometry']]])\n",
        "# vert_max = na_small_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "# file_stub = 'na_diy_below_250k'\n",
        "# leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "# title_txt = f'Audience Sizing for DIY Interest: NA Farm Operations  $250k'\n",
        "# footer_txt = 'Data Sources: 2022 USDA Ag Census, 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "# plot_us_map(na_small_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n"
      ],
      "metadata": {
        "id": "gBi-bTVQth2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOVING on to rural lifestylers"
      ],
      "metadata": {
        "id": "ZGiMUWdsdgSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_us['DOMAIN_DESC'].drop_duplicates().to_list()"
      ],
      "metadata": {
        "id": "Jr4Ne1_3eQUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_acreage = (df_us[(df_us['SHORT_DESC'].str.contains(\"AREA OPERATED\")) &\n",
        "        (df_us['DOMAIN_DESC'] == \"AREA OPERATED\") &\n",
        "        (df_us['AGG_LEVEL_DESC'] == \"STATE\") &\n",
        "        (df_us['YEAR'] == 2022)])\n",
        "df_acreage['DOMAINCAT_DESC'].unique()"
      ],
      "metadata": {
        "id": "AqRbOh3RfBSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_acreage_categories = [\n",
        "    'AREA OPERATED: (1.0 TO 9.9 ACRES)'\n",
        "]\n",
        "over_10_acreage_categories = [\n",
        "    'AREA OPERATED: (70.0 TO 99.9 ACRES)',\n",
        "       'AREA OPERATED: (100 TO 139 ACRES)',\n",
        "       'AREA OPERATED: (140 TO 179 ACRES)',\n",
        "       'AREA OPERATED: (10.0 TO 49.9 ACRES)',\n",
        "       'AREA OPERATED: (50.0 TO 69.9 ACRES)',\n",
        "       'AREA OPERATED: (2,000 OR MORE ACRES)',\n",
        "       'AREA OPERATED: (180 TO 219 ACRES)',\n",
        "       'AREA OPERATED: (500 TO 999 ACRES)',\n",
        "       'AREA OPERATED: (220 TO 259 ACRES)',\n",
        "       'AREA OPERATED: (260 TO 499 ACRES)',\n",
        "       'AREA OPERATED: (1,000 TO 1,999 ACRES)'\n",
        "]"
      ],
      "metadata": {
        "id": "zJ6LKI7yfj0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operator_fields_to_use = ['AREA OPERATED', 'COMMODITY TOTALS - OPERATIONS WITH SALES']\n",
        "over_5_acre_factor = 0.35\n",
        "under_250k_factor = 0.9\n",
        "increaser_factor = 1.3\n",
        "\n",
        "below_250k_categories = [\n",
        "    'FARM SALES: (LESS THAN 1,000 $)',\n",
        "    'FARM SALES: (1,000 TO 2,499 $)',\n",
        "    'FARM SALES: (2,500 TO 4,999 $)',\n",
        "    'FARM SALES: (5,000 TO 9,999 $)',\n",
        "    'FARM SALES: (10,000 TO 24,999 $)',\n",
        "    'FARM SALES: (25,000 TO 49,999 $)',\n",
        "    'FARM SALES: (50,000 TO 99,999 $)',\n",
        "    'FARM SALES: (100,000 TO 249,999 $)'\n",
        "]\n",
        "\n",
        "above_250k_categories = [\n",
        "    'FARM SALES: (250,000 TO 499,999 $)',\n",
        "    'FARM SALES: (500,000 TO 999,999 $)',\n",
        "    'FARM SALES: (1,000,000 OR MORE $)'\n",
        "]\n",
        "\n",
        "df_us_rl_raw = df_us[\n",
        "    (df_us['SHORT_DESC'].isin(operator_fields_to_use)) &\n",
        "    (df_us['YEAR'] == 2022)].copy().reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_us_rl_raw['VALUE_NUMERIC'] = pd.to_numeric(df_us_rl_raw['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "\n",
        "df_us_rl_raw.columns.to_list()\n",
        "\n",
        "state_rl_df = df_us[\n",
        "    (df_us['SHORT_DESC'].isin(operator_fields_to_use)) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'STATE') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "\n",
        "state_rl_df['VALUE_NUMERIC'] = pd.to_numeric(state_rl_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "state_rl_df.columns.to_list()\n",
        "# Aggregate clearly by state\n",
        "state_rl_df['STATEFP'] = state_rl_df['STATE_FIPS_CODE'].astype(int).astype(str).str.zfill(2)\n",
        "state_summary = state_rl_df.groupby(['STATEFP','STATE_NAME']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "\n",
        "state_land_summary = state_rl_df.groupby(['STATEFP','STATE_NAME']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_10_acres': x[x['DOMAINCAT_DESC'].isin(split_acreage_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_10_acres_or_more': x[x['DOMAINCAT_DESC'].isin(over_10_acreage_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "state_land_summary['Ops_5_acres_or_more_raw'] = ((state_land_summary['Ops_below_10_acres'] * over_5_acre_factor) + state_land_summary['Ops_10_acres_or_more']).round(0)\n",
        "state_summary = state_summary.merge(state_land_summary[['STATEFP','Ops_5_acres_or_more_raw']], on='STATEFP', how='left')\n",
        "\n",
        "\n",
        "state_summary['Ops_5_acres_or_more_under250k'] = state_summary['Ops_5_acres_or_more_raw'] - state_summary['Ops_250k_or_more']\n",
        "\n",
        "# np.minimum((state_summary['Ops_5_acres_or_more_raw'] * under_250k_factor), (state_summary['Ops_below_250k']))\n",
        "# state_summary['revenue_proportion'] = state_summary['Ops_below_250k'] / (\n",
        "#     state_summary['Ops_below_250k'] + state_summary['Ops_250k_or_more']\n",
        "# )\n",
        "\n",
        "# # Vectorized min operation explicitly using np.minimum\n",
        "# state_summary['Ops_5_acres_or_more_adjusted_2'] = np.minimum(\n",
        "#     state_summary['Ops_5_acres_or_more'] * state_summary['revenue_proportion'] * increaser_factor,\n",
        "#     state_summary['Ops_below_250k'],  # explicitly capped\n",
        "#     state_summary['Ops_5_acres_or_more']\n",
        "# )\n",
        "# state_summary['Ops_5_acres_or_more_adjusted_2'] = min((state_summary['Ops_5_acres_or_more'] * ((state_summary['Ops_below_250k'] / (state_summary['Ops_below_250k'] + state_summary['Ops_250k_or_more'])))* increaser_factor), (state_summary['Ops_250k_or_more']))\n",
        "\n",
        "state_summary\n",
        "\n",
        "\n",
        "\n",
        "# Add total clearly\n",
        "state_summary['Total_Ops'] = state_summary['Ops_below_250k'] + state_summary['Ops_250k_or_more']\n",
        "state_summary\n",
        "# print(state_summary.head(10))\n",
        "# county_df = df_us[\n",
        "#     (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "#     (df_us['AGG_LEVEL_DESC'] == 'COUNTY') &\n",
        "#     (df_us['YEAR'] == 2022)\n",
        "# ].copy()\n",
        "\n",
        "# county_df['VALUE_NUMERIC'] = pd.to_numeric(county_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "# # Construct County FIPS explicitly\n",
        "# try:\n",
        "#     county_df['FIPS'] = county_df['STATE_FIPS_CODE'].astype(str).str.zfill(2) + \\\n",
        "#                         county_df['COUNTY_CODE'].astype(int).astype(str).str.zfill(3)\n",
        "# except ValueError:\n",
        "#     print(ValueError)\n",
        "\n",
        "# county_summary = county_df.groupby(['STATE_NAME', 'COUNTY_NAME', 'FIPS']).apply(lambda x: pd.Series({\n",
        "#     'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "#     'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "# }), include_groups=False).reset_index()\n",
        "\n",
        "# # Calculate total explicitly\n",
        "# county_summary['Total_Ops'] = county_summary['Ops_below_250k'] + county_summary['Ops_250k_or_more']\n",
        "\n",
        "# print(county_summary.head(10))\n",
        "# national_summary = df_us[\n",
        "#     (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "#     (df_us['AGG_LEVEL_DESC'] == 'NATIONAL') &\n",
        "#     (df_us['YEAR'] == 2022)\n",
        "# ].copy()\n",
        "\n",
        "# national_summary['VALUE_NUMERIC'] = pd.to_numeric(national_summary['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "# national_below_250k = national_summary[\n",
        "#     national_summary['DOMAINCAT_DESC'].isin(below_250k_categories)\n",
        "# ]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "# national_above_250k = national_summary[\n",
        "#     national_summary['DOMAINCAT_DESC'].isin(above_250k_categories)\n",
        "# ]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "# total_national_ops = national_below_250k + national_above_250k\n",
        "\n",
        "# print(f\"National Operations < $250k: {int(national_below_250k):,}\")\n",
        "# print(f\"National Operations  $250k: {int(national_above_250k):,}\")\n",
        "# print(f\"National Total Operations: {int(total_national_ops):,}\")"
      ],
      "metadata": {
        "id": "FREXMjF1df8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "break 1"
      ],
      "metadata": {
        "id": "k5gdV5Mk7Lz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_rl_audiences = repositioned_us.merge(state_summary, left_on='STATEFP', right_on='STATEFP', how='left')\n",
        "\n",
        "\n",
        "data_col = 'Ops_5_acres_or_more_under250k'\n",
        "file_stub = 'us_rl_over5_acres_under250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for Rural Life Stylers: US Farm Operations < $250k and > 5 acres'\n",
        "footer_txt = 'Data Sources: 2022 USDA Ag Census'\n",
        "vert_max = us_rl_audiences[data_col].max()\n",
        "plot_us_map(us_rl_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "counts_col='Ops_below_250k'\n",
        "us_rl_audiences[['STATE_NAME', 'STATEFP',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "us_rl_audiences_clean = us_rl_audiences.copy()\n",
        "us_rl_audiences_clean['geo_code'] = us_rl_audiences_clean['STATEFP']\n",
        "us_rl_audiences_clean['geo_name'] = us_rl_audiences_clean['STATE_NAME']\n",
        "us_rl_audiences_clean[['geo_code', 'geo_name', 'Ops_below_250k', 'Ops_250k_or_more', 'Ops_5_acres_or_more_raw', 'Ops_5_acres_or_more_under250k', 'Total_Ops']].to_csv('us_rl_audiences_clean.csv', index=False)\n",
        "\n",
        "\n",
        "ca_acre_data_url = 'https://www150.statcan.gc.ca/n1/tbl/csv/32100232-eng.zip'\n",
        "local_filename = 'ca_agriculture_acre.csv'\n",
        "df_acre_canada = load_or_download_csv(local_filename, ca_acre_data_url)\n",
        "# df_canada = df_canada.rename(columns={'Total farm revenues distribution': 'revenue_distribution'})\n",
        "# df_canada['Total farm revenues distribution'].unique()\n",
        "\n",
        "ca_over_5_under_10_factor = 0.16\n",
        "\n",
        "df_acre_canada['Total farm area distribution'].drop_duplicates().to_list()\n",
        "ca_split_acreage_categories = ['Under 10.00 acres']\n",
        "ca_over_10_acreage_categories = ['10.00 to 69.99 acres',\n",
        "    '70.00 to 129.99 acres',\n",
        "    '130.00 to 179.99 acres',\n",
        "    '180.00 to 239.99 acres',\n",
        "    '240.00 to 399.99 acres',\n",
        "    '400.00 to 559.99 acres',\n",
        "    '560.00 to 759.99 acres',\n",
        "    '760.00 to 1,119.99 acres',\n",
        "    '1,120.00 to 1,599.99 acres',\n",
        "    '1,600.00 to 2,239.99 acres',\n",
        "    '2,240.00 to 2,879.99 acres',\n",
        "    '2,880.00 to 3,519.99 acres',\n",
        "    '3,520.00 acres and over']\n",
        "\n",
        "\n",
        "geo_level='provincial'\n",
        "provincial_summary_acre_canada = categorize_revenues(df_acre_canada, geo_level=geo_level, geo_col='GEO', country_geo_val='Canada [000000000]', above_250k_cats=ca_split_acreage_categories, below_250k_cats=ca_over_10_acreage_categories, country='CA', pivot_col='Total farm area distribution')\n",
        "provincial_summary_acre_canada = extract_2_vectorized_stats(df_acre_canada, geo_level=geo_level, geo_col='GEO',\n",
        "                                                            country_geo_val='Canada [000000000]', pivot1=ca_split_acreage_categories, pivot2=ca_over_10_acreage_categories, pivot1_label='Ops_under_10_acres', pivot2_label='Ops_10_acres_or_more',\n",
        "                                                            pivot_total_label='Total_Ops', country='CA', pivot_col='Total farm area distribution', values_col='VALUE', geo_code_col='geo_code', geo_name_col='geo_name', fips_col='PRUID', agg_function='sum')\n",
        "\n",
        "provincial_summary_acre_canada = provincial_summary_acre_canada.merge(provincial_summary_canada[['PRUID','Ops_250k_or_more']], on='PRUID', how='left')\n",
        "provincial_summary_acre_canada[data_col] = ((provincial_summary_acre_canada['Ops_under_10_acres']*ca_over_5_under_10_factor)+ provincial_summary_acre_canada['Ops_10_acres_or_more'] - provincial_summary_acre_canada['Ops_250k_or_more']).round(0).astype(int)\n",
        "print('/nn')\n",
        "# print(provincial_summary_acre_canada.head())\n",
        "provincial_summary_acre_canada['PRUID'] = provincial_summary_acre_canada['PRUID'].astype(str)\n",
        "provincial_summary_acre_canada.head()\n",
        "ca_rl_over_5_under_250k = canada_provinces_gdf.merge(provincial_summary_acre_canada[['PRUID',data_col]], on='PRUID', how='left')\n",
        "\n",
        "title_txt = f'Audience Sizing for Rural Life Stylers: CA Farm Operations < $250k and > 5 acres'\n",
        "footer_txt = 'Data Sources: 2021 CA Ag Census'\n",
        "vert_max = ca_rl_over_5_under_250k[data_col].max()\n",
        "file_stub = 'ca_rl_over5_acres_under250k'\n",
        "plot_us_map(ca_rl_over_5_under_250k, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "ca_rl_over_5_under_250k_small = ca_rl_over_5_under_250k.copy()\n",
        "ca_rl_over_5_under_250k_small['geo_code'] = ca_rl_over_5_under_250k_small['PRUID'].astype(str)\n",
        "ca_rl_over_5_under_250k_small['geo_name'] = ca_rl_over_5_under_250k_small['PRENAME']\n",
        "\n",
        "ca_rl_over_5_under_250k[['PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_province.csv', index=False)\n",
        "ca_rl_over_5_under_250k_small.columns\n",
        "\n",
        "\n",
        "na_rl_over_5_under_250k = pd.concat([us_rl_audiences[['STATE_NAME', 'STATEFP',data_col, 'geometry']], ca_rl_over_5_under_250k[['PRENAME', 'PRUID',data_col, 'geometry']]])\n",
        "\n",
        "title_txt = f'Audience Sizing for Rural Life Stylers: North American Farm Operations < $250k and > 5 acres'\n",
        "footer_txt = 'Data Sources: 2022 USDA Ag Census, 2021 CA Ag Census'\n",
        "vert_max = na_rl_over_5_under_250k[data_col].max()\n",
        "file_stub = 'na_rl_over5_acres_under250k'\n",
        "plot_us_map(na_rl_over_5_under_250k, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "na_rl_over_5_under_250k[['STATE_NAME', 'STATEFP', 'PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "na_rl_over_5_under_250k\n",
        "\n",
        "\n",
        "# PRUID\n"
      ],
      "metadata": {
        "id": "owwleaLWm_Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recreational Off roaders"
      ],
      "metadata": {
        "id": "uUN0UmccLtuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_interests(client, customer_id, type='AFFINITY'):\n",
        "    googleads_service = client.get_service(\"GoogleAdsService\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "          user_interest.resource_name,\n",
        "          user_interest.user_interest_id,\n",
        "          user_interest.name\n",
        "        FROM user_interest\n",
        "        WHERE user_interest.taxonomy_type = '{type}'\n",
        "    \"\"\"\n",
        "\n",
        "    response = googleads_service.search(customer_id=customer_id, query=query)\n",
        "\n",
        "    affinity_interests = []\n",
        "    for row in response:\n",
        "        affinity_interests.append({\n",
        "            \"resource_name\": row.user_interest.resource_name,\n",
        "            \"id\": row.user_interest.user_interest_id,\n",
        "            \"name\": row.user_interest.name\n",
        "        })\n",
        "\n",
        "    return affinity_interests\n",
        "\n",
        "gads_affinity_interests = get_user_interests(client, gads_account)\n",
        "gads_affinity_interests\n",
        "gads_in_market_interests = get_user_interests(client, gads_account, type='IN_MARKET')\n"
      ],
      "metadata": {
        "id": "w4xLMmc_Vz_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import process\n",
        "\n",
        "def fuzzy_match_interests(interests_list, keywords, threshold=75):\n",
        "    matched = []\n",
        "    interest_names = [interest['name'] for interest in interests_list]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        matches = process.extract(keyword, interest_names, limit=10)\n",
        "        for name, score in matches:\n",
        "            if score >= threshold:\n",
        "                # find original interest dictionary\n",
        "                interest = next(i for i in interests_list if i['name'] == name)\n",
        "                if interest not in matched:\n",
        "                    matched.append(interest)\n",
        "    return matched"
      ],
      "metadata": {
        "id": "SXStHLZAX3u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load sentence-transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Example categories\n",
        "interest_names = [interest['name'] for interest in gads_in_market_interests]\n",
        "interest_ids = [interest['id'] for interest in gads_in_market_interests]\n",
        "\n",
        "# Embed your interests\n",
        "interest_embeddings = model.encode(interest_names, convert_to_tensor=False, show_progress_bar=True)\n",
        "\n",
        "# Setup FAISS CPU index\n",
        "dimension = interest_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add vectors to index\n",
        "index.add(np.array(interest_embeddings))\n",
        "\n",
        "# Your keywords for off-road / overlanding\n",
        "keywords = [\"air conditioner repair\", \"air conditioner replacement\", \"ac checkup\"]\n",
        "\n",
        "# Embed your keywords\n",
        "keyword_embeddings = model.encode(keywords, convert_to_tensor=False)\n",
        "\n",
        "# Perform semantic search on CPU\n",
        "k = 5  # top 5 matches per keyword\n",
        "D, I = index.search(np.array(keyword_embeddings), k)\n",
        "\n",
        "# Display matches\n",
        "for idx, keyword in enumerate(keywords):\n",
        "    print(f\"\\nTop matches for '{keyword}':\")\n",
        "    for dist, interest_idx in zip(D[idx], I[idx]):\n",
        "        print(f\"  - {interest_names[interest_idx]} (ID: {interest_ids[interest_idx]}, Distance: {dist:.2f})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nXWP9VaqY3Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "general semantic search * census"
      ],
      "metadata": {
        "id": "aD-JcYR-PTLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class SemanticSearcher:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', use_gpu=False):\n",
        "        device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
        "        self.model = SentenceTransformer(model_name, device=device)\n",
        "        self.index = None\n",
        "        self.data_items = []\n",
        "\n",
        "    def build_index(self, data_items):\n",
        "        self.data_items = data_items\n",
        "        embeddings = self.model.encode(data_items, show_progress_bar=True)\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(np.array(embeddings))\n",
        "\n",
        "    def query(self, query_items, top_k=5):\n",
        "        query_embeddings = self.model.encode(query_items)\n",
        "        distances, indices = self.index.search(np.array(query_embeddings), top_k)\n",
        "        matches = {}\n",
        "        for idx, query in enumerate(query_items):\n",
        "            matches[query] = [\n",
        "                {'item': self.data_items[i], 'distance': float(distances[idx][j])}\n",
        "                for j, i in enumerate(indices[idx])\n",
        "            ]\n",
        "        return matches\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class ACSSemanticSearcher:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2', use_gpu=False):\n",
        "        device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
        "        self.model = SentenceTransformer(model_name, device=device)\n",
        "        self.index = None\n",
        "        self.data = []\n",
        "\n",
        "    def build_index(self, data):\n",
        "        self.data = data\n",
        "        descriptions = [item['description'] for item in data]\n",
        "        embeddings = self.model.encode(descriptions, show_progress_bar=True)\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(np.array(embeddings))\n",
        "\n",
        "    def query(self, query_texts, top_k=5):\n",
        "        query_embeddings = self.model.encode(query_texts)\n",
        "        distances, indices = self.index.search(np.array(query_embeddings), top_k)\n",
        "        matches = {}\n",
        "        for idx, query in enumerate(query_texts):\n",
        "            matches[query] = [\n",
        "                {\n",
        "                    'name': self.data[i]['name'],\n",
        "                    'description': self.data[i]['description'],\n",
        "                    'variables': self.data[i]['variables'],\n",
        "                    'universe': self.data[i]['universe '].strip(),\n",
        "                    'distance': float(distances[idx][j])\n",
        "                }\n",
        "                for j, i in enumerate(indices[idx])\n",
        "            ]\n",
        "        return matches"
      ],
      "metadata": {
        "id": "ijWFussRPSu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aafm9EogY3R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C4EPOj7RP6Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords tailored to your audience\n",
        "keywords = ['off-road', 'auto', '4x4', 'outdoor', 'camping', 'recreational', 'adventure', 'trail', 'motor']\n",
        "\n",
        "# Get matched interests\n",
        "matched_affinity = fuzzy_match_interests(gads_affinity_interests, keywords)\n",
        "matched_inmarket = fuzzy_match_interests(gads_in_market_interests, keywords)\n",
        "\n",
        "print(\"Affinity Matches:\")\n",
        "for match in matched_affinity:\n",
        "    print(match)\n",
        "\n",
        "print(\"\\nIn-Market Matches:\")\n",
        "for match in matched_inmarket:\n",
        "    print(match)"
      ],
      "metadata": {
        "id": "1jLd9g6eX5v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_relevant_interests(interests_list, keywords):\n",
        "    relevant = []\n",
        "    for interest in interests_list:\n",
        "        if any(keyword.lower() in interest['name'].lower() for keyword in keywords):\n",
        "            relevant.append(interest)\n",
        "    return relevant\n",
        "\n",
        "# Define keywords relevant to offroad/overlanding\n",
        "keywords = ['off-road', 'auto', '4x4', 'outdoor', 'camping', 'recreational', 'adventure', 'trail', 'motor', 'overland', 'off road']\n",
        "\n",
        "# Example usage:\n",
        "relevant_affinity = filter_relevant_interests(gads_affinity_interests, keywords)\n",
        "relevant_inmarket = filter_relevant_interests(gads_in_market_interests, keywords)\n",
        "\n",
        "print(\"Affinity:\")\n",
        "for interest in relevant_affinity:\n",
        "    print(interest)\n",
        "\n",
        "print(\"\\nIn-Market:\")\n",
        "for interest in relevant_inmarket:\n",
        "    print(interest)"
      ],
      "metadata": {
        "id": "JzKIW1rpXWGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_custom_segment_geo_insights(client, customer_id, custom_segment_resource_name, geo_location_ids):\n",
        "    audience_insights_service = client.get_service(\"AudienceInsightsService\")\n",
        "    googleads_service = client.get_service(\"GoogleAdsService\")\n",
        "\n",
        "    insights_results = []\n",
        "\n",
        "    for location_id in geo_location_ids:\n",
        "        request = client.get_type(\"GenerateAudienceCompositionInsightsRequest\")\n",
        "        request.customer_id = customer_id\n",
        "\n",
        "        # Define custom segment audience\n",
        "        custom_segment_attr = client.get_type(\"AudienceInsightsAttribute\")\n",
        "        custom_segment_attr.custom_audience = custom_segment_resource_name\n",
        "\n",
        "        insights_group = client.get_type(\"InsightsAudienceAttributeGroup\")\n",
        "        insights_group.attributes.append(custom_segment_attr)\n",
        "\n",
        "        request.audience.topic_audience_combinations.append(insights_group)\n",
        "\n",
        "        # Location info\n",
        "        location_info = client.get_type(\"LocationInfo\")\n",
        "        location_info.geo_target_constant = googleads_service.geo_target_constant_path(location_id)\n",
        "        request.audience.country_locations.append(location_info)\n",
        "\n",
        "        request.dimensions.append(\"GEO_TARGET_REGION\")\n",
        "\n",
        "        response = audience_insights_service.generate_audience_composition_insights(request=request)\n",
        "\n",
        "        for dimension in response.dimensions:\n",
        "            insights_results.append({\n",
        "                \"region\": dimension.geo_target_constant.name,\n",
        "                \"audience_size\": dimension.metrics.audience_size,\n",
        "                \"audience_share\": dimension.metrics.audience_share,\n",
        "            })\n",
        "\n",
        "    return insights_results\n",
        "\n",
        "def audience_composition_insights(\n",
        "    client,\n",
        "    audience_insights_service,\n",
        "    googleads_service,\n",
        "    customer_id,\n",
        "    location_id,\n",
        "    user_interest,\n",
        "    custom_name,\n",
        "):\n",
        "    \"\"\"Returns a collection of attributes represented in an audience of interest.\n",
        "\n",
        "        Please refere here for more:\n",
        "        https://developers.google.com/google-ads/api/data/codes-formats\n",
        "\n",
        "    Args:\n",
        "        client: an initialized GoogleAdsClient instance.\n",
        "        audience_insights_service: an initialized AudienceInsightsService\n",
        "          instance.\n",
        "        googleads_service: an initialized GoogleAds Service instance.\n",
        "        customer_id: The customer ID for the audience insights service.\n",
        "        location_id: The location ID for the audience of interest.\n",
        "        user_interest: The criterion ID of the category.\n",
        "        custom_name: custom defined name.\n",
        "    \"\"\"\n",
        "    request = client.get_type(\"GenerateAudienceCompositionInsightsRequest\")\n",
        "    request.customer_id = customer_id\n",
        "\n",
        "    insights_info = client.get_type(\"InsightsAudienceAttributeGroup\")\n",
        "    attributes = client.get_type(\"AudienceInsightsAttribute\")\n",
        "    attributes.user_interest.user_interest_category = (\n",
        "        googleads_service.user_interest_path(customer_id, user_interest)\n",
        "    )\n",
        "\n",
        "    insights_info.attributes.append(attributes)\n",
        "    request.audience.topic_audience_combinations.append(insights_info)\n",
        "\n",
        "    location_info = client.get_type(\"LocationInfo\")\n",
        "    location_info.geo_target_constant = (\n",
        "        googleads_service.geo_target_constant_path(location_id)\n",
        "    )\n",
        "    request.audience.country_locations.append(location_info)\n",
        "\n",
        "    request.customer_insights_group = custom_name\n",
        "    request.dimensions = (\n",
        "        \"AFFINITY_USER_INTEREST\",\n",
        "        \"IN_MARKET_USER_INTEREST\",\n",
        "        \"YOUTUBE_CHANNEL\",\n",
        "    )\n",
        "    response = audience_insights_service.generate_audience_composition_insights(\n",
        "        request=request\n",
        "    )\n",
        "    print(response)\n",
        "\n",
        "keywords = keywords = [\"overlanding\", \"off-road vehicles\", \"4x4 accessories\"]\n",
        "# geo_location_ids = [\"21137\", \"21176\", \"9000964\"]  # Example: California, Texas, Ontario\n",
        "\n",
        "us_geo_location_ids =[]\n",
        "\n",
        "for geo_name, geo_location_id in us_states_google.items():\n",
        "    # print(geo_name, geo_location_id)\n",
        "    us_geo_location_ids.append(geo_location_id)\n",
        "\n",
        "ca_geo_location_ids =[]\n",
        "\n",
        "for geo_name, geo_location_id in ca_provinces_google.items():\n",
        "    # print(geo_name, geo_location_id)\n",
        "    ca_geo_location_ids.append(geo_location_id)\n",
        "\n",
        "# audience_insights_service = client.get_service(\"AudienceInsightsService\")\n",
        "# googleads_service = client.get_service(\"GoogleAdsService\")\n",
        "# results = audience_composition_insights(client, audience_insights_service,  googleads_service, '7759116401', '21138', '91505', 'off-over-empty')\n",
        "\n",
        "# for r in results:\n",
        "#     print(f\"{r['region']}: {r['audience_size']} ({r['audience_share']:.2%})\")\n",
        "\n",
        "# # us_states ca_provinces"
      ],
      "metadata": {
        "id": "gyoiAPVFLwFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using census data for land ownership"
      ],
      "metadata": {
        "id": "0Nvr891ChBVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the tables for the ACS 5-year data\n",
        "acs5_tables = c.acs5.tables()\n",
        "acs5_tables_df = pd.DataFrame(acs5_tables)\n",
        "acs5_tables_df.head()\n",
        "acs5_tables_df.loc[acs5_tables_df['name'].str.contains('B25056')]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xb9LdyyDMYUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "searcher = ACSSemanticSearcher(use_gpu=True)\n",
        "searcher.build_index(data = acs5_tables)"
      ],
      "metadata": {
        "id": "_47XAOE-TNMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = searcher.query([\"american housing survey\"], top_k=8)\n",
        "\n",
        "for query, matches in results.items():\n",
        "    print(f\"\\nMatches for '{query}':\")\n",
        "    for match in matches:\n",
        "        print(f\"  - Table: {match['name']}\")\n",
        "        print(f\"    Description: {match['description']}\")\n",
        "        print(f\"    Variables URL: {match['variables']}\")\n",
        "        print(f\"    Universe: {match['universe']}\")\n",
        "        print(f\"    Distance: {match['distance']:.2f}\\n\")"
      ],
      "metadata": {
        "id": "ZJxfJpvpQMD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# us_land = pd.read_csv('/content/us_land_ownership.csv')\n",
        "# us_land.columns.to_list()\n",
        "# us_land['geo_code'] = us_land['GEOID'].astype(str).str.zfill(2)\n",
        "# us_land['geo_name'] = us_land['NAME'].str.upper()\n",
        "# good_cols = ['geo_code', 'geo_name',  'under_0.5_acres',\n",
        "#  '0.5_to_5_acres',\n",
        "#  '5_to_10_acres',\n",
        "#  'over_10_acres',\n",
        "#  'over_5_acres']\n",
        "\n",
        "\n",
        "\n",
        "# us_land_good = us_land[good_cols]\n",
        "# us_land_good.to_csv('us_land_ownership_updated.csv', index=False)\n",
        "\n",
        "\n",
        "# ca_land_gpd = gpd.read_file('/content/ca_land_ownership-simplified.geojson')\n",
        "\n",
        "# ca_land = pd.read_csv('/content/us_land_ownership.csv')\n",
        "# us_land.columns.to_list()\n",
        "# us_land['geo_code'] = us_land['GEOID'].astype(str).str.zfill(2)\n",
        "# us_land['geo_name'] = us_land['NAME'].str.upper()\n",
        "# good_cols = ['geo_code', 'geo_name',  'under_0.5_acres',\n",
        "#  '0.5_to_5_acres',\n",
        "#  '5_to_10_acres',\n",
        "#  'over_10_acres',\n",
        "#  'over_5_acres']\n",
        "\n",
        "\n",
        "\n",
        "# us_land_good = us_land[good_cols]\n",
        "# us_land_good.to_csv('us_land_ownership_updated.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "avCptN0cBwzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acs_variables = {\n",
        "    \"B25056_002E\": \"Under 1 acre\",\n",
        "    \"B25056_003E\": \"1 to 9.9 acres\",\n",
        "    \"B25056_004E\": \"10 or more acres\"\n",
        "}\n",
        "\n",
        "# Fetch county-level ACS data (all counties)\n",
        "acs_data = c.acs5.get(list(acs_variables.keys()), {'for': 'county:*'})\n",
        "\n",
        "df_acs = pd.DataFrame(acs_data).rename(columns=acs_variables)\n",
        "\n",
        "# Convert numeric columns\n",
        "for col in acs_variables.values():\n",
        "    df_acs[col] = pd.to_numeric(df_acs[col])\n",
        "\n",
        "# Create full FIPS code\n",
        "df_acs['FIPS'] = df_acs['state'] + df_acs['county']\n",
        "\n",
        "df_acs = df_acs[['FIPS', 'Under 1 acre', '1 to 9.9 acres', '10 or more acres']]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XS1ZKCqMMB5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjRg8bzTk1mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_rucc = pd.read_excel('/content/Ruralurbancontinuumcodes2023.xlsx', sheet_name='Rural-urban Continuum Code 2023')\n",
        "df_rucc['FIPS'] = df_rucc['FIPS'].astype(str).str.zfill(5)\n",
        "df_final = df_acs.merge(df_rucc[['FIPS', 'RUCC_2023', 'Description']], on='FIPS', how='left')"
      ],
      "metadata": {
        "id": "Gg6FoVVYl0cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_area(rucc):\n",
        "    if rucc <= 3:\n",
        "        return 'Urban'\n",
        "    elif rucc <= 6:\n",
        "        return 'Suburban'\n",
        "    else:\n",
        "        return 'Rural'\n",
        "\n",
        "df_final['AreaType'] = df_final['RUCC_2023'].apply(classify_area)\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "yhyifUCepQWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allocation_factors = {\n",
        "    'Urban': {'under_05': 0.75, '05_to_1': 0.25, '1_to_5': 0.85, '5_to_10': 0.15},\n",
        "    'Suburban': {'under_05': 0.40, '05_to_1': 0.60, '1_to_5': 0.60, '5_to_10': 0.40},\n",
        "    'Rural': {'under_05': 0.10, '05_to_1': 0.90, '1_to_5': 0.45, '5_to_10': 0.55}\n",
        "}\n",
        "\n",
        "# Correct allocations:\n",
        "df_final['under_0.5_acres'] = (df_final.apply(\n",
        "    lambda row: row['Under 1 acre'] * allocation_factors[row['AreaType']]['under_05'], axis=1)).round(0).astype(int)\n",
        "\n",
        "df_final['0.5_to_5_acres'] = (df_final.apply(\n",
        "    lambda row: (row['Under 1 acre'] * allocation_factors[row['AreaType']]['05_to_1']) +\n",
        "                (row['1 to 9.9 acres'] * allocation_factors[row['AreaType']]['1_to_5']),\n",
        "    axis=1)).round(0).astype(int)\n",
        "\n",
        "df_final['5_to_10_acres'] = (df_final.apply(\n",
        "    lambda row: row['1 to 9.9 acres'] * allocation_factors[row['AreaType']]['5_to_10'], axis=1))\n",
        "\n",
        "df_final['over_10_acres'] = df_final['10 or more acres'].round(0).astype(int)\n",
        "df_final['STATEFP'] = df_final['FIPS'].str[:2]\n",
        "df_final['geo_code'] = df_final['STATEFP']\n",
        "df_final['COUNTY'] = df_final['FIPS'].str[2:]\n",
        "df_final_summary = df_final.groupby(['geo_code'])[['under_0.5_acres', '0.5_to_5_acres', '5_to_10_acres', 'over_10_acres']].sum().reset_index()\n",
        "df_final_summary = pd.merge(df_final_summary, df_us_state_pop[['geo_code', 'geo_name']], on='geo_code', how='inner')\n",
        "df_final_summary[['geo_code', 'geo_name', 'under_0.5_acres', '0.5_to_5_acres', '5_to_10_acres', 'over_10_acres']].to_csv('us_land_ownership_new.csv', index=False)\n",
        "us_land_ownership_summary = df_final.groupby(['STATEFP'])[['under_0.5_acres', '0.5_to_5_acres', '5_to_10_acres', 'over_10_acres']].sum().reset_index()\n",
        "us_land_ownership_summary['over_5_acres'] =  (us_land_ownership_summary['5_to_10_acres'] + us_land_ownership_summary['over_10_acres']).round(0).astype(int)\n",
        "us_land_ownership_summary['under_0.5_acres'] = us_land_ownership_summary['under_0.5_acres'].round().astype(int)\n",
        "us_land_ownership_summary['0.5_to_5_acres'] = us_land_ownership_summary['0.5_to_5_acres'].round().astype(int)\n",
        "us_land_ownership_summary\n",
        "df_final.head()\n",
        "us_land_ownership = repositioned_us.merge(us_land_ownership_summary[['STATEFP', 'under_0.5_acres', '0.5_to_5_acres', '5_to_10_acres', 'over_10_acres', 'over_5_acres']], on=['STATEFP'], how='left')\n",
        "#\n",
        "\n",
        "\n",
        "us_land_ownership.to_csv('us_land_ownership.csv', index=False)\n",
        "\n",
        "data_col = 'under_0.5_acres'\n",
        "title_txt = f'Audience Sizing for Land Owners < 0.5 acres'\n",
        "footer_txt = 'Data Sources: 2022 US American Community Survey and 2023 USDA  Rural-Urban Continuum Codes'\n",
        "vert_max = us_land_ownership[data_col].max()\n",
        "file_stub = 'nus_land'\n",
        "\n",
        "plot_us_map(us_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# us_land_ownership[['STATE_NAME', 'STATEFP', 'PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "data_col = '0.5_to_5_acres'\n",
        "title_txt = f'Audience Sizing for Land Owners 0.5 to 5 acres'\n",
        "footer_txt = 'Data Sources: 2022 US American Community Survey and 2023 USDA  Rural-Urban Continuum Codes'\n",
        "vert_max = us_land_ownership[data_col].max()\n",
        "file_stub = 'nus_land'\n",
        "\n",
        "plot_us_map(us_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# us_land_ownership[['STATE_NAME', 'STATEFP', 'PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "data_col = 'over_5_acres'\n",
        "title_txt = f'Audience Sizing for Land Owners over 5 acres'\n",
        "footer_txt = 'Data Sources: 2022 US American Community Survey and 2023 USDA  Rural-Urban Continuum Codes'\n",
        "vert_max = us_land_ownership[data_col].max()\n",
        "file_stub = 'nus_land'\n",
        "\n",
        "plot_us_map(us_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "us_land_ownership['geo_code'] = us_land_ownership['STATEFP']\n",
        "us_land_ownership_summary = us_land_ownership.groupby(['geo_code'])[['under_0.5_acres', '0.5_to_5_acres', '5_to_10_acres', 'over_10_acres']].sum().reset_index()\n",
        "\n",
        "# us_land_ownership_summary_states = pd.merge(us_land_ownership_summary, us_land_ownership[['STATEFP', 'STATE_NAME', 'PRENAME', 'PRUID']], on=['STATEFP'], how='left')\n",
        "us_land_ownership_summary_export = pd.merge(us_land_ownership_summary, us_land_ownership[['geo_code', 'NAME']], on=['geo_code'], how='left')\n",
        "us_land_ownership_summary_export['geo_name'] = us_land_ownership_summary_export['NAME'].str.upper()\n",
        "us_land_ownership_summary_export[['geo_code', 'geo_name','under_0.5_acres','0.5_to_5_acres','5_to_10_acres','over_10_acres'  ]].to_csv('us_land_ownership_summary_new.csv', index=False)\n",
        "# us_land_ownership[['STATE_NAME', 'STATEFP', 'PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ujrxVhMiprLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "CaLgwxCRfb5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "operator_fields_to_use = ['AREA OPERATED', 'COMMODITY TOTALS - OPERATIONS WITH SALES']\n",
        "over_5_acre_factor = 0.35\n",
        "under_250k_factor = 0.9\n",
        "increaser_factor = 1.3\n",
        "\n",
        "ca_split_acreage_categories = ['Under 10.00 acres']\n",
        "\n",
        "\n",
        "\n",
        "ca_over_10_acreage_categories = ['10.00 to 69.99 acres',\n",
        "    '70.00 to 129.99 acres',\n",
        "    '130.00 to 179.99 acres',\n",
        "    '180.00 to 239.99 acres',\n",
        "    '240.00 to 399.99 acres',\n",
        "    '400.00 to 559.99 acres',\n",
        "    '560.00 to 759.99 acres',\n",
        "    '760.00 to 1,119.99 acres',\n",
        "    '1,120.00 to 1,599.99 acres',\n",
        "    '1,600.00 to 2,239.99 acres',\n",
        "    '2,240.00 to 2,879.99 acres',\n",
        "    '2,880.00 to 3,519.99 acres',\n",
        "    '3,520.00 acres and over']\n",
        "\n",
        "\n",
        "below_250k_categories = [\n",
        "\n",
        "     '$0',\n",
        "        '$1 to $9,999',\n",
        "\n",
        "        '$10,000 to $24,999',\n",
        "        '$100,000 to $249,999',\n",
        "\n",
        "        '$25,000 to $49,999',\n",
        "\n",
        "        '$50,000 to $99,999'\n",
        "\n",
        "]\n",
        "\n",
        "above_250k_categories = [\n",
        "\n",
        "            '$500,000 to $999,999',\n",
        "            '$250,000 to $499,999',\n",
        "            '$2,000,000 and over',\n",
        "        '$1,000,000 to $1,999,999'\n",
        "]\n",
        "\n",
        "geo_level='csd'\n",
        "\n",
        "ca_farmland_df = sc.table_to_df(\"32100232\")\n",
        "ca_farmland_df['VALUE'] = ca_farmland_df['VALUE'].fillna(0.0).astype(int)\n",
        "ca_csd_farmland_df = ca_farmland_df.loc[ca_farmland_df['DGUID'].str.len() == 16]\n",
        "ca_csd_farmland_df_filtered = ca_csd_farmland_df.loc[~(ca_csd_farmland_df['Total farm area distribution'] == 'Total number of farms')]\n",
        "ca_csd_farmland_df_filtered['geo_name'] = ''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_col = 'Total farm area distribution'\n",
        "geo_col='GEO'\n",
        "pivot1=ca_split_acreage_categories\n",
        "pivot2=ca_over_10_acreage_categories\n",
        "pivot1_label='Ops_under_10_acres'\n",
        "pivot2_label='Ops_10_acres_or_more'\n",
        "pivot_total_label='Total_Ops'\n",
        "country='CA'\n",
        "pivot_col='Total farm area distribution'\n",
        "values_col='VALUE'\n",
        "geo_code_col='DGUID'\n",
        "geo_name_col='geo_name'\n",
        "fips_col='geo_code'\n",
        "agg_function='sum'\n",
        "\n",
        "summary_df = ca_csd_farmland_df_filtered.pivot_table(\n",
        "    index=geo_code_col,\n",
        "    columns=pivot_col,\n",
        "    values=values_col,\n",
        "    aggfunc=agg_function,\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "# summary_df['Ops_10_acres_or_more'] = summary_df.copy().reset_index()\n",
        "\n",
        "# Explicit aggregation for below and above $250k\n",
        "summary_df[pivot1_label] = summary_df[pivot1].sum(axis=1)\n",
        "summary_df[pivot2_label] = summary_df[pivot2].sum(axis=1)\n",
        "# summary_df[geo_code_col] = summary_df[geo_col].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "# summary_df[geo_name_col] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "summary_df[pivot_total_label] = summary_df[pivot1_label] + summary_df[pivot2_label]\n",
        "ca_csd_farmland_df_filtered_summary = pd.merge(ca_csd_farmland_df_filtered, summary_df, on=geo_code_col, how='left')\n",
        "ca_csd_farmland_df_filtered_summary[[pivot1_label, pivot2_label]] = ca_csd_farmland_df_filtered_summary[[pivot1_label, pivot2_label]].fillna(0.0).round(0).astype(int)\n",
        "\n",
        "# summary_df[fips_col] = (\n",
        "#     summary_df[geo_code_col]\n",
        "#     .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "#     .str[:2]                             # clearly take the first two characters\n",
        "#     .astype(int)                         # clearly convert to integer\n",
        "# )\n",
        "final_columns = [geo_code_col, pivot1_label, pivot2_label, pivot_total_label]\n",
        "ca_csd_farmland_df_filtered_summary['geo_code'] = ca_csd_farmland_df_filtered_summary['DGUID']\n",
        "ca_csd_farmland_df_filtered_summary = ca_csd_farmland_df_filtered_summary[final_columns]\n",
        "ca_csd_farmland_df_filtered_summary.columns = final_columns\n",
        "ca_csd_farmland_df_filtered_summary = ca_csd_farmland_df_filtered_summary.copy().sort_values('DGUID').reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# adding rev - but don't care yet\n",
        "farmland_geo_codes = ca_csd_farmland_df_filtered_summary['DGUID'].drop_duplicates().to_list()\n",
        "len(farmland_geo_codes)\n",
        "ca_csd_farmIncome_df = df_canada\n",
        "ca_csd_farmIncome_df['geo_code'] = ca_csd_farmIncome_df['DGUID'].str[-7:]\n",
        "# ca_csd_farmIncome_df_filtered = ca_csd_farmIncome_df.loc[~(ca_csd_farmIncome_df['Total farm revenues distribution'] == 'Total number of farms') & (ca_csd_farmIncome_df['geo_code'].isin(farmland_geo_codes))].sort_values(by='geo_code').reset_index(drop=True)\n",
        "ca_csd_farmIncome_df_filtered = ca_csd_farmIncome_df.loc[~(ca_csd_farmIncome_df['Total farm revenues distribution'] == 'Total number of farms') ].sort_values(by='geo_code').reset_index(drop=True)\n",
        "ca_csd_farmIncome_df_filtered.head()\n",
        "data_col = 'Ops_5_acres_or_more'\n",
        "geo_col='DGUID'\n",
        "pivot1=ca_split_acreage_categories\n",
        "pivot2=ca_over_10_acreage_categories\n",
        "pivot1_label='Ops_below_250k'\n",
        "pivot2_label='Ops_250k_or_more'\n",
        "\n",
        "pivot_total_label='Total_Ops'\n",
        "country='CA'\n",
        "pivot_col='Total farm revenues distribution'\n",
        "values_col='VALUE'\n",
        "geo_code_col='geo_code'\n",
        "geo_name_col='geo_name'\n",
        "fips_col='geo_code'\n",
        "agg_function='sum'\n",
        "summary_df2 = ca_csd_farmIncome_df_filtered.pivot_table(\n",
        "    index=geo_code_col,\n",
        "    columns=pivot_col,\n",
        "    values='VALUE',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "\n",
        "# Explicit aggregation for below and above $250k\n",
        "summary_df2['Ops_below_250k'] = summary_df2[below_250k_categories].sum(axis=1)\n",
        "summary_df2['Ops_250k_or_more'] = summary_df2[above_250k_categories].sum(axis=1)\n",
        "summary_df2['Ops_below_250k'] = summary_df2['Ops_below_250k'].round(0).astype(int)\n",
        "summary_df2.columns.to_list()\n",
        "# summary_df['geo_code'] = summary_df[geo_col].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "# summary_df['geo_name'] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "summary_df2['Total_Ops_with_rev'] = summary_df2['Ops_below_250k'] + summary_df2['Ops_250k_or_more']\n",
        "\n",
        "\n",
        "# summary_df['PRUID'] = (\n",
        "#     summary_df['geo_code']\n",
        "#     .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "#     .str[:2]                             # clearly take the first two characters\n",
        "#     .astype(int)                         # clearly convert to integer\n",
        "# )\n",
        "final_columns = ['geo_code', 'Ops_below_250k', 'Ops_250k_or_more', 'Total_Ops_with_rev']\n",
        "summary_df2 = summary_df2[final_columns]\n",
        "summary_df2.columns = final_columns\n",
        "summary_df2 = summary_df2.copy().sort_values('geo_code').reset_index(drop=True)\n",
        "\n",
        "\n",
        "summary_df['geo_code'] = summary_df['DGUID']\n",
        "total_summary_df = pd.merge(summary_df, summary_df2, on='geo_code', how='left')\n",
        "total_summary_df\n",
        "\n",
        "\n",
        "\n",
        "ca_farmland_df = sc.table_to_df(\"32100232\")\n",
        "ca_farmland_df.columns.to_list()\n",
        "\n",
        "# use DGUID of 16 for census farm area / 11 for province\n",
        "# ca_csd_farmland_df = ca_farmland_df.loc[ca_farmland_df['DGUID'].str.len() == 16]\n",
        "# ca_csd_farmland_df['ca_geo_code'] = ca_farmland_df['DGUID'].str[-7:]\n",
        "ca_csd_farmland_df['geo_code'] = (ca_farmland_df['DGUID'])\n",
        "ca_csd_farmland_df['Total farm area distribution'].drop_duplicates().to_list()\n",
        "\n",
        "ca_csd_farmland_df = ca_csd_farmland_df.loc[ca_csd_farmland_df['Total farm area distribution'] != 'Total number of farms'].copy()\n",
        "\n",
        "ca_csd_farmland_df['Total farm area distribution'].drop_duplicates().to_list()\n",
        "ca_csd_farmland_df.head()\n",
        "\n",
        "ca_csd_farmland_df = ca_csd_farmland_df.pivot_table(\n",
        "    index='geo_code',\n",
        "    columns='Total farm area distribution',\n",
        "    values='VALUE',\n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ").reset_index()\n",
        "\n",
        "# Explicit aggregation for below and above $250k\n",
        "ca_csd_farmland_df['Ops_under_10_acres'] = ca_csd_farmland_df[ca_split_acreage_categories].sum(axis=1).fillna(0.0).astype(int)\n",
        "ca_csd_farmland_df['Ops_10_acres_or_more'] = ca_csd_farmland_df[ca_over_10_acreage_categories].sum(axis=1).fillna(0.0).astype(int)\n",
        "ca_csd_farmland_df = ca_csd_farmland_df[['geo_code', 'Ops_under_10_acres', 'Ops_10_acres_or_more']].copy().reset_index(drop=True)\n",
        "ca_csd_farmland_df.head()\n",
        "\n",
        "# summary_df['geo_code'] = summary_df['geo_code'].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "# summary_df['geo_name'] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "summary_df['Total_Ops'] = summary_df['Ops_below_250k'] + summary_df['Ops_250k_or_more']\n",
        "summary_df['PRUID'] = (\n",
        "    summary_df['geo_code']\n",
        "    .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "    .str[:2]                             # clearly take the first two characters\n",
        "    .astype(int)                         # clearly convert to integer\n",
        ")\n",
        "final_columns = ['PRUID','geo_name', 'geo_code', 'Ops_below_250k', 'Ops_250k_or_more', 'Total_Ops']\n",
        "summary_df = summary_df[final_columns]\n",
        "summary_df.columns = final_columns\n",
        "summary_df = summary_df.copy().sort_values('PRUID').reset_index(drop=True)\n",
        "\n",
        "# post income\n",
        "\n",
        "\n",
        "geo_level='provincial'\n",
        "ca_csd_farmland_summary_df = categorize_revenues(df_acre_canada, geo_level=geo_level, geo_col='GEO', country_geo_val='Canada [000000000]', above_250k_cats=ca_split_acreage_categories, below_250k_cats=ca_over_10_acreage_categories, country='CA', pivot_col='Total farm area distribution')\n",
        "ca_csd_farmland_df.head()\n",
        "\n",
        "#\n",
        "farm_area_dist_values =  ['Under 10.00 acres',\n",
        " '10.00 to 69.99 acres',\n",
        " '70.00 to 129.99 acres',\n",
        " '130.00 to 179.99 acres',\n",
        " '180.00 to 239.99 acres',\n",
        " '240.00 to 399.99 acres',\n",
        " '400.00 to 559.99 acres',\n",
        " '560.00 to 759.99 acres',\n",
        " '760.00 to 1,119.99 acres',\n",
        " '1,120.00 to 1,599.99 acres',\n",
        " '1,600.00 to 2,239.99 acres',\n",
        " '2,240.00 to 2,879.99 acres',\n",
        " '2,880.00 to 3,519.99 acres',\n",
        " '3,520.00 acres and over']\n",
        "\n",
        "ca_hh_df = sc.table_to_df(\"9810000201\")\n",
        "\n",
        "ca_hh_df.columns.to_list()\n",
        "# ca_gaf =  pd.read_csv('/content/2021_92-151_Xb.csv', encoding='ISO-8859-1', dtype=str )\n",
        "occupied_dwellings_col = 'Population and dwelling counts (13): Private dwellings occupied by usual residents, 2021 [7]'\n",
        "# ca_rucc = sc.table_to_df(\"48f544ed-e578-436c-8460-eacb64e61a9d\")\n",
        "ca_rucc = gpd.read_file('/content/gpc_000a11a_e.zip')\n",
        "\n",
        "ca_rucc['geo_code'] = ca_rucc['PRUID'].astype(str).str.zfill(2) + ca_rucc['PCUID'].astype(str).str.zfill(5)\n",
        "\n",
        "ca_hh_df['household_count'] = ca_hh_df[occupied_dwellings_col]\n",
        "\n",
        "ca_hh_df = ca_hh_df[['GEO', 'DGUID', 'household_count']]\n",
        "ca_hh_df = ca_hh_df.loc[~(ca_hh_df['GEO'] == 'Canada')]\n",
        "ca_hh_df['GEO'] = ca_hh_df['GEO'].str.strip().str.upper()\n",
        "ca_hh_df.head()\n",
        "ca_hh_df.loc[ca_hh_df['GEO'].str.contains('CSD')]\n",
        "ca_csd_hh_df = ca_hh_df[ca_hh_df['DGUID'].str.len() == 16]\n",
        "ca_csd_hh_df['PRUID'] = ca_csd_hh_df['DGUID'].str[-7:]\n",
        "ca_csd_hh_df['geo_code'] = ca_csd_hh_df['PRUID'].str[:2]\n",
        "\n",
        "ca_csd_hh_df['household_count'] = ca_csd_hh_df['household_count'].fillna(0).astype(int)\n",
        "ca_csd_hh_df_summary = ca_csd_hh_df.groupby('geo_code')['household_count'].sum().reset_index()\n",
        "ca_csd_hh_df_summary.head()\n",
        "\n",
        "merged_csd_dr = pd.merge(ca_csd_hh_df_summary, ca_csd_farmland_df, on='geo_code', how='left',suffixes=('', '_hh')).copy().sort_values(by='geo_code').reset_index(drop=True)\n",
        "merged_csd_dr['Ops_under_10_acres'] = merged_csd_dr['Ops_under_10_acres'].fillna(0).astype(int)\n",
        "merged_csd_dr['Ops_10_acres_or_more'] = merged_csd_dr['Ops_10_acres_or_more'].fillna(0).astype(int)\n",
        "# merged_csd_dr['Total_Ops'] = merged_csd_dr['Total_Ops'].fillna(0).astype(int)\n",
        "# merged_csd_dr['Total_Ops_with_rev'] = merged_csd_dr['Total_Ops_with_rev'].fillna(0).astype(int)\n",
        "merged_csd_dr['Ops_5_acres_or_more'] = merged_csd_dr['Ops_5_acres_or_more'].fillna(0).astype(int)\n",
        "# merged_csd_dr['Ops_below_250k'] = merged_csd_dr['Ops_below_250k'].fillna(0).astype(int)\n",
        "# merged_csd_dr['Ops_250k_or_more'] = merged_csd_dr['Ops_250k_or_more'].fillna(0).astype(int)\n",
        "merged_csd_dr['household_count'] = merged_csd_dr['household_count'].fillna(0).astype(int)\n",
        "merged_csd_dr['HH_after_farms'] = merged_csd_dr['household_count'] - merged_csd_dr['Total_Ops']\n",
        "merged_csd_dr\n",
        "\n",
        "\n",
        "26205 2021S05021001214\n",
        "Division No. 1, Subd. C, Newfoundland and Lab...\n",
        "ca_gaf['POPCTRRACLASS_CTRPOPRRCLASSE'] = ca_gaf['POPCTRRACLASS_CTRPOPRRCLASSE'].astype(str)\n",
        "ca_gaf['CSDUID_SDRIDU'] = ca_gaf['CSDUID_SDRIDU'].astype(str)\n",
        "full_csd_data_classed  = pd.merge(\n",
        "    merged_csd_dr,\n",
        "    ca_gaf[['CSDUID_SDRIDU', 'POPCTRRACLASS_CTRPOPRRCLASSE']],\n",
        "    left_on='geo_code',\n",
        "    right_on='CSDUID_SDRIDU',\n",
        "    how='left'\n",
        ")\n",
        "full_csd_data_classed.head()\n"
      ],
      "metadata": {
        "id": "AwTtJL6-f3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# df = full_csd_data_classed.copy()\n",
        "# Define farm-to-acreage allocation assumptions explicitly\n",
        "farm_allocations = {\n",
        "    '1': 0.75,  # Large Urban\n",
        "    '2': 0.60,  # Medium Urban\n",
        "    '3': 0.40,  # Small Urban\n",
        "    '4': 0.10   # Rural\n",
        "}\n",
        "\n",
        "# Define non-farm household acreage allocation explicitly\n",
        "nonfarm_allocations = {\n",
        "    '1': {'under_05': 0.80, '05_to_5': 0.15, 'over_5': 0.05},\n",
        "    '2': {'under_05': 0.60, '05_to_5': 0.30, 'over_5': 0.10},\n",
        "    '3': {'under_05': 0.40, '05_to_5': 0.40, 'over_5': 0.20},\n",
        "    '4': {'under_05': 0.15, '05_to_5': 0.35, 'over_5': 0.50}\n",
        "}\n",
        "\n",
        "# Force correct data types explicitly\n",
        "df['POPCTRRACLASS_CTRPOPRRCLASSE'] = df['POPCTRRACLASS_CTRPOPRRCLASSE'].astype(str)\n",
        "numeric_cols = [\n",
        "    'household_count', 'HH_after_farms', 'Total_Ops',\n",
        "    'Ops_under_10_acres', 'Ops_5_acres_or_more'\n",
        "]\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "farm_allocations = {\n",
        "    '1': 0.75,  # Large Urban\n",
        "    '2': 0.60,  # Medium Urban\n",
        "    '3': 0.40,  # Small Urban\n",
        "    '4': 0.10   # Rural\n",
        "}\n",
        "\n",
        "nonfarm_allocations = {\n",
        "    '1': {'under_05': 0.80, '05_to_5': 0.15, 'over_5': 0.05},\n",
        "    '2': {'under_05': 0.60, '05_to_5': 0.30, 'over_5': 0.10},\n",
        "    '3': {'under_05': 0.40, '05_to_5': 0.40, 'over_5': 0.20},\n",
        "    '4': {'under_05': 0.15, '05_to_5': 0.35, 'over_5': 0.50}\n",
        "}\n",
        "\n",
        "def calculate_acreage(row):\n",
        "    urban_class = row['POPCTRRACLASS_CTRPOPRRCLASSE']\n",
        "\n",
        "    hh_total = row['household_count']\n",
        "    hh_after_farms = row['HH_after_farms']\n",
        "\n",
        "    farms_under_5 = row['Ops_under_10_acres']\n",
        "    farms_over_5 = row['Ops_5_acres_or_more']\n",
        "\n",
        "    if row['Total_Ops'] > 0:\n",
        "        # Farms present, allocate explicitly\n",
        "        farms_under_05 = farms_under_5 * farm_allocations.get(urban_class, 0)\n",
        "        farms_05_to_5 = farms_under_5 - farms_under_05\n",
        "\n",
        "        nonfarm_under_05 = hh_after_farms * nonfarm_allocations[urban_class]['under_05']\n",
        "        nonfarm_05_to_5 = hh_after_farms * nonfarm_allocations[urban_class]['05_to_5']\n",
        "        nonfarm_over_5 = hh_after_farms * nonfarm_allocations[urban_class]['over_5']\n",
        "\n",
        "        parcels_under_05 = farms_under_05 + nonfarm_under_05\n",
        "        parcels_05_to_5 = farms_05_to_5 + nonfarm_05_to_5\n",
        "        parcels_over_5 = farms_over_5 + nonfarm_over_5\n",
        "    else:\n",
        "        # No farm data, allocate clearly using households\n",
        "        parcels_under_05 = hh_total * nonfarm_allocations[urban_class]['under_05']\n",
        "        parcels_05_to_5 = hh_total * nonfarm_allocations[urban_class]['05_to_5']\n",
        "        parcels_over_5 = hh_total * nonfarm_allocations[urban_class]['over_5']\n",
        "\n",
        "    return pd.Series({\n",
        "        'parcels_under_0.5_acres': parcels_under_05,\n",
        "        'parcels_0.5_to_5_acres': parcels_05_to_5,\n",
        "        'parcels_over_5_acres': parcels_over_5\n",
        "    })\n",
        "\n",
        "# Apply calculation explicitly\n",
        "results_df = df.apply(calculate_acreage, axis=1)\n",
        "\n",
        "# Concatenate results explicitly and safely back into original dataframe\n",
        "df = pd.concat([df.reset_index(drop=True), results_df.reset_index(drop=True)], axis=1)\n",
        "df['PRUID'] = df['geo_code'].str[:2]\n",
        "df['CSDUID'] = df['geo_code'].str[2:]\n",
        "# Verify explicitly\n",
        "print(df[['geo_code', 'parcels_under_0.5_acres', 'parcels_0.5_to_5_acres', 'parcels_over_5_acres']].head())\n",
        "df.to_csv('ca_full_csd_land_owner_data_classed.csv', index=False)\n",
        "\n",
        "# Apply clearly to your DataFrame (assuming df is your dataset)\n",
        "# full_csd_data_classed[['parcels_under_0.5_acres', 'parcels_0.5_to_5_acres', 'parcels_over_5_acres']] = df.apply(calculate_acreage, axis=1)\n",
        "\n",
        "# # Quick verification clearly displayed\n",
        "# print(full_csd_data_classed[['geo_code', 'parcels_under_0.5_acres', 'parcels_0.5_to_5_acres', 'parcels_over_5_acres']].head())\n"
      ],
      "metadata": {
        "id": "ZyTuTaLRRxh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H8yAJ0CFWVWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ca_full_csd_land_owner_data_classed.csv')\n",
        "\n",
        "\n",
        "df_ca_land_ownership = df.copy()\n",
        "df_ca_land_ownership.head()\n",
        "\n",
        "df['geo_code'] = df['geo_code'].astype(str)\n",
        "df.head()\n",
        "# shape_dir = '/content/ca_csd_shapes/'\n",
        "# ca_csd_shape = fetch_unzip_load_shapefile_flexible('https://www12.statcan.gc.ca/census-recensement/2021/geo/sip-pis/boundary-limites/files-fichiers/lcsd000b21a_e.zip',shape_dir)\n",
        "ca_csd_shape = gpd.read_file('/content/ca_csd_simplified.geojson')\n",
        "ca_csd_shape.to_file('ca_csd_shape.geojson', driver='GeoJSON')\n",
        "ca_csd_shape.head()\n",
        "data_col = 'parcels_over_5_acres'\n",
        "ca_land_ownership = ca_csd_shape.merge(df[['geo_code', 'parcels_under_0.5_acres', 'parcels_0.5_to_5_acres', 'parcels_over_5_acres']], left_on='CSDUID', right_on='geo_code', how='left')\n",
        "ca_land_ownership.head()\n",
        "# ca_land_ownership.to_file('ca_land_ownership.geojson', driver='GeoJSON')\n",
        "\n",
        "title_txt = f'Audience Sizing for CA Land Owners under over 5 acres.'\n",
        "footer_txt = 'Data Sources: 2021 CA Ag Census, 2021 CA Census'\n",
        "vert_max = 1000\n",
        "# ca_land_ownership[data_col].max()\n",
        "file_stub = 'parcels_over_5_acres'\n",
        "plot_us_map(ca_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# ca_land_ownership[['PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "data_col = 'parcels_0.5_to_5_acres'\n",
        "title_txt = f'Audience Sizing for CA Land Owners over 0.5 to 5 acres.'\n",
        "footer_txt = 'Data Sources: 2021 CA Ag Census, 2021 CA Census'\n",
        "vert_max = 1000\n",
        "# ca_land_ownership[data_col].max()\n",
        "plot_us_map(ca_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# ca_land_ownership[['PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "data_col = 'parcels_under_0.5_acres'\n",
        "title_txt = f'Audience Sizing for CA Land Owners under 0.5.'\n",
        "footer_txt = 'Data Sources: 2021 CA Ag Census, 2021 CA Census'\n",
        "vert_max = 1000\n",
        "# ca_land_ownership[data_col].max()\n",
        "plot_us_map(ca_land_ownership, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "# ca_land_ownership[['PRENAME', 'PRUID',data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "ca_land_ownership.to_file('ca_land_ownership-simplified.geojson', driver='GeoJSON')\n"
      ],
      "metadata": {
        "id": "agEcFQMiTEL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.downloaded_tables"
      ],
      "metadata": {
        "id": "D5pX4it5fxWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyBBrPmrnShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Off roaders & overlanders"
      ],
      "metadata": {
        "id": "a2g7LdijpLll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "off_roader_keywords = [\n",
        "    \"Off-road trails\",\n",
        "    \"4x4 trails\",\n",
        "    \"Jeep trails\",\n",
        "    \"ATV trails near me\",\n",
        "    \"Mud bogging\",\n",
        "    \"Rock crawling\",\n",
        "    \"Off-road parks\",\n",
        "    \"Dirt trails\",\n",
        "    \"Off-road events\",\n",
        "    \"Off-road driving\",\n",
        "    \"Lift kits\",\n",
        "    \"Mud tires\",\n",
        "    \"Off-road bumpers\",\n",
        "    \"Off-road wheels\",\n",
        "    \"Winches for Jeeps\",\n",
        "    \"Rock sliders\",\n",
        "    \"Suspension lifts\",\n",
        "    \"Off-road lights\",\n",
        "    \"Snorkel kit\",\n",
        "    \"Differential lockers\",\n",
        "    \"Jeep Wrangler mods\",\n",
        "    \"Ford Bronco off-road\",\n",
        "    \"Toyota Tacoma off-road\",\n",
        "    \"ATV off-road gear\",\n",
        "    \"Side-by-side (UTV) accessories\",\n",
        "    \"Polaris Ranger accessories\",\n",
        "    \"Rubicon Trail\"\n",
        "]\n",
        "google_geos = fetch_google_ads_geo_targets(client)\n",
        "\n",
        "# google_geos = pd.read_csv('/content/geotargets-2025-04-01.csv')\n",
        "google_us_states = google_geos[((google_geos['country_code'] == 'US') & (google_geos['target_type'] == 'State'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_us_states = google_us_states[['name', 'id']].sort_values('name').reset_index(drop=True)\n",
        "google_us_states['name'] = google_us_states['name'].str.upper()\n",
        "# google_us_states.rename(columns={'Name': 'NAME', 'Criteria ID': 'geo_id'}, inplace=True)\n",
        "us_states = google_us_states.set_index('name')['id'].to_dict()\n",
        "google_ca_provinces = google_geos[((google_geos['country_code'] == 'CA') & (google_geos['target_type'] == 'Province'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_ca_provinces['name'] = google_ca_provinces['name'].str.upper()\n",
        "google_ca_provinces = google_ca_provinces[['name', 'id']].sort_values('name').reset_index(drop=True)\n",
        "ca_provinces = google_ca_provinces.set_index('name')['id'].to_dict()\n",
        "\n",
        "\n",
        "# df_estimates = get_keyword_estimates(client, keywords, us_states)\n",
        "\n",
        "overlander_keywords = [\n",
        "    \"Overlanding routes\",\n",
        "    \"Overlanding gear\",\n",
        "    \"Roof top tents\",\n",
        "    \"Expedition vehicles\",\n",
        "    \"Vehicle camping\",\n",
        "    \"Self-reliant travel\",\n",
        "    \"Off-grid vehicle setups\",\n",
        "    \"Long-distance off-road travel\",\n",
        "    \"Adventure travel vehicles\",\n",
        "    \"Overlanding trips\",\n",
        "    \"Roof racks for camping\",\n",
        "    \"Overlanding trailers\",\n",
        "    \"Vehicle fridge/freezers\",\n",
        "    \"Vehicle storage solutions\",\n",
        "    \"Portable power stations\",\n",
        "    \"Recovery gear overlanding\",\n",
        "    \"Vehicle awnings\",\n",
        "    \"Off-grid vehicle showers\",\n",
        "    \"Portable water filtration systems\",\n",
        "    \"Overlanding communication devices\",\n",
        "    \"Overland Expo\",\n",
        "    \"Overland Bound\",\n",
        "    \"Expedition Portal\",\n",
        "    \"Off-grid camping communities\",\n",
        "    \"EarthRoamer\",\n",
        "    \"Adventure vans\",\n",
        "    \"Land Rover Defender overlanding\",\n",
        "    \"Toyota Land Cruiser overlanding\"\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3AR5t28gkBOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "acs_variables = {\n",
        "    \"B01003_001E\": \"total_population\"\n",
        "}\n",
        "acs_pop_data = c.acs5.get(list(acs_variables.keys()), {'for': 'county:*'})\n",
        "\n",
        "df_acs_pop = pd.DataFrame(acs_pop_data).rename(columns=acs_variables)\n",
        "\n",
        "# Convert numeric columns\n",
        "for col in acs_variables.values():\n",
        "    df_acs_pop[col] = pd.to_numeric(df_acs_pop[col])\n",
        "\n",
        "# Create full FIPS code\n",
        "df_acs_pop['FIPS'] = df_acs_pop['state'] + df_acs_pop['county']\n",
        "df_acs_pop['total_population'] = df_acs_pop['total_population'].astype(int)\n",
        "\n",
        "df_us_state_pop = df_acs_pop[['state', 'total_population']].groupby('state').sum().sort_values(by='state').reset_index()\n",
        "df_us_state_pop['total_population'] = df_us_state_pop['total_population'].astype(int)\n",
        "import us\n",
        "\n",
        "# Assuming your dataframe is named 'us_pop_df' and has a column 'state' with abbreviations\n",
        "\n",
        "df_us_state_pop_clean = convert_state_abbrev_to_full(df_us_state_pop)\n",
        "df_us_state_pop_clean['geo_name'] = df_us_state_pop_clean['state_full'].str.upper()\n",
        "df_us_state_pop_clean['geo_code'] = 'us-' + df_us_state_pop_clean['state']\n",
        "df_us_state_pop_clean\n",
        "df_us_state_pop = df_us_state_pop_clean[['geo_code', 'geo_name', 'total_population']].copy()\n",
        "df_us_state_pop\n",
        "ca_pop_df = sc.table_to_df(\"9810000201\") # canadian population\n",
        "ca_pop_df['Population and dwelling counts (13): Private dwellings occupied by usual residents, 2021 [7]'] = ca_pop_df['Population and dwelling counts (13): Private dwellings occupied by usual residents, 2021 [7]'].fillna(0).astype(int)\n",
        "ca_province_pop_df = ca_pop_df[ca_pop_df['DGUID'].str.len() == 11].copy()\n",
        "ca_province_pop_df['geo_code'] = 'ca-' +  ca_province_pop_df['DGUID'].str[-2:]\n",
        "ca_province_pop_df['total_population'] = ca_province_pop_df['Population and dwelling counts (13): Private dwellings occupied by usual residents, 2021 [7]']\n",
        "ca_province_pop_df['geo_name'] = ca_province_pop_df['GEO'].str.upper()\n",
        "ca_province_pop_df = ca_province_pop_df[['geo_code','geo_name', 'total_population']].sort_values(by='geo_code').copy().reset_index(drop=True)\n",
        "ca_province_pop_df\n",
        "na_population_by_region = pd.concat([df_us_state_pop, ca_province_pop_df], ignore_index=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "FYJO3RJ_9KPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# us_small_search_trends = load_trends_from_search_api(us_under_250k_diy_keywords, search_api_io_key)\n",
        "kws = off_roader_keywords\n",
        "\n",
        "na_locales = {**us_states, **ca_provinces}\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, na_locales)\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].fillna(0).astype(int)\n",
        "paid_trends.head()\n",
        "# paid_trends['keyword'].drop_duplicates().sort_values()\n",
        "# paid_trends.to_csv('na_off_roader_paid_estimates.csv', index=False)\n",
        "\n",
        "\n",
        "off_roaders_audience_df = estimate_audience_distribution(paid_trends, na_population_by_region)\n",
        "off_roaders_audience_df.to_csv('na_off_roader_audience_estimates.csv', index=False)\n",
        "\n",
        "kws = overlander_keywords\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, na_locales)\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].fillna(0).astype(int)\n",
        "\n",
        "overlanders_audience_df = estimate_audience_distribution(paid_trends, na_population_by_region, total_audience=12000000)\n",
        "overlanders_audience_df.to_csv('na_overlanders_audience_estimates.csv', index=False)\n",
        "\n",
        "# us_small_search_trends = get_trends(kws, geo='US')\n",
        "us_small_search_trends = load_trends_from_search_api_parallel(kws, search_api_io_key)\n",
        "# get_trends\n",
        "\n",
        "\n",
        "ca_small_search_trends = load_trends_from_search_api(ca_under_250k_diy_keywords, search_api_io_key, geo='CA')\n",
        "ca_small_paid = get_keyword_estimates(client, ca_under_250k_diy_keywords, ca_provinces)\n",
        "\n",
        "us_small_search_trends_flat = us_small_search_trends.reset_index()\n",
        "us_small_op_trends_prepped = prep_trends_data(us_small_search_trends_flat, us_all_regions, neutral_factor=0.5)\n",
        "us_small_op_trends_prepped\n",
        "us_small_op_paid_prepped = prep_paid_search_data(us_small_paid, us_all_regions, neutral_factor=0.5)\n",
        "us_small_op_paid_prepped\n",
        "counts_col='Ops_below_250k'\n",
        "us_small_op_audiences = add_us_stats_and_geos(repositioned_us, us_small_op_trends_prepped, state_summary, us_small_op_paid_prepped, counts_col=counts_col)\n",
        "sus_mall_op_audiences = us_small_op_audiences.to_crs(epsg=5070)\n",
        "us_small_op_audiences.loc[us_small_op_audiences['NAME'] == 'PUERTO RICO']\n",
        "vert_max = us_small_op_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "data_col = 'adjusted_audience_Ops_below_250k'\n",
        "file_stub = 'us_diy_under250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations < $250k'\n",
        "footer_txt = 'Data Sources: USDA, Google Ads API, Google Trends (Data normalized individually then summed)'\n",
        "\n",
        "plot_us_map(us_small_op_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "us_small_op_audiences[['NAME', 'STATEFP',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n"
      ],
      "metadata": {
        "id": "thh5f_05pzrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scratchpad"
      ],
      "metadata": {
        "id": "2zxyLSHtKQE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "us_landowner_df = pd.read_csv('/content/us_land_ownership.csv')\n",
        "us_landowner_df['under_0.5_acres'].sum()\n",
        "us_landowner_df['0.5_to_5_acres'].sum()\n",
        "us_landowner_df['over_5_acres'].sum()\n",
        "us_landowner_df['under_0.5_acres'].sum() + us_landowner_df['0.5_to_5_acres'].sum() + us_landowner_df['over_5_acres'].sum()\n",
        "132000000 * .65\n",
        "# 29,644,595 under .5\n",
        "# 12,676,255 0.5 to 5\n",
        "# 334,649\n",
        "# 132,000,000\n",
        "# 85,800,000\n",
        "# 42,655,499"
      ],
      "metadata": {
        "id": "r0Q8OF3bKSeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "labor counts"
      ],
      "metadata": {
        "id": "V6saB-ImZmYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ.['BLS_API_KEY'] = userdata.get('us_bol_stats_api_key')"
      ],
      "metadata": {
        "id": "xxX5fxEcZ7sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOK3xYD2aX9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = userdata.get('us_bol_stats_api_key')\n",
        "API_KEY = '69c108d10be243598c9cbd5a31a3592d'"
      ],
      "metadata": {
        "id": "31oEfxOlatnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import us\n",
        "\n",
        "def correct_series_id(state_fips, naics, ownership='5'):\n",
        "    return f\"ENU{state_fips:02d}00000{ownership}{naics}\"\n",
        "\n",
        "series = []\n",
        "state_map = {}\n",
        "\n",
        "naics_codes = {\n",
        "    '561730': '5',  # Private\n",
        "    '811210': '5',  # Private\n",
        "    '811310': '5',  # Private\n",
        "    '921190': '0'   # Government (All ownership)\n",
        "}\n",
        "\n",
        "for state in us.states.STATES:\n",
        "    fips = int(state.fips)\n",
        "    state_map[f\"{fips:02d}\"] = state.name\n",
        "    for naics, ownership in naics_codes.items():\n",
        "        series_id = correct_series_id(fips, naics, ownership)\n",
        "        series.append(series_id)\n",
        "        print(f\"Validated Series ID: {series_id} for {state.name}\")"
      ],
      "metadata": {
        "id": "8cWj9nWSjn5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import us\n",
        "import pandas as pd\n",
        "\n",
        "API_URL = 'https://api.bls.gov/publicAPI/v2/timeseries/data/'\n",
        "\n",
        "naics_codes = {\n",
        "    '561730': '5',  # Private\n",
        "    '811210': '5',  # Private\n",
        "    '811310': '5',  # Private\n",
        "    '921190': '0'   # Government (All ownership)\n",
        "}\n",
        "\n",
        "# Helper function to build series ID\n",
        "def qcew_series_id(naics, state_fips):\n",
        "    return f'ENU{state_fips:02d}000' + '0' + f'{int(naics):06d}'\n",
        "\n",
        "def correct_series_id(state_fips, naics, ownership='5'):\n",
        "    return f\"ENU{state_fips:02d}00000{ownership}{naics}\"\n",
        "\n",
        "\n",
        "\n",
        "naics_codes = ['561730', '811210', '811310', '921190']\n",
        "\n",
        "series = []\n",
        "state_map = {}\n",
        "\n",
        "for state in us.states.STATES:\n",
        "    fips = int(state.fips)\n",
        "    state_map[f\"{fips:02d}\"] = state.name\n",
        "    for naics in naics_codes:\n",
        "        ownership = '5' if naics != '921190' else '0'  # Government support is usually public (ownership=0)\n",
        "        series_id = f\"ENU{fips:02d}0000{ownership}{naics}\"\n",
        "        series.append(series_id)\n",
        "        print(f\"Generated Correct Series ID: {series_id} for {state.name}\")\n",
        "\n",
        "print(\"Requesting data from BLS...\")\n",
        "response = requests.post(API_URL, json={\n",
        "    \"seriesid\": series[:50],  # BLS API allows up to 50 series per request\n",
        "    \"startyear\": \"2023\",\n",
        "    \"endyear\": \"2023\",\n",
        "    \"registrationkey\": API_KEY\n",
        "})\n",
        "\n",
        "results = response.json()\n",
        "\n",
        "response = requests.post(API_URL, json={\n",
        "    \"seriesid\": ['ENU48000561730'],  # BLS API allows up to 50 series per request\n",
        "    \"startyear\": \"2023\",\n",
        "    \"endyear\": \"2023\",\n",
        "    \"registrationkey\": API_KEY\n",
        "})\n",
        "\n",
        "results = response.json()\n",
        "results\n",
        "\n",
        "\n",
        "# Inspect response thoroughly\n",
        "print(results)\n",
        "\n",
        "# Parsing Results if data is returned\n",
        "records = []\n",
        "for s in results.get('Results', {}).get('series', []):\n",
        "    series_id = s['seriesID']\n",
        "    state_fips = series_id[3:5]\n",
        "    naics_code = series_id[-6:]\n",
        "\n",
        "    for data_point in s.get('data', []):\n",
        "        records.append({\n",
        "            'State': state_map.get(state_fips, 'Unknown'),\n",
        "            'NAICS': naics_code,\n",
        "            'Year': data_point['year'],\n",
        "            'Employment': data_point['value']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "OIGSTzFlLGAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import prettytable\n",
        "headers = {'Content-type': 'application/json'}\n",
        "data = json.dumps({\"seriesid\": ['CUUR0000SA0','SUUR0000SA0'],\"startyear\":\"2022\", \"endyear\":\"2023\"})\n",
        "p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
        "json_data = json.loads(p.text)\n",
        "json_data\n",
        "for series in json_data['Results']['series']:\n",
        "    x=prettytable.PrettyTable([\"series id\",\"year\",\"period\",\"value\",\"footnotes\"])\n",
        "    seriesId = series['seriesID']\n",
        "    for item in series['data']:\n",
        "        year = item['year']\n",
        "        period = item['period']\n",
        "        value = item['value']\n",
        "        footnotes=\"\"\n",
        "        for footnote in item['footnotes']:\n",
        "            if footnote:\n",
        "                footnotes = footnotes + footnote['text'] + ','\n",
        "        if 'M01' <= period <= 'M12':\n",
        "            x.add_row([seriesId,year,period,value,footnotes[0:-1]])\n",
        "    output = open(seriesId + '.txt','w')\n",
        "    output.write (x.get_string())\n",
        "    output.close()\n",
        "\n",
        "dol_df = pd.read_csv('/content/CUUR0000SA0.txt', sep=\"\\t\")\n",
        "dol_df.head()\n"
      ],
      "metadata": {
        "id": "hjNB07MUndKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import us\n",
        "\n",
        "def fetch_qcew_employment_all_states(year, qtr, industry_codes):\n",
        "    all_states_data = []\n",
        "\n",
        "    for state in us.states.STATES:\n",
        "        area_fips = state.fips + '000'\n",
        "        url = f\"http://data.bls.gov/cew/data/api/{year}/{qtr.lower()}/area/{area_fips}.csv\"\n",
        "\n",
        "        try:\n",
        "            with urllib.request.urlopen(url) as response:\n",
        "                csv_data = response.read().decode('utf-8')\n",
        "\n",
        "            df = pd.read_csv(StringIO(csv_data))\n",
        "            df_filtered = df[df['industry_code'].isin(industry_codes)]\n",
        "\n",
        "            df_filtered['state'] = state.name\n",
        "            all_states_data.append(df_filtered[[\n",
        "                'state', 'area_fips', 'own_code', 'industry_code', 'agglvl_code',\n",
        "                'size_code', 'year', 'qtr', 'annual_avg_estabs', 'annual_avg_emplvl'\n",
        "            ]])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch data for {state.name}: {e}\")\n",
        "\n",
        "    return pd.concat(all_states_data, ignore_index=True)\n",
        "\n",
        "# Example usage:\n",
        "industry_codes = [\"561730\", \"811210\", \"811310\", \"921190\"]\n",
        "all_states_employment = fetch_qcew_employment_all_states(\"2024\", \"a\", industry_codes)\n",
        "print(all_states_employment.head(20))"
      ],
      "metadata": {
        "id": "gHg2j-CJpDwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fixing ca lots"
      ],
      "metadata": {
        "id": "pKKbZO21kw_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_land = pd.read_csv('/content/ca_full_csd_land_owner_data_classed.csv')\n",
        "\n",
        "ca_land_unique = ca_land.drop_duplicates(subset=['DGUID'])\n",
        "ca_land_unique.columns.to_list()\n",
        "ca_land_summary = (\n",
        "    ca_land_unique\n",
        "    .groupby('PRUID', as_index=False)\n",
        "    .agg({\n",
        "        'parcels_under_0.5_acres': 'sum',\n",
        "        'parcels_0.5_to_5_acres': 'sum',\n",
        "        'parcels_over_5_acres': 'sum'\n",
        "    })\n",
        ")\n",
        "ca_land_summary['PRUID'] = ca_land_summary['PRUID'].astype(str)\n",
        "ca_land_summary['PRUID'] = 'ca-' + ca_land_summary['PRUID']\n",
        "\n",
        "ca_land_summary = pd.merge(ca_land_summary,ca_province_pop_df[['geo_code', 'geo_name']], left_on='PRUID', right_on='geo_code', how='left')\n",
        "ca_land_summary.to_csv('ca_land_ownership_updated.csv', index=False)"
      ],
      "metadata": {
        "id": "bQclR9TTkwyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_states_employment.loc[all_states_employment['state'] == 'Colorado']"
      ],
      "metadata": {
        "id": "p0Aw5d56uVb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "landscaping_operations = all_states_employment.loc[all_states_employment['industry_code'] == '561730']\n",
        "print(landscaping_operations['annual_avg_estabs'].sum())\n",
        "landscaping_operations_summary = landscaping_operations.groupby('state').agg({'annual_avg_estabs': 'sum', 'annual_avg_emplvl': 'sum'}).reset_index()\n",
        "landscaping_operations_summary.to_csv('us_landscaping_operations_audience_summary.csv', index=False)\n",
        "\n",
        "government_support = all_states_employment.loc[all_states_employment['industry_code'] == '921190']\n",
        "print(government_support['annual_avg_emplvl'].sum())\n",
        "government_support_summary = government_support.groupby('state').agg({'annual_avg_estabs': 'sum', 'annual_avg_emplvl': 'sum'}).reset_index()\n",
        "government_support_summary.to_csv('us_government_support_audience_summary.csv', index=False)\n",
        "diesel_and_precision_repair = all_states_employment.loc[all_states_employment['industry_code'].isin(['811210','811310']) ]\n",
        "print(diesel_and_precision_repair['annual_avg_emplvl'].sum())\n",
        "diesel_and_precision_repair_summary = diesel_and_precision_repair.groupby('state').agg({'annual_avg_estabs': 'sum', 'annual_avg_emplvl': 'sum'}).reset_index()\n",
        "diesel_and_precision_repair_summary.to_csv('us_diesel_and_precision_repair_audience_summary.csv', index=False)\n",
        "\n",
        "# landscaping 123772 902787\n",
        "# government support 436704 - 8800 establishments\n",
        "# diesel and precision repair 345420"
      ],
      "metadata": {
        "id": "4EvPw1oSp7v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# canada\n",
        "ca_biz_df = sc.table_to_df(\"3310080601\") # canadian employment 9810000201 14-10-0320-02 1410022301\n",
        "ca_biz_df['PRUID'] = ca_biz_df['DGUID'].str[-2:]\n",
        "ca_biz_df['geo_name'] = ca_biz_df['GEO'].str.upper()\n",
        "ca_provinces = ca_biz_df[['PRUID', 'geo_name']].drop_duplicates().sort_values('PRUID').reset_index(drop=True)\n",
        "ca_provinces.to_csv('ca_provinces.csv', index=False)\n",
        "ca_biz_df['North American Industry Classification System (NAICS)'].drop_duplicates().sort_values()\n",
        "ca_biz_df.loc[ca_biz_df['North American Industry Classification System (NAICS)'].str.contains('561730')]\n",
        "ca_province_landscapers = ca_biz_df.loc[(ca_biz_df['North American Industry Classification System (NAICS)'].str.contains('561730')) & (ca_biz_df['DGUID'].str.len() == 11) & (ca_biz_df['Employment size'] == 'Total, with employees')].copy().reset_index(drop=True)\n",
        "ca_province_landscapers['VALUE_NUMERIC'] = ca_province_landscapers['VALUE'].fillna(0.0).round(0).astype(int)\n",
        "# ca_province_landscapers['PRUID'] = ca_province_landscapers['DGUID'].str[-2:]\n",
        "\n",
        "ca_province_landscapers = ca_province_landscapers.rename(columns={'VALUE_NUMERIC': 'annual_avg_estabs', 'PRUID' : 'geo_code'})\n",
        "columns = ['geo_code', 'geo_name', 'annual_avg_estabs']\n",
        "ca_province_landscapers = ca_province_landscapers[columns]\n",
        "ca_province_landscapers.to_csv('ca_landscaping_operations_audience_summary.csv', index=False)\n",
        "#\n",
        "ca_province_procurement = ca_biz_df.loc[(ca_biz_df['North American Industry Classification System (NAICS)'].str.contains('921190')) & (ca_biz_df['DGUID'].str.len() == 11) & (ca_biz_df['Employment size'] == 'Total, with employees')].copy().reset_index(drop=True)\n",
        "\n",
        "ca_province_diesel = ca_biz_df.loc[((ca_biz_df['North American Industry Classification System (NAICS)'].str.contains('811210'))| (ca_biz_df['North American Industry Classification System (NAICS)'].str.contains('811310'))) & (ca_biz_df['DGUID'].str.len() == 11) & (ca_biz_df['Employment size'] == 'Total, with employees')].copy().reset_index(drop=True)\n",
        "\n",
        "# https://www150.statcan.gc.ca/n1/pub/71m0001x/2021001/2025-05-CSV.zip\n",
        "try2 = pd.read_csv('/content/9810045201_databaseLoadingData (1).csv')\n",
        "try2.columns\n",
        "# ca_emp_df = sc.table_to_df(\"98-10-0452-01\") # canadian employment 9810000201 14-10-0320-02 1410022301 14-10-0023-01\n",
        "ca_emp_df.head()\n",
        "ca_emp_df.loc[ca_emp_df['National Occupational Classification (NOC)'].str.contains('8255')]\n",
        "ca_emp_df['National Occupational Classification (NOC)'].drop_duplicates().sort_values()"
      ],
      "metadata": {
        "id": "tvOh4sUTqZKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your microdata (assuming CSV or similar)\n",
        "lfs_df = try2\n",
        "\n",
        "# Filter employed in NOC 8255\n",
        "noc_8255_employed = lfs_df[\n",
        "    (lfs_df['NOC_43'] == '8255')\n",
        "]\n",
        "\n",
        "# Group by province and sum weights to get employment counts\n",
        "employment_counts = (\n",
        "    noc_8255_employed\n",
        "    .groupby('PROV')['FINALWT']\n",
        "    .sum()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(employment_counts)"
      ],
      "metadata": {
        "id": "Zt50B0CyhvFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working on consumer equipment diyers - \"Where's Barry?\""
      ],
      "metadata": {
        "id": "LPiXRk_DeocJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consumer_lawn_garden_keywords = [\n",
        "    # Lawn Mower & Lawn Tractor Maintenance\n",
        "    \"lawn mower repair\",\n",
        "    \"lawn tractor maintenance\",\n",
        "    \"riding mower troubleshooting\",\n",
        "    \"lawn tractor belt replacement\",\n",
        "    \"lawn mower blades replacement\",\n",
        "    \"lawn tractor battery replacement\",\n",
        "    \"riding mower tire change\",\n",
        "    \"garden tractor oil change\",\n",
        "    \"lawn tractor engine maintenance\",\n",
        "    \"mower deck leveling\",\n",
        "\n",
        "    # Compact & Garden Tractor Repairs\n",
        "    \"garden tractor attachments\",\n",
        "    \"small tractor DIY repair\",\n",
        "    \"garden tractor electrical issues\",\n",
        "    \"compact tractor engine troubleshooting\",\n",
        "    \"garden tractor carburetor cleaning\",\n",
        "    \"tractor spark plug replacement\",\n",
        "    \"tractor air filter replacement\",\n",
        "    \"garden tractor fuel system repair\",\n",
        "    \"tractor starter replacement\",\n",
        "    \"garden tractor transmission repair\",\n",
        "\n",
        "    # Utility Vehicles (\"Gator\" type vehicles)\n",
        "    \"utility vehicle DIY repair\",\n",
        "    \"Gator oil change\",\n",
        "    \"side by side maintenance\",\n",
        "    \"utility vehicle tire replacement\",\n",
        "    \"UTV battery issues\",\n",
        "    \"Gator transmission troubleshooting\",\n",
        "    \"side by side brake repair\",\n",
        "    \"UTV electrical repair\",\n",
        "    \"utility vehicle accessory installation\",\n",
        "    \"side by side drive belt replacement\",\n",
        "\n",
        "    # Parts & Manuals (Online shopping intent)\n",
        "    \"buy lawn mower parts online\",\n",
        "    \"garden tractor parts online\",\n",
        "    \"utility vehicle accessories online\",\n",
        "    \"John Deere Gator parts online\",\n",
        "    \"riding mower replacement parts\",\n",
        "    \"tractor service manuals online\",\n",
        "    \"UTV repair manuals\",\n",
        "    \"garden tractor aftermarket parts\",\n",
        "    \"lawn tractor oil change kit\",\n",
        "    \"utility vehicle service kits\",\n",
        "\n",
        "    # Seasonal & General DIY\n",
        "    \"winterize garden tractor\",\n",
        "    \"utility vehicle winter preparation\",\n",
        "    \"lawn tractor storage prep\",\n",
        "    \"tractor seasonal maintenance\",\n",
        "    \"Gator winter maintenance\",\n",
        "    \"garden tractor DIY videos\",\n",
        "    \"small engine DIY repairs\",\n",
        "    \"lawn equipment troubleshooting guide\",\n",
        "    \"utility vehicle maintenance schedule\",\n",
        "    \"lawn mower seasonal upkeep\",\n",
        "]\n",
        "\n",
        "\n",
        "df_us_state_pop\n",
        "ca_province_pop_df\n",
        "# kws = us_high_revenue_keywords\n",
        "kws = consumer_lawn_garden_keywords\n",
        "\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, us_states_google)\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].astype(int)\n",
        "paid_trends['avg_monthly_searches'].sum()\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "us_consumer_repair = paid_trends\n",
        "us_consumer_repair['avg_monthly_searches'].sum()\n",
        "\n",
        "paid_trends = get_exact_keyword_volumes(client, kws, ca_provinces_google)\n",
        "paid_trends['avg_monthly_searches'] = paid_trends['avg_monthly_searches'].astype(int)\n",
        "paid_trends['avg_monthly_searches'].sum()\n",
        "paid_trends['geo_name'] = paid_trends['state']\n",
        "ca_consumer_repair = paid_trends\n",
        "ca_consumer_repair['avg_monthly_searches'].sum()\n",
        "\n",
        "\n",
        "us_consumer_diy_spi_index = estimate_search_population_indexes(us_consumer_repair, df_us_state_pop, population_col='adults_18_to_64')\n",
        "us_consumer_diy_repair_audience = pd.merge(df_us_state_pop, us_consumer_diy_spi_index[['geo_code','SPI', 'normalized_SPI'] ], on='geo_code', suffixes=('', '_spi'))\n",
        "us_consumer_diy_repair_audience['modeled_audience_size'] = us_consumer_diy_repair_audience['adults_18_to_64'] * us_consumer_diy_repair_audience['normalized_SPI']\n",
        "us_consumer_diy_repair_audience['modeled_audience_size'] = us_consumer_diy_repair_audience['modeled_audience_size'].astype(int)\n",
        "us_consumer_diy_repair_audience.to_csv('us_consumer_repair_audience_estimates.csv', index=False)\n",
        "# 47350.0 3,444,187\n",
        "ca_consumer_diy_spi_index = estimate_search_population_indexes(ca_consumer_repair, ca_province_pop_df, population_col='adults_18_to_64')\n",
        "ca_consumer_diy_repair_audience = pd.merge(ca_province_pop_df, ca_consumer_diy_spi_index[['geo_code','SPI', 'normalized_SPI'] ], on='geo_code', suffixes=('', '_spi'))\n",
        "ca_consumer_diy_repair_audience['modeled_audience_size'] = ca_consumer_diy_repair_audience['adults_18_to_64'] * us_consumer_diy_repair_audience['normalized_SPI']\n",
        "ca_consumer_diy_repair_audience['modeled_audience_size'] = ca_consumer_diy_repair_audience['modeled_audience_size'].astype(int)\n",
        "ca_consumer_diy_repair_audience.to_csv('ca_consumer_repair_audience_estimates.csv', index=False)\n",
        "ca_consumer_diy_repair_audience['modeled_audience_size'].sum()\n"
      ],
      "metadata": {
        "id": "rhy3tyKAzn4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k1Bp4NDhpOKW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}