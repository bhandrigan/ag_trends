{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNqMjGApATwvq05U41MzWp8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhandrigan/ag_trends/blob/main/ag_trends_us_states.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends google-ads scrapingbee  shapely>=2.0.0 -q"
      ],
      "metadata": {
        "id": "tiIKBDK609ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_LoX0e81uFmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load libraries and functions\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import numpy as np\n",
        "import requests\n",
        "import gzip\n",
        "from io import BytesIO\n",
        "from pytrends.request import TrendReq\n",
        "from google.ads.googleads.client import GoogleAdsClient\n",
        "from google.api_core.exceptions import ResourceExhausted\n",
        "from requests.exceptions import RequestException\n",
        "from requests.exceptions import ReadTimeout\n",
        "import time\n",
        "import os\n",
        "import zipfile\n",
        "from urllib.parse import urlparse\n",
        "import tempfile\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import zipfile\n",
        "import re\n",
        "import random\n",
        "from scrapingbee import ScrapingBeeClient\n",
        "import shapely\n",
        "from shapely.geometry import shape\n",
        "import multiprocessing as mp\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scrapingbee_api_key = userdata.get('SCRAPING_BEE')\n",
        "gads_sa = json.loads(userdata.get('N90_GADS_SA_ACCOUNT_JSON'))\n",
        "gads_api_key = userdata.get('N90_GADS_API_KEY')\n",
        "sa_account = tempfile.NamedTemporaryFile(delete=False)\n",
        "sa_account.write(json.dumps(gads_sa).encode())\n",
        "sa_account.close()\n",
        "sa_account_path = sa_account.name\n",
        "\n",
        "gads_account = \"8417741864\"\n",
        "use_proto_plus = True\n",
        "impersonated_email = \"api@n90.co\"\n",
        "\n",
        "\n",
        "# create google-ads.yaml file text\n",
        "yaml_content = f\"\"\"\n",
        "developer_token: {gads_api_key}\n",
        "use_proto_plus: {use_proto_plus}\n",
        "json_key_file_path: {sa_account_path}\n",
        "impersonated_email: {impersonated_email}\n",
        "login_customer_id: {gads_account}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize the client with the dictionary configuration (hypothetical)\n",
        "client = GoogleAdsClient.load_from_string(yaml_content)\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    return ''.join(c for c in text if not unicodedata.combining(c)).upper()\n",
        "\n",
        "def fetch_unzip_load_shapefile_flexible(url, output_dir):\n",
        "    \"\"\"\n",
        "    Fetches a zip file containing shapefiles from a URL,\n",
        "    unzips it if needed, and loads the first .shp file found\n",
        "    in the output directory into a GeoDataFrame.\n",
        "\n",
        "    Removes query string variables from the local filename.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the zip file containing the shapefiles.\n",
        "        output_dir (str): The directory where the zip file will be downloaded\n",
        "                          and unzipped.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: The loaded shapefile as a GeoDataFrame, or None if\n",
        "                          download, unzipping, or loading fails, or if no\n",
        "                          .shp files are found.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Parse the URL to get the path component\n",
        "    parsed_url = urlparse(url)\n",
        "    # Get the base filename from the URL path and remove query string\n",
        "    zip_filename = os.path.basename(parsed_url.path)\n",
        "    zip_file_path = os.path.join(output_dir, zip_filename)\n",
        "\n",
        "    # Check if the zip file already exists (indicates previous download/unzip)\n",
        "    if os.path.exists(zip_file_path):\n",
        "        print(f\"Zip file '{zip_filename}' already exists in '{output_dir}'. Skipping download.\")\n",
        "    else:\n",
        "        # If the zip file doesn't exist, attempt to download\n",
        "        print(f\"Zip file '{zip_filename}' not found. Attempting to download from {url}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True, verify=False)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "\n",
        "            # Download the zip file\n",
        "            with open(zip_file_path, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"Zip file downloaded to '{zip_file_path}'.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading file: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Check if .shp files already exist in the output directory\n",
        "    shp_files = [f for f in os.listdir(output_dir) if f.endswith('.shp')]\n",
        "    if shp_files:\n",
        "        print(f\"Shapefile(s) found in '{output_dir}'. Loading the first one...\")\n",
        "        shapefile_name_to_load = shp_files[0]  # Load the first .shp file found\n",
        "        shape_file_path = os.path.join(output_dir, shapefile_name_to_load)\n",
        "        try:\n",
        "            gdf = gpd.read_file(shape_file_path)\n",
        "            print(f\"Shapefile '{shapefile_name_to_load}' loaded successfully.\")\n",
        "            return gdf\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading shapefile '{shapefile_name_to_load}': {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        # If no .shp files are found, attempt to unzip\n",
        "        print(f\"No .shp files found in '{output_dir}'. Attempting to unzip '{zip_filename}'...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(output_dir)\n",
        "            print(f\"Zip file extracted to '{output_dir}'.\")\n",
        "\n",
        "            # After unzipping, look for .shp files again\n",
        "            shp_files_after_unzip = [f for f in os.listdir(output_dir) if f.endswith('.shp')]\n",
        "            if shp_files_after_unzip:\n",
        "                print(f\"Shapefile(s) found after unzipping. Loading the first one...\")\n",
        "                shapefile_name_to_load = shp_files_after_unzip[0]\n",
        "                shape_file_path = os.path.join(output_dir, shapefile_name_to_load)\n",
        "                try:\n",
        "                    gdf = gpd.read_file(shape_file_path)\n",
        "                    print(f\"Shapefile '{shapefile_name_to_load}' loaded successfully.\")\n",
        "                    return gdf\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading shapefile '{shapefile_name_to_load}': {e}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(f\"No .shp files found in '{output_dir}' after unzipping.\")\n",
        "                return None\n",
        "\n",
        "        except zipfile.BadZipFile as e:\n",
        "            print(f\"Error unzipping file (Bad zip file): {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during unzipping or loading: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "# Build request to fetch keyword plan metrics by location\n",
        "def get_keyword_estimates(client, keywords, geo_ids):\n",
        "    keyword_plan_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for state, geo_id in geo_ids.items():\n",
        "        request = {\n",
        "            \"customer_id\": client.login_customer_id,\n",
        "            \"language\": \"languageConstants/1000\",  # English\n",
        "            \"geo_target_constants\": [f\"geoTargetConstants/{geo_id}\"],\n",
        "            \"keyword_plan_network\": client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH_AND_PARTNERS,\n",
        "            \"keyword_seed\": {\"keywords\": keywords},\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = keyword_plan_service.generate_keyword_ideas(request=request)\n",
        "\n",
        "            for result in response:\n",
        "                results.append({\n",
        "                    'state': state,\n",
        "                    'keyword': result.text,\n",
        "                    'avg_monthly_searches': result.keyword_idea_metrics.avg_monthly_searches,\n",
        "                    'competition': result.keyword_idea_metrics.competition.name,\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error in state {state} with geo ID {geo_id}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_repair_keyword_ideas(client, seed_keywords, geo_id='2840', language_id='1000', suggestions_per_keyword=10, retries=5):\n",
        "    \"\"\"\n",
        "    Fetch keyword ideas per seed keyword, salvaging results if API limit reached.\n",
        "\n",
        "    Args:\n",
        "        client: GoogleAdsClient instance.\n",
        "        seed_keywords (list): Initial seed keywords.\n",
        "        geo_id (str): Geo target ID ('2840' for US).\n",
        "        language_id (str): Language ID ('1000' for English).\n",
        "        suggestions_per_keyword (int): Top suggestions per seed keyword.\n",
        "        retries (int): Retries on API rate-limit errors.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Collected keyword suggestions and metrics.\n",
        "    \"\"\"\n",
        "    keyword_plan_service = client.get_service(\"KeywordPlanIdeaService\")\n",
        "    results = []\n",
        "\n",
        "    for seed_keyword in seed_keywords:\n",
        "        request = {\n",
        "            \"customer_id\": client.login_customer_id,\n",
        "            \"language\": f\"languageConstants/{language_id}\",\n",
        "            \"geo_target_constants\": [f\"geoTargetConstants/{geo_id}\"],\n",
        "            \"keyword_plan_network\": client.enums.KeywordPlanNetworkEnum.GOOGLE_SEARCH_AND_PARTNERS,\n",
        "            \"keyword_seed\": {\"keywords\": [seed_keyword]},\n",
        "            \"page_size\": suggestions_per_keyword\n",
        "        }\n",
        "\n",
        "        attempt = 0\n",
        "        wait_time = 5  # Initial backoff in seconds\n",
        "\n",
        "        while attempt <= retries:\n",
        "            try:\n",
        "                response = keyword_plan_service.generate_keyword_ideas(request=request)\n",
        "\n",
        "                for result in response:\n",
        "                    results.append({\n",
        "                        'seed_keyword': seed_keyword,\n",
        "                        'suggested_keyword': result.text,\n",
        "                        'avg_monthly_searches': result.keyword_idea_metrics.avg_monthly_searches,\n",
        "                        'competition': result.keyword_idea_metrics.competition.name,\n",
        "                    })\n",
        "                # Successfully retrieved results; break retry loop\n",
        "                break\n",
        "\n",
        "            except ResourceExhausted:\n",
        "                print(f\"Rate limit hit fetching '{seed_keyword}'. Attempt {attempt + 1}/{retries}. Retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                wait_time *= 2  # Exponential backoff\n",
        "                attempt += 1\n",
        "\n",
        "        if attempt > retries:\n",
        "            print(f\"Exceeded max retries for '{seed_keyword}'. Moving to next keyword.\")\n",
        "\n",
        "    # Return all successfully collected results even if some requests failed\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def prep_paid_search_data(df, geo_col='state'):\n",
        "    df = df.groupby(geo_col).sum('avg_monthly_searches')\n",
        "    df = df.reset_index()\n",
        "    df['state'] = df['state'].str.upper()\n",
        "    total_searches = df['avg_monthly_searches'].sum()\n",
        "    df['paid_search_composite_factor_100'] = (df['avg_monthly_searches'] / total_searches) * 100\n",
        "    return df\n",
        "\n",
        "def get_trends_via_scrapingbee(\n",
        "        keyword_list, scrapingbee_api_key, language='en-US', tz=360, geo='US',\n",
        "        resolution='REGION', timeframe='today 12-m', inc_low_vol=True,\n",
        "        max_retries=4, initial_wait=10, timeout=(10, 30)):  # increased timeouts clearly\n",
        "\n",
        "    proxy_url = f\"http://{scrapingbee_api_key}:@proxy.scrapingbee.com:8886\"\n",
        "    proxies = [proxy_url]\n",
        "    dfs = []\n",
        "\n",
        "    for kw in keyword_list:\n",
        "        retries = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retries <= max_retries:\n",
        "            try:\n",
        "                pytrends = TrendReq(\n",
        "                    hl=language,\n",
        "                    tz=tz,\n",
        "                    proxies=proxies,\n",
        "                    timeout=timeout,\n",
        "                    requests_args={'verify': False}\n",
        "                )\n",
        "\n",
        "                pytrends.build_payload([kw], geo=geo, timeframe=timeframe)\n",
        "                df_kw = pytrends.interest_by_region(resolution=resolution, inc_low_vol=inc_low_vol)\n",
        "                df_kw.rename(columns={kw: kw.replace(' ', '_')}, inplace=True)\n",
        "                dfs.append(df_kw)\n",
        "                print(f\"[Success] '{kw}' retrieved successfully.\")\n",
        "                break\n",
        "\n",
        "            except (RequestException, ReadTimeout) as e:\n",
        "                error_str = str(e).lower()\n",
        "                if '429' in error_str or 'too many requests' in error_str or 'timeout' in error_str:\n",
        "                    print(f\"[Retryable Error] '{kw}' - Retrying in {wait_time}s (attempt {retries+1}/{max_retries}): {e}\")\n",
        "                    jitter = random.uniform(2, 5)\n",
        "                    time.sleep(wait_time + jitter)\n",
        "                    wait_time *= 2\n",
        "                    retries += 1\n",
        "                else:\n",
        "                    print(f\"[Critical Error] '{kw}' - Non-retryable error: {e}\")\n",
        "                    raise\n",
        "\n",
        "        if retries > max_retries:\n",
        "            print(f\"[Skipped] Exceeded maximum retries for keyword '{kw}'.\")\n",
        "\n",
        "        sleep_between_keywords = random.uniform(5, 10)\n",
        "        print(f\"Waiting {sleep_between_keywords:.1f}s before next keyword...\")\n",
        "        time.sleep(sleep_between_keywords)\n",
        "\n",
        "    if dfs:\n",
        "        df_combo = pd.concat(dfs, axis=1)\n",
        "        print(\"Data successfully retrieved for some/all keywords.\")\n",
        "    else:\n",
        "        df_combo = pd.DataFrame()\n",
        "        print(\"No data retrieved for keywords after retries.\")\n",
        "\n",
        "    return df_combo\n",
        "\n",
        "def get_trends(keyword_list, language='en-US', tz=360, geo='US', resolution='REGION',\n",
        "               timeframe='today 12-m', inc_low_vol=True, max_retries=5, initial_wait=10):\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for kw in keyword_list:\n",
        "        retries = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retries <= max_retries:\n",
        "            try:\n",
        "                # Create pytrends instance without internal retries\n",
        "                pytrends = TrendReq(hl=language, tz=tz)\n",
        "\n",
        "                pytrends.build_payload([kw], geo=geo, timeframe=timeframe)\n",
        "                df_kw = pytrends.interest_by_region(resolution=resolution, inc_low_vol=inc_low_vol)\n",
        "                df_kw.rename(columns={kw: kw.replace(' ', '_')}, inplace=True)\n",
        "                dfs.append(df_kw)\n",
        "                print(f\"Successfully retrieved data for keyword '{kw}'\")\n",
        "                break  # Exit loop on success\n",
        "\n",
        "            except RequestException as e:\n",
        "                error_str = str(e).lower()\n",
        "                if '429' in error_str or 'too many requests' in error_str:\n",
        "                    print(f\"429 error on keyword '{kw}' - retrying in {wait_time}s \"\n",
        "                          f\"(attempt {retries+1}/{max_retries})...\")\n",
        "                    jitter = random.uniform(1, 5)\n",
        "                    time.sleep(wait_time + jitter)\n",
        "                    wait_time *= 2\n",
        "                    retries += 1\n",
        "                else:\n",
        "                    print(f\"Non-retryable error: {e}\")\n",
        "                    raise\n",
        "\n",
        "        if retries > max_retries:\n",
        "            print(f\"Exceeded maximum retries for keyword '{kw}'. Skipping keyword.\")\n",
        "\n",
        "        sleep_duration = random.uniform(5, 10)\n",
        "        print(f\"Pausing for {sleep_duration:.1f}s before next keyword...\")\n",
        "        time.sleep(sleep_duration)\n",
        "\n",
        "    if dfs:\n",
        "        df_combo = pd.concat(dfs, axis=1)\n",
        "    else:\n",
        "        df_combo = pd.DataFrame()\n",
        "        print(\"Warning: No keyword data successfully retrieved.\")\n",
        "\n",
        "    return df_combo\n",
        "\n",
        "def prep_trends_data(df):\n",
        "    # df['st_composite_sum'] = 0\n",
        "    df['st_composite_sum'] = (df.sum(axis=1))\n",
        "\n",
        "    df['st_composite_factor_100'] = ((\n",
        "        (df['st_composite_sum'] ) /\n",
        "        (df['st_composite_sum'].sum() )\n",
        "    ) * 100)\n",
        "    df.loc[df['st_composite_factor_100'] > 5, 'st_composite_factor_100'] = 5\n",
        "    df['st_composite_factor_100'] = df['st_composite_factor_100'].fillna(0)\n",
        "    df = df.reset_index()\n",
        "    # df.rename(columns={'geoName': 'region'}, inplace=True)\n",
        "    df['region'] = df['region'].str.upper()\n",
        "    return df\n",
        "\n",
        "def add_us_stats_and_geos(gdf, df_trends, df_stats, paid_df, gdf_geo_col='NAME', trends_geo_col='region', stats_geo_col='STATE_NAME', paid_geo_col='state', counts_col=None):\n",
        "    if counts_col is None:\n",
        "        return \"Error: counts_col must be provided.\"\n",
        "    gdf[gdf_geo_col] = gdf[gdf_geo_col].apply(normalize_text)\n",
        "    df_trends[trends_geo_col] = df_trends[trends_geo_col].apply(normalize_text)\n",
        "    df_stats[stats_geo_col] = df_stats[stats_geo_col].apply(normalize_text)\n",
        "    paid_df[paid_geo_col] = paid_df[paid_geo_col].apply(normalize_text)\n",
        "    merged_df = gdf.merge(df_trends, left_on=gdf_geo_col, right_on=trends_geo_col, how='left')\n",
        "    merged_df = pd.merge(merged_df, df_stats, left_on=gdf_geo_col, right_on=stats_geo_col, how='left', suffixes=('', '_extra'))\n",
        "    for col in merged_df.columns:\n",
        "        if col.endswith('_extra'):\n",
        "            merged_df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    merged_df['Ops_below_250k'] = merged_df['Ops_below_250k'].fillna(0).round(0).astype(int)\n",
        "    merged_df['Ops_250k_or_more'] = merged_df['Ops_250k_or_more'].fillna(0).round(0).astype(int)\n",
        "    merged_df['Total_Ops'] = merged_df['Total_Ops'].fillna(0).round(0).astype(int)\n",
        "    comp_col_label = f\"{counts_col}_composite_factor_100\"\n",
        "    merged_df[comp_col_label] = (merged_df[counts_col] / merged_df[counts_col].sum()) * 100\n",
        "    # merged_df['Ops_below_250k_composite_factor_100'] = (merged_df['Ops_below_250k'] / merged_df['Ops_250k_or_more'].sum()) * 100\n",
        "    merged_df2 = merged_df.merge(paid_df, left_on=gdf_geo_col, right_on=paid_geo_col, how='left').copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "    # merged_df2 = merged_df2.loc[~merged_df2[paid_geo_col].isna()].copy().sort_values(gdf_geo_col).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    mean_trends = merged_df2['st_composite_factor_100'].fillna(0).mean()\n",
        "    mean_volume = merged_df2['paid_search_composite_factor_100'].fillna(0).mean()\n",
        "\n",
        "    # Clearly calculate relative positions\n",
        "    merged_df2['trends_relative'] = merged_df2['st_composite_factor_100'] / mean_trends\n",
        "    merged_df2['volume_relative'] = merged_df2['paid_search_composite_factor_100'] / mean_volume\n",
        "\n",
        "    # Clearly combine both into single adjustment factor\n",
        "    merged_df2['combined_relative_factor'] = (merged_df2['trends_relative'] + merged_df2['volume_relative']) / 2\n",
        "\n",
        "    # Adjusted audience clearly calculated\n",
        "    audience_label = f\"adjusted_audience_{counts_col}\"\n",
        "    merged_df2[audience_label] = (merged_df2[counts_col] * merged_df2['combined_relative_factor']).fillna(0).round(0).astype(int)\n",
        "    return merged_df2\n",
        "\n",
        "def reposition_alaska_hawaii(gdf):\n",
        "    # Separate states\n",
        "    contiguous_us = gdf[~gdf['NAME'].isin(['ALASKA', 'HAWAII'])]\n",
        "    alaska = gdf[gdf['NAME'] == 'ALASKA'].copy()\n",
        "    hawaii = gdf[gdf['NAME'] == 'HAWAII'].copy()\n",
        "    # Adjusted scale and translate for Alaska and Hawaii\n",
        "    alaska.geometry = alaska.scale(xfact=0.4, yfact=0.4, origin='center').translate(xoff=1500000, yoff=-4400000)\n",
        "    hawaii.geometry = hawaii.scale(xfact=0.7, yfact=0.7, origin='center').translate(xoff=5200000, yoff=-1700000)\n",
        "    # Combine adjusted geometries\n",
        "    repositioned_us = pd.concat([contiguous_us, alaska, hawaii])\n",
        "    # and remove all other islands like puerto rico\n",
        "    repositioned_us = repositioned_us[~(repositioned_us['STATEFP'] >= '60')].sort_values('NAME').reset_index(drop=True)\n",
        "    return repositioned_us\n",
        "\n",
        "def plot_us_map(gdf, _density_cmap, _column, _legend_kwds, _title, _footer, _vmax=50000, projection=None):\n",
        "    if projection is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        # ax.set_aspect('equal')\n",
        "    else:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5), subplot_kw={'projection': projection})\n",
        "    # fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    gdf.plot(ax=ax, column=_column, cmap=density_cmap, legend=True, vmin=0, vmax=_vmax,\n",
        "                legend_kwds=_legend_kwds)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(_title, fontsize=14, pad=20, horizontalalignment= 'center') # Increased padding\n",
        "    fig.subplots_adjust(bottom=0.1) # Adjust the bottom margin\n",
        "\n",
        "    # Add footer\n",
        "    fig.text(0.5, 0.01, _footer, ha='center', fontsize=10)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def fetch_google_ads_geo_targets(client, country_codes=['US', 'CA']):\n",
        "    ga_service = client.get_service(\"GoogleAdsService\")\n",
        "\n",
        "    country_codes_formatted = ', '.join(f\"'{code}'\" for code in country_codes)\n",
        "\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            geo_target_constant.resource_name,\n",
        "            geo_target_constant.id,\n",
        "            geo_target_constant.name,\n",
        "            geo_target_constant.country_code,\n",
        "            geo_target_constant.target_type\n",
        "        FROM geo_target_constant\n",
        "        WHERE geo_target_constant.country_code IN ({country_codes_formatted})\n",
        "        AND geo_target_constant.target_type IN ('State', 'Province', 'Country')\n",
        "    \"\"\"\n",
        "\n",
        "    response = ga_service.search_stream(customer_id=client.login_customer_id, query=query)\n",
        "\n",
        "    geo_targets = []\n",
        "    for batch in response:\n",
        "        for row in batch.results:\n",
        "            geo_targets.append({\n",
        "                'resource_name': row.geo_target_constant.resource_name,\n",
        "                'id': row.geo_target_constant.id,\n",
        "                'name': row.geo_target_constant.name,\n",
        "                'country_code': row.geo_target_constant.country_code,\n",
        "                'target_type': row.geo_target_constant.target_type,\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(geo_targets)\n",
        "\n",
        "def load_or_download_csv(local_filename, data_url):\n",
        "    \"\"\"\n",
        "    Loads a CSV file locally or downloads it if not available locally.\n",
        "    Handles CSV files compressed in ZIP, GZIP, or uncompressed formats.\n",
        "\n",
        "    Args:\n",
        "        local_filename (str): Path to the local CSV file.\n",
        "        data_url (str): URL to download the CSV from if not available locally.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataframe from CSV.\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_filename):\n",
        "        try:\n",
        "            df = pd.read_csv(local_filename)\n",
        "            print(f\"Loaded data successfully from {local_filename}.\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading local file, will attempt download: {e}\")\n",
        "\n",
        "    print(f\"Downloading data from {data_url}...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    content_type = response.headers.get('Content-Type', '').lower()\n",
        "\n",
        "    # Handle ZIP compressed files\n",
        "    if 'zip' in content_type or data_url.lower().endswith('.zip'):\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            csv_files = [name for name in z.namelist() if name.endswith('.csv')]\n",
        "            if not csv_files:\n",
        "                raise ValueError(\"No CSV file found in ZIP archive.\")\n",
        "            with z.open(csv_files[0]) as f:\n",
        "                df = pd.read_csv(f)\n",
        "\n",
        "    # Handle GZIP compressed files\n",
        "    elif 'gzip' in content_type or data_url.lower().endswith('.gz'):\n",
        "        with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "    # Handle regular CSV files\n",
        "    else:\n",
        "        df = pd.read_csv(BytesIO(response.content))\n",
        "\n",
        "    # Save locally\n",
        "    df.to_csv(local_filename, index=False)\n",
        "    print(f\"Data downloaded and saved locally to {local_filename}.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Clearly define Canadian sales categories explicitly\n",
        "below_250k_categories_canada = [\n",
        "    '$0',\n",
        "    '$1 to $9,999',\n",
        "    '$10,000 to $24,999',\n",
        "    '$25,000 to $49,999',\n",
        "    '$50,000 to $99,999',\n",
        "    '$100,000 to $249,999'\n",
        "]\n",
        "\n",
        "above_250k_categories_canada = [\n",
        "    '$250,000 to $499,999',\n",
        "    '$500,000 to $999,999',\n",
        "    '$1,000,000 to $1,999,999',\n",
        "    '$2,000,000 and over'\n",
        "]\n",
        "\n",
        "# Function to categorize revenues explicitly\n",
        "def categorize_revenues(df, geo_level, geo_col, country_geo_val, above_250k_cats, below_250k_cats, country, pivot_col='Total farm revenues distribution'):\n",
        "    if country == 'CA':\n",
        "        df_filtered = df.loc[df[geo_col] != country_geo_val] if geo_level != 'national' else df.loc[df[geo_col] == country_geo_val]\n",
        "        df_filtered = df[(df[geo_col] != country_geo_val) & ~(df[geo_col].str.contains(','))] if geo_level == 'provincial' else df_filtered\n",
        "        # Example assuming your dataframe is df:\n",
        "        summary_df = df_filtered.pivot_table(\n",
        "            index=geo_col,\n",
        "            columns=pivot_col,\n",
        "            values='VALUE',\n",
        "            aggfunc='sum',\n",
        "            fill_value=0\n",
        "        ).reset_index()\n",
        "\n",
        "        # Explicit aggregation for below and above $250k\n",
        "        summary_df['Ops_below_250k'] = summary_df[below_250k_cats].sum(axis=1)\n",
        "        summary_df['Ops_250k_or_more'] = summary_df[above_250k_cats].sum(axis=1)\n",
        "        summary_df['geo_code'] = summary_df[geo_col].apply(lambda x: re.findall(r'\\[(.*?)\\]', x)[0])\n",
        "        summary_df['geo_name'] = summary_df[geo_col].apply(lambda x: x.split('[')[0].strip()).str.upper()\n",
        "\n",
        "        summary_df['Total_Ops'] = summary_df['Ops_below_250k'] + summary_df['Ops_250k_or_more']\n",
        "        summary_df['PRUID'] = (\n",
        "            summary_df['geo_code']\n",
        "            .str.replace('PR', '', regex=False)  # clearly remove 'PR' if present\n",
        "            .str[:2]                             # clearly take the first two characters\n",
        "            .astype(int)                         # clearly convert to integer\n",
        "        )\n",
        "        final_columns = ['PRUID','geo_name', 'geo_code', 'Ops_below_250k', 'Ops_250k_or_more', 'Total_Ops']\n",
        "        summary_df = summary_df[final_columns]\n",
        "        summary_df.columns = final_columns\n",
        "        summary_df = summary_df.copy().sort_values('PRUID').reset_index(drop=True)\n",
        "        return summary_df[final_columns].reset_index(drop=True)\n",
        "\n",
        "    else:\n",
        "        print('not configured for US yet')\n",
        "\n",
        "def load_or_download_csv(local_filename, data_url):\n",
        "    \"\"\"\n",
        "    Loads a CSV file locally or downloads it if not available locally.\n",
        "    Handles CSV files compressed in ZIP, GZIP, or uncompressed formats.\n",
        "\n",
        "    Args:\n",
        "        local_filename (str): Path to the local CSV file.\n",
        "        data_url (str): URL to download the CSV from if not available locally.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Loaded dataframe from CSV.\n",
        "    \"\"\"\n",
        "    if os.path.exists(local_filename):\n",
        "        try:\n",
        "            df = pd.read_csv(local_filename)\n",
        "            print(f\"Loaded data successfully from {local_filename}.\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading local file, will attempt download: {e}\")\n",
        "\n",
        "    print(f\"Downloading data from {data_url}...\")\n",
        "    response = requests.get(data_url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    content_type = response.headers.get('Content-Type', '').lower()\n",
        "\n",
        "    # Handle ZIP compressed files\n",
        "    if 'zip' in content_type or data_url.lower().endswith('.zip'):\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            csv_files = [name for name in z.namelist() if name.endswith('.csv')]\n",
        "            if not csv_files:\n",
        "                raise ValueError(\"No CSV file found in ZIP archive.\")\n",
        "            with z.open(csv_files[0]) as f:\n",
        "                df = pd.read_csv(f)\n",
        "\n",
        "    # Handle GZIP compressed files\n",
        "    elif 'gzip' in content_type or data_url.lower().endswith('.gz'):\n",
        "        with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "            df = pd.read_csv(f)\n",
        "\n",
        "    # Handle regular CSV files\n",
        "    else:\n",
        "        df = pd.read_csv(BytesIO(response.content))\n",
        "\n",
        "    # Save locally\n",
        "    df.to_csv(local_filename, index=False)\n",
        "    print(f\"Data downloaded and saved locally to {local_filename}.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def simplify_geom(geom, tolerance=0.1):\n",
        "    return geom.simplify(tolerance, preserve_topology=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "vA6dmsLE7kif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load US and Canada State / Province shapefiles\n",
        "\n",
        "# Remove GDAL's GeoJSON size limit explicitly\n",
        "os.environ['OGR_GEOJSON_MAX_OBJ_SIZE'] = '0'\n",
        "\n",
        "try:\n",
        "    gdf_us_states = gpd.read_file('simplified_us_state_geos.gpkg')\n",
        "except Exception as e:\n",
        "    print(\"File not found. Attempting to fetch and load shapefile.\")\n",
        "\n",
        "    geo_address = 'https://www2.census.gov/geo/tiger/TIGER2023/STATE/tl_2023_us_state.zip'\n",
        "\n",
        "    gdf_us_states = (fetch_unzip_load_shapefile_flexible(geo_address, 'state_tiger').to_crs('EPSG:5070'))\n",
        "# gdf_us_states = gdf_us_states.simplify(\n",
        "#         tolerance=0.05,  # Adjust this if needed; higher = more simplified\n",
        "#         preserve_topology=True\n",
        "#     )\n",
        "# gdf_us_states = gdf_us_states.simplify()\n",
        "\n",
        "# https://www2.census.gov/geo/tiger/TIGER2024/ESTATE/tl_2024_78_estate.zip\n",
        "\n",
        "# Load geographic shapes for US/Canada\n",
        "# gdf_us_states = gpd.read_file('/content/us_state/tl_2024_us_state.shp').to_crs('EPSG:5070')\n",
        "    gdf_us_states_ref = gdf_us_states[['STATEFP', 'NAME']].drop_duplicates().sort_values('STATEFP').reset_index(drop=True)\n",
        "    gdf_us_states_ref['NAME'] = gdf_us_states_ref['NAME'].str.upper()\n",
        "    gdf_us_states['NAME'] = gdf_us_states['NAME'].str.upper()\n",
        "\n",
        "# save this ref to a gcs bucket\n",
        "    gdf_us_states_ref.to_csv('us_state_ref.csv', index=False)\n",
        "    with mp.Pool(mp.cpu_count()) as pool:\n",
        "        simplified_geometries = pool.map(simplify_geom, gdf_us_states.geometry)\n",
        "    gdf_us_states['geometry'] = simplified_geometries\n",
        "    gdf_us_states.to_file('simplified_us_state_geos.gpkg', driver='GPKG')\n",
        "\n",
        "\n",
        "\n",
        "repositioned_us = reposition_alaska_hawaii(gdf_us_states).sort_values('STATEFP').reset_index(drop=True)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "\n",
        "try:\n",
        "    canada_provinces_gdf = gpd.read_file('simplified_canada_province_geos.gpkg')\n",
        "except Exception as e:\n",
        "    print(\"File not found. Attempting to fetch and load shapefile.\")\n",
        "    url = \"https://www12.statcan.gc.ca/census-recensement/2021/geo/sip-pis/boundary-limites/files-fichiers/lpr_000b21a_e.zip?st=wk4IrLBG\"\n",
        "    output_directory = '/content/province-shapes/'\n",
        "    canada_provinces_gdf = fetch_unzip_load_shapefile_flexible(url, output_directory)\n",
        "# canada_provinces_gdf = canada_provinces_gdf.simplify(\n",
        "#         tolerance=0.05,  # Adjust this if needed; higher = more simplified\n",
        "#         preserve_topology=True\n",
        "#     )\n",
        "\n",
        "    if canada_provinces_gdf is not None:\n",
        "        canada_provinces_gdf['PRENAME'] = canada_provinces_gdf['PRENAME'].str.upper()\n",
        "        canada_provinces_gdf = canada_provinces_gdf.to_crs('EPSG:5070')\n",
        "        print(\"\\nGeoDataFrame loaded:\")\n",
        "        print(canada_provinces_gdf.head())\n",
        "        # # Adjust the tolerance to balance simplification and detail retention\n",
        "        # simplification_tolerance = 0.05  # Example tolerance (increase to simplify more aggressively)\n",
        "        # # Simplify geometries explicitly\n",
        "        # canada_provinces_gdf['geometry'] = canada_provinces_gdf['geometry'].simplify(\n",
        "        #     tolerance=simplification_tolerance, preserve_topology=True\n",
        "        # )\n",
        "        # # Ensure geometries remain valid\n",
        "        # canada_provinces_gdf = canada_provinces_gdf[canada_provinces_gdf.is_valid]\n",
        "        with mp.Pool(mp.cpu_count()) as pool:\n",
        "            simplified_geometries = pool.map(simplify_geom, canada_provinces_gdf.geometry)\n",
        "\n",
        "        canada_provinces_gdf['geometry'] = simplified_geometries\n",
        "        canada_provinces_gdf = canada_provinces_gdf.copy().sort_values('PRUID').reset_index(drop=True)\n",
        "        canada_provinces_gdf.to_file('simplified_canada_province_geos.gpkg', driver='GPKG')\n",
        "    else:\n",
        "        print(\"No valid GeoDataFrame loaded.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J1ahh-Ctt2UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "params = {\n",
        "  \"engine\": \"google_trends\",\n",
        "  \"q\": \"John Deere repair\",\n",
        "  \"data_type\": \"GEO_MAP\",\n",
        "  \"tz\": 360,\n",
        "  \"geo\": \"US\",\n",
        "  \"resolution\": \"REGION\",\n",
        "  \"timeframe\": \"today 12-m\",\n",
        "  \"inc_low_vol\": True,\n",
        "  \"region\": \"REGION\",\n",
        "  \"api_key\": \"iHtMbpxNLN8j18mG5LyMHL8D\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, params=params)\n",
        "# print(response.text)"
      ],
      "metadata": {
        "id": "yiZ-cGQ6IBKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def safe_extract(x, item='value'):\n",
        "    try:\n",
        "        val = x[0][item]\n",
        "        return val.strip() if isinstance(val, str) else val\n",
        "    except (IndexError, KeyError, TypeError, AttributeError):\n",
        "        return None\n",
        "\n",
        "search_api_io_key = userdata.get('SEARCH_API_IO_KEY')\n",
        "def load_trends_from_search_api(kw_list, api_key, geo='US'):\n",
        "    dfs = []\n",
        "    kw_list = kw_list if isinstance(kw_list, list) else [kw_list]\n",
        "\n",
        "    for kw in kw_list:\n",
        "        params = {\n",
        "            \"engine\": \"google_trends\",\n",
        "            \"q\": kw,\n",
        "            \"data_type\": \"GEO_MAP\",\n",
        "            \"tz\": 360,\n",
        "            \"geo\": geo,\n",
        "            \"resolution\": \"REGION\",\n",
        "            \"timeframe\": \"today 12-m\",\n",
        "            \"inc_low_vol\": True,\n",
        "            \"region\": \"REGION\",\n",
        "            \"api_key\": api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "        interest_raw = response.json().get('interest_by_region', [])\n",
        "\n",
        "        df = pd.DataFrame(interest_raw)\n",
        "        if df.empty:\n",
        "            print(f\"No data found for keyword: {kw}\")\n",
        "            continue\n",
        "        df['region'] = df['name'].str.strip().str.upper()\n",
        "\n",
        "        val_col_name = f\"{kw.replace(' ', '_').lower()}\"\n",
        "        df[val_col_name] = df['values'].apply(safe_extract, item='extracted_value')\n",
        "        df[val_col_name] = df[val_col_name].fillna(0).astype(int)\n",
        "\n",
        "        dfs.append(df[['region', val_col_name]])\n",
        "\n",
        "    # Concatenate and explicitly aggregate by region\n",
        "    combo_df = pd.concat(dfs)\n",
        "    combo_df = combo_df.groupby('region').sum()\n",
        "\n",
        "    return combo_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0h7xsJusa8WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load USDA and Canada Agriculture census data\n",
        "# URL to USDA Census of Agriculture 2022 data (corrected link)\n",
        "data_url = 'https://www.nass.usda.gov/datasets/qs.census2022.txt.gz'\n",
        "\n",
        "# Download the GZIP file\n",
        "local_filename = 'us_agriculture.csv'\n",
        "try:\n",
        "    df = pd.read_csv(local_filename)\n",
        "    print(\"Data loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading data: - downloading\", e)\n",
        "    response = requests.get(data_url)\n",
        "    response.raise_for_status()\n",
        "    # Decompress and load the file into a pandas DataFrame\n",
        "    with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "        df = pd.read_csv(f, delimiter='\\t')  # adjust delimiter if necessary\n",
        "        df.to_csv(local_filename, index=False)\n",
        "df_us = df.copy()\n",
        "\n",
        "data_url = base_url = \"https://www150.statcan.gc.ca/n1/en/tbl/csv/32100239-eng.zip?st=wk4IrLBG\"\n",
        "local_filename = 'ca_agriculture.csv'\n",
        "df_canada = load_or_download_csv(local_filename, data_url)\n",
        "# df_canada = df_canada.rename(columns={'Total farm revenues distribution': 'revenue_distribution'})\n",
        "# df_canada['Total farm revenues distribution'].unique()\n",
        "\n",
        "\n",
        "geo_level='provincial'\n",
        "provincial_summary_canada = categorize_revenues(df_canada, geo_level=geo_level, geo_col='GEO', country_geo_val='Canada [000000000]', above_250k_cats=above_250k_categories_canada, below_250k_cats=below_250k_categories_canada, country='CA')\n",
        "\n",
        "print('/nn')\n",
        "print(provincial_summary_canada.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "aIfNLctg_JXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rkGf195ROs_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your service account data loaded into a dictionary\n",
        "# ca_tractor_census = 'https://www150.statcan.gc.ca/t1/tbl1/en/dtl!downloadDbLoadingData-nonTraduit.action?pid=3210022901&latestN=5&startDate=&endDate=&csvLocale=en&selectedMembers=%5B%5B1%5D%2C%5B%5D%5D&checkedLevels=1D1'\n",
        "\n"
      ],
      "metadata": {
        "id": "93xqZZstAaOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# us_diy_over_250k config\n",
        "us_over_250k_keywords = ['John Deere parts', 'John Deere repair', 'John Deere tractor parts', 'John Deere service manual', 'john deere fuel pump replacement', 'john deere part']\n",
        "\n",
        "google_geos = fetch_google_ads_geo_targets(client)\n",
        "\n",
        "# google_geos = pd.read_csv('/content/geotargets-2025-04-01.csv')\n",
        "google_us_states = google_geos[((google_geos['country_code'] == 'US') & (google_geos['target_type'] == 'State'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_us_states = google_us_states[['name', 'id']].sort_values('name').reset_index(drop=True)\n",
        "google_us_states['name'] = google_us_states['name'].str.upper()\n",
        "# google_us_states.rename(columns={'Name': 'NAME', 'Criteria ID': 'geo_id'}, inplace=True)\n",
        "us_states = google_us_states.set_index('name')['id'].to_dict()\n",
        "google_ca_provinces = google_geos[((google_geos['country_code'] == 'CA') & (google_geos['target_type'] == 'Province'))].copy().sort_values('name').reset_index(drop=True)\n",
        "google_ca_provinces['name'] = google_ca_provinces['name'].str.upper()\n",
        "google_ca_provinces = google_ca_provinces[['name', 'id']].sort_values('name').reset_index(drop=True)\n",
        "ca_provinces = google_ca_provinces.set_index('name')['id'].to_dict()\n",
        "\n",
        "\n",
        "# df_estimates = get_keyword_estimates(client, keywords, us_states)\n",
        "ca_over_250k_keywords = [\n",
        "    'John Deere parts',\n",
        "    'John Deere repair',\n",
        "    'John Deere tractor parts',\n",
        "    'John Deere service manual',\n",
        "    'John Deere fuel pump replacement',\n",
        "    'John Deere combine parts',  # Important for grain-growing prairies\n",
        "    'John Deere baler parts',    # Hay operations prevalent in dairy/livestock regions\n",
        "    'John Deere planter parts',  # Common in row-crop regions (e.g., corn, soybeans)\n",
        "    'John Deere sprayer repair',  # Important across grain and specialty crop operations\n",
        "    'pièces John Deere',\n",
        "    'réparation John Deere',\n",
        "    'manuel de service John Deere',\n",
        "    'pièces tracteur John Deere',\n",
        "    'pièces moissonneuse-batteuse John Deere'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlmUeR-EFYXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bBNfv9hbKu0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKLarufPrgS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYK8CgwU0wme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# county_df = df.dropna(subset=['COUNTY_CODE', 'STATE_FIPS_CODE']).copy().reset_index(drop=True)\n",
        "# county_df['COUNTY_FIPS'] = county_df['STATE_FIPS_CODE'].astype(str).str.zfill(2) + \\\n",
        "#                            county_df['COUNTY_CODE'].astype(int).astype(str).str.zfill(3)\n",
        "operator_field_to_use = 'COMMODITY TOTALS - OPERATIONS WITH SALES'"
      ],
      "metadata": {
        "id": "7oar2YqXr5qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize usda farm operations data\n",
        "state_df = None\n",
        "county_df = None\n",
        "national_df = None\n",
        "state_summary = None\n",
        "county_summary = None\n",
        "national_summary = None\n",
        "\n",
        "below_250k_categories = [\n",
        "    'FARM SALES: (LESS THAN 1,000 $)',\n",
        "    'FARM SALES: (1,000 TO 2,499 $)',\n",
        "    'FARM SALES: (2,500 TO 4,999 $)',\n",
        "    'FARM SALES: (5,000 TO 9,999 $)',\n",
        "    'FARM SALES: (10,000 TO 24,999 $)',\n",
        "    'FARM SALES: (25,000 TO 49,999 $)',\n",
        "    'FARM SALES: (50,000 TO 99,999 $)',\n",
        "    'FARM SALES: (100,000 TO 249,999 $)'\n",
        "]\n",
        "\n",
        "above_250k_categories = [\n",
        "    'FARM SALES: (250,000 TO 499,999 $)',\n",
        "    'FARM SALES: (500,000 TO 999,999 $)',\n",
        "    'FARM SALES: (1,000,000 OR MORE $)'\n",
        "]\n",
        "\n",
        "state_df = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'STATE') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "\n",
        "state_df['VALUE_NUMERIC'] = pd.to_numeric(state_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "state_df.columns.to_list()\n",
        "# Aggregate clearly by state\n",
        "state_df['STATEFP'] = state_df['STATE_FIPS_CODE'].astype(int).astype(str).str.zfill(2)\n",
        "state_summary = state_df.groupby(['STATEFP','STATE_NAME']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "\n",
        "# Add total clearly\n",
        "state_summary['Total_Ops'] = state_summary['Ops_below_250k'] + state_summary['Ops_250k_or_more']\n",
        "\n",
        "print(state_summary.head(10))\n",
        "county_df = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'COUNTY') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "county_df['VALUE_NUMERIC'] = pd.to_numeric(county_df['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "# Construct County FIPS explicitly\n",
        "try:\n",
        "    county_df['FIPS'] = county_df['STATE_FIPS_CODE'].astype(str).str.zfill(2) + \\\n",
        "                        county_df['COUNTY_CODE'].astype(int).astype(str).str.zfill(3)\n",
        "except ValueError:\n",
        "    print(ValueError)\n",
        "\n",
        "county_summary = county_df.groupby(['STATE_NAME', 'COUNTY_NAME', 'FIPS']).apply(lambda x: pd.Series({\n",
        "    'Ops_below_250k': x[x['DOMAINCAT_DESC'].isin(below_250k_categories)]['VALUE_NUMERIC'].sum(),\n",
        "    'Ops_250k_or_more': x[x['DOMAINCAT_DESC'].isin(above_250k_categories)]['VALUE_NUMERIC'].sum()\n",
        "}), include_groups=False).reset_index()\n",
        "\n",
        "# Calculate total explicitly\n",
        "county_summary['Total_Ops'] = county_summary['Ops_below_250k'] + county_summary['Ops_250k_or_more']\n",
        "\n",
        "print(county_summary.head(10))\n",
        "national_summary = df_us[\n",
        "    (df_us['SHORT_DESC'] == operator_field_to_use) &\n",
        "    (df_us['AGG_LEVEL_DESC'] == 'NATIONAL') &\n",
        "    (df_us['YEAR'] == 2022)\n",
        "].copy()\n",
        "\n",
        "national_summary['VALUE_NUMERIC'] = pd.to_numeric(national_summary['VALUE'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "national_below_250k = national_summary[\n",
        "    national_summary['DOMAINCAT_DESC'].isin(below_250k_categories)\n",
        "]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "national_above_250k = national_summary[\n",
        "    national_summary['DOMAINCAT_DESC'].isin(above_250k_categories)\n",
        "]['VALUE_NUMERIC'].sum()\n",
        "\n",
        "total_national_ops = national_below_250k + national_above_250k\n",
        "\n",
        "print(f\"National Operations < $250k: {int(national_below_250k):,}\")\n",
        "print(f\"National Operations ≥ $250k: {int(national_above_250k):,}\")\n",
        "print(f\"National Total Operations: {int(total_national_ops):,}\")"
      ],
      "metadata": {
        "id": "ueO_ZKLIsWQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8lgKGi6teIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsO22s6-Xqgk"
      },
      "outputs": [],
      "source": [
        "us_large_search_trends = load_trends_from_search_api(us_over_250k_keywords, search_api_io_key)\n",
        "\n",
        "us_large_paid = get_keyword_estimates(client, us_over_250k_keywords, us_states)\n",
        "ca_search_trends = load_trends_from_search_api(ca_over_250k_keywords, search_api_io_key, geo='CA')\n",
        "ca_large_paid = get_keyword_estimates(client, ca_over_250k_keywords, ca_provinces)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "density_colors = ['#E0EDF7', '#5E96AE', '#237D83', '#1A4599', '#F36D32']\n",
        "\n",
        "density_cmap = LinearSegmentedColormap.from_list('audience', density_colors)\n",
        "data_col='adjusted_audience_Ops_250k_or_more'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations > $250k'\n",
        "footer_txt = 'Data Sources: USDA, Google Ads API, Google Trends (Data normalized individually then summed)'\n",
        "\n",
        "\n",
        "# us_large_search_trends = get_trends_via_scrapingbee(us_over_250k_keywords, scrapingbee_api_key)\n",
        "\n",
        "us_large_trends_prepped = prep_trends_data(us_large_search_trends)\n",
        "\n",
        "\n",
        "us_large_paid_prepped = prep_paid_search_data(us_large_paid)\n",
        "\n",
        "counts_col='Ops_250k_or_more'\n",
        "us_large_audiences = add_us_stats_and_geos(repositioned_us, us_large_trends_prepped, state_summary, us_large_paid_prepped, counts_col=counts_col)\n",
        "us_large_audiences = us_large_audiences.to_crs(epsg=5070)\n",
        "vert_max = us_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "data_col = 'adjusted_audience_Ops_250k_or_more'\n",
        "file_stub = 'us_diy_over250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations >= $250k'\n",
        "footer_txt = 'Data Sources: 2022 USDA Ag Census, Google Ads API, Google Search Trends'\n",
        "\n",
        "plot_us_map(us_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "\n",
        "us_large_audiences[['NAME', 'STATEFP',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "# ca_search_trends['geoName'] = ca_search_trends['geoName'].apply(normalize_text)\n",
        "ca_large_trends_prepped = prep_trends_data(ca_search_trends)\n",
        "# ca_large_trends_prepped['region'] = ca_large_trends_prepped['region'].apply(normalize_text)\n",
        "ca_large_paid_prepped = prep_paid_search_data(ca_large_paid)\n",
        "\n",
        "ca_large_paid_prepped\n",
        "counts_col='Ops_250k_or_more'\n",
        "\n",
        "ca_large_audiences = add_us_stats_and_geos(canada_provinces_gdf, ca_large_trends_prepped, provincial_summary_canada, ca_large_paid_prepped, counts_col=counts_col, gdf_geo_col='PRENAME',\n",
        "                                           stats_geo_col='geo_name').copy()\n",
        "\n",
        "canada_provinces_gdf.columns\n",
        "ca_large_audiences = ca_large_audiences.to_crs(epsg=5070)\n",
        "vert_max = ca_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "data_col = 'adjusted_audience_Ops_250k_or_more'\n",
        "file_stub = 'ca_diy_over250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: CA Farm Operations >= $250k'\n",
        "footer_txt = 'Data Sources: 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "\n",
        "plot_us_map(ca_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "ca_large_audiences[['PRENAME', 'PRUID',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "na_large_audiences = pd.concat([us_large_audiences[['adjusted_audience_Ops_250k_or_more', 'geometry']], ca_large_audiences[['adjusted_audience_Ops_250k_or_more', 'geometry']]])\n",
        "vert_max = na_large_audiences['adjusted_audience_Ops_250k_or_more'].max()\n",
        "ile_stub = 'na_diy_over250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: NA Farm Operations >= $250k'\n",
        "footer_txt = 'Data Sources: 2022 USDA Ag Census, 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "plot_us_map(na_large_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)"
      ],
      "metadata": {
        "id": "CHtsxsAtX5ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  start on small operators\n",
        "us_under_250k_diy_keywords = [\n",
        "    'John Deere mower repair',\n",
        "    'John Deere lawn tractor parts',\n",
        "    'John Deere mower maintenance',\n",
        "    'small tractor repair services',\n",
        "    'small tractor troubleshooting',\n",
        "    'john deere parts'\n",
        "]\n",
        "ca_under_250k_diy_keywords = [\n",
        "    'John Deere mower repair',\n",
        "    'John Deere lawn tractor parts',\n",
        "    'John Deere mower maintenance',\n",
        "    'small tractor repair services',\n",
        "    'small tractor troubleshooting',\n",
        "    'John Deere parts',\n",
        "    'John Deere snow blower parts',       # Explicitly relevant due to snow conditions\n",
        "    'John Deere snow blower repair',      # Frequent seasonal repairs\n",
        "    'garden tractor snow blade',          # Attachments specifically useful in Canada\n",
        "    'compact tractor winter maintenance',  # Maintenance specific to colder climates\n",
        "    'réparation tondeuse John Deere',\n",
        "    'pièces tracteur pelouse John Deere'\n",
        "]\n",
        "us_small_search_trends = load_trends_from_search_api(us_under_250k_diy_keywords, search_api_io_key)\n",
        "us_small_paid = get_keyword_estimates(client, us_under_250k_diy_keywords, us_states)\n",
        "ca_small_search_trends = load_trends_from_search_api(ca_under_250k_diy_keywords, search_api_io_key, geo='CA')\n",
        "ca_small_paid = get_keyword_estimates(client, ca_under_250k_diy_keywords, ca_provinces)"
      ],
      "metadata": {
        "id": "x93z7UENK4Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDYeI5kaL16Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmOnfun6J-7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVu4U-fbYZVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xJuxe-5-fxnF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ti-HmhOh0CNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5QDQbrzZ-LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naYJfD5vkwWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "us_small_op_trends_prepped = prep_trends_data(us_small_search_trends)\n",
        "us_small_op_trends_prepped\n",
        "us_small_op_paid_prepped = prep_paid_search_data(us_small_paid)\n",
        "us_small_op_paid_prepped\n",
        "counts_col='Ops_below_250k'\n",
        "us_small_op_audiences = add_us_stats_and_geos(repositioned_us, us_small_op_trends_prepped, state_summary, us_small_op_paid_prepped, counts_col=counts_col)\n",
        "sus_mall_op_audiences = us_small_op_audiences.to_crs(epsg=5070)\n",
        "us_small_op_audiences.loc[us_small_op_audiences['NAME'] == 'PUERTO RICO']\n",
        "vert_max = us_small_op_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "data_col = 'adjusted_audience_Ops_below_250k'\n",
        "file_stub = 'us_diy_under250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations < $250k'\n",
        "footer_txt = 'Data Sources: USDA, Google Ads API, Google Trends (Data normalized individually then summed)'\n",
        "\n",
        "plot_us_map(us_small_op_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "us_small_op_audiences[['NAME', 'STATEFP',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "ca_small_op_trends_prepped = prep_trends_data(ca_small_search_trends)\n",
        "ca_small_op_trends_prepped\n",
        "ca_small_op_paid_prepped = prep_paid_search_data(ca_small_paid)\n",
        "ca_small_op_paid_prepped\n",
        "counts_col='Ops_below_250k'\n",
        "ca_small_op_audiences = add_us_stats_and_geos(canada_provinces_gdf, ca_small_op_trends_prepped, provincial_summary_canada, ca_small_op_paid_prepped, counts_col=counts_col, gdf_geo_col='PRENAME',\n",
        "                                           stats_geo_col='geo_name')\n",
        "ca_small_op_audiences = ca_small_op_audiences.to_crs(epsg=5070)\n",
        "vert_max = ca_small_op_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "data_col = 'adjusted_audience_Ops_below_250k'\n",
        "file_stub = 'ca_diy_under250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: US Farm Operations < $250k'\n",
        "footer_txt = 'Data Sources: USDA, Google Ads API, Google Trends (Data normalized individually then summed)'\n",
        "\n",
        "plot_us_map(ca_small_op_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n",
        "ca_small_op_audiences[['PRENAME', 'PRUID',counts_col,data_col]].to_csv(f'{file_stub}_adjusted_audience_by_state.csv', index=False)\n",
        "\n",
        "\n",
        "na_small_audiences = pd.concat([us_small_op_audiences[['adjusted_audience_Ops_below_250k', 'geometry']], ca_small_op_audiences[['adjusted_audience_Ops_below_250k', 'geometry']]])\n",
        "vert_max = na_small_audiences['adjusted_audience_Ops_below_250k'].max()\n",
        "file_stub = 'na_diy_below_250k'\n",
        "leg_kwds_dict={'label': \"Estimated Audience\", 'orientation': \"vertical\", 'shrink': 0.7}\n",
        "title_txt = f'Audience Sizing for DIY Interest: NA Farm Operations  $250k'\n",
        "footer_txt = 'Data Sources: 2022 USDA Ag Census, 2021 CA Ag Census, Google Ads API, Google Search Trends'\n",
        "plot_us_map(na_small_audiences, density_cmap, data_col, leg_kwds_dict, title_txt, footer_txt, vert_max)\n"
      ],
      "metadata": {
        "id": "gBi-bTVQth2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}